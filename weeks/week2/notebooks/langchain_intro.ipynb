{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Fundamentals: Building Your First AI Chains\n",
        "\n",
        "Welcome to LangChain! This notebook will teach you the fundamentals of building AI applications using LangChain Expression Language (LCEL) and local LLMs.\n",
        "\n",
        "## ðŸŽ¯ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you'll understand:\n",
        "- **LangChain Core Concepts**: Prompts, Models, Output Parsers\n",
        "- **LCEL Syntax**: Building chains with the pipe operator\n",
        "- **Local LLM Integration**: Working with Ollama models\n",
        "- **Chain Composition**: Creating complex multi-step workflows\n",
        "- **Practical Applications**: Real-world AI chain examples\n",
        "\n",
        "## ðŸ“š What is LangChain?\n",
        "\n",
        "LangChain is a framework for developing applications powered by language models. It provides:\n",
        "\n",
        "- **ðŸ”— Chain Composition**: Connect multiple AI components\n",
        "- **ðŸ§  Model Abstraction**: Work with different LLMs uniformly\n",
        "- **ðŸ“ Prompt Management**: Structured prompt templates\n",
        "- **ðŸ”„ Memory & State**: Maintain context across interactions\n",
        "- **ðŸ”Œ Integrations**: Connect to databases, APIs, and tools\n",
        "\n",
        "## âš¡ LCEL (LangChain Expression Language)\n",
        "\n",
        "LCEL is LangChain's declarative way to compose chains:\n",
        "\n",
        "```python\n",
        "# Basic pattern\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "# More complex\n",
        "chain = input_formatter | prompt | model | output_parser | post_processor\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ› ï¸ Environment Setup\n",
        "\n",
        "Let's start by setting up our LangChain environment and verifying everything works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run this if needed)\n",
        "# !pip install langchain langchain-community langchain-core\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def setup_langchain_environment():\n",
        "    \"\"\"Set up and verify LangChain environment\"\"\"\n",
        "    print(\"ðŸ”§ LANGCHAIN ENVIRONMENT SETUP\")\n",
        "    print(\"=\" * 35)\n",
        "    \n",
        "    # Check Python version\n",
        "    python_version = sys.version_info\n",
        "    print(f\"ðŸ Python: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
        "    \n",
        "    # Check required packages\n",
        "    required_packages = {\n",
        "        'langchain': 'Core LangChain framework',\n",
        "        'langchain_community': 'Community integrations',\n",
        "        'langchain_core': 'Core abstractions'\n",
        "    }\n",
        "    \n",
        "    print(\"\\nðŸ“¦ Checking LangChain packages...\")\n",
        "    for package, description in required_packages.items():\n",
        "        try:\n",
        "            module = __import__(package)\n",
        "            version = getattr(module, '__version__', 'unknown')\n",
        "            print(f\"   âœ… {package} v{version} - {description}\")\n",
        "        except ImportError:\n",
        "            print(f\"   âŒ {package} - MISSING ({description})\")\n",
        "            print(f\"      Install with: pip install {package}\")\n",
        "    \n",
        "    # Check Ollama\n",
        "    print(\"\\nðŸ§  Checking Ollama...\")\n",
        "    try:\n",
        "        result = subprocess.run(['ollama', '--version'], \n",
        "                              capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            print(\"   âœ… Ollama installed and running\")\n",
        "            \n",
        "            # Check for available models\n",
        "            models_result = subprocess.run(['ollama', 'list'], \n",
        "                                         capture_output=True, text=True, timeout=10)\n",
        "            if models_result.returncode == 0:\n",
        "                models = models_result.stdout.strip().split('\\n')[1:]  # Skip header\n",
        "                if models and models[0]:\n",
        "                    print(f\"   ðŸ“‹ Available models: {len(models)}\")\n",
        "                    for model in models[:3]:  # Show first 3\n",
        "                        model_name = model.split()[0] if model.strip() else 'Unknown'\n",
        "                        print(f\"      â€¢ {model_name}\")\n",
        "                else:\n",
        "                    print(\"   âš ï¸  No models downloaded\")\n",
        "                    print(\"      Download with: ollama pull llama3.2\")\n",
        "        else:\n",
        "            print(\"   âŒ Ollama command failed\")\n",
        "    except (subprocess.TimeoutExpired, FileNotFoundError):\n",
        "        print(\"   âŒ Ollama not found\")\n",
        "        print(\"      Install from: https://ollama.com\")\n",
        "        print(\"      Required for local LLM exercises\")\n",
        "    \n",
        "    print(\"\\nðŸŽ¯ Environment setup complete!\")\n",
        "    return True\n",
        "\n",
        "setup_langchain_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LangChain Core Components\n",
        "\n",
        "Let's explore the three fundamental building blocks of LangChain applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core LangChain components\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "print(\"ðŸ“š Core LangChain Components\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "print(\"\\nðŸ”§ 1. PROMPTS - Structure your inputs\")\n",
        "print(\"   â€¢ ChatPromptTemplate: For chat-based models\")\n",
        "print(\"   â€¢ PromptTemplate: For completion models\")\n",
        "print(\"   â€¢ Custom templates with variables\")\n",
        "\n",
        "print(\"\\nðŸ§  2. MODELS - The AI brain\")\n",
        "print(\"   â€¢ ChatOllama: Local chat models\")\n",
        "print(\"   â€¢ Ollama: Local completion models\")\n",
        "print(\"   â€¢ Consistent interface across providers\")\n",
        "\n",
        "print(\"\\nðŸ“¤ 3. OUTPUT PARSERS - Format responses\")\n",
        "print(\"   â€¢ StrOutputParser: Plain text output\")\n",
        "print(\"   â€¢ JsonOutputParser: Structured JSON\")\n",
        "print(\"   â€¢ Custom parsers for specific formats\")\n",
        "\n",
        "print(\"\\nâœ… Core components imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Creating Your First Prompt Template\n",
        "\n",
        "Prompt templates help you create reusable, structured prompts with variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“ Creating Prompt Templates\")\n",
        "print(\"=\" * 28)\n",
        "\n",
        "# 1. Simple prompt template\n",
        "simple_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in simple terms.\"\n",
        ")\n",
        "\n",
        "print(\"ðŸ”¹ Simple Prompt Template:\")\n",
        "print(f\"   Template: {simple_prompt.template}\")\n",
        "print(f\"   Variables: {simple_prompt.input_variables}\")\n",
        "\n",
        "# Test the template\n",
        "formatted_prompt = simple_prompt.format(topic=\"machine learning\")\n",
        "print(f\"   Example: {formatted_prompt}\")\n",
        "\n",
        "# 2. Chat prompt template (more sophisticated)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful {role} with expertise in {domain}.\"),\n",
        "    (\"human\", \"Please explain {concept} and provide a practical example.\"),\n",
        "])\n",
        "\n",
        "print(\"\\nðŸ’¬ Chat Prompt Template:\")\n",
        "print(f\"   Messages: {len(chat_prompt.messages)}\")\n",
        "print(f\"   Variables: {chat_prompt.input_variables}\")\n",
        "\n",
        "# Test the chat template\n",
        "formatted_chat = chat_prompt.format_messages(\n",
        "    role=\"data scientist\",\n",
        "    domain=\"machine learning\",\n",
        "    concept=\"overfitting\"\n",
        ")\n",
        "\n",
        "print(\"   Example messages:\")\n",
        "for msg in formatted_chat:\n",
        "    print(f\"     {msg.type}: {msg.content}\")\n",
        "\n",
        "# 3. Advanced template with multiple variables and formatting\n",
        "analysis_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an expert data analyst. \n",
        "    \n",
        "Dataset: {dataset_name}\n",
        "Size: {num_rows} rows, {num_cols} columns\n",
        "Task: {analysis_task}\n",
        "\n",
        "Please provide a {analysis_type} analysis and {num_insights} key insights.\n",
        "Format your response as:\n",
        "1. Overview\n",
        "2. Key Findings\n",
        "3. Recommendations\"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ“Š Advanced Analysis Template:\")\n",
        "print(f\"   Variables: {analysis_prompt.input_variables}\")\n",
        "\n",
        "# Test advanced template\n",
        "test_analysis = analysis_prompt.format(\n",
        "    dataset_name=\"Customer Churn Data\",\n",
        "    num_rows=10000,\n",
        "    num_cols=15,\n",
        "    analysis_task=\"predict customer churn\",\n",
        "    analysis_type=\"statistical\",\n",
        "    num_insights=3\n",
        ")\n",
        "\n",
        "print(f\"   Example (first 200 chars): {test_analysis[:200]}...\")\n",
        "\n",
        "print(\"\\nâœ… Prompt templates created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Setting Up Local LLM Models\n",
        "\n",
        "Let's connect to Ollama and test different local models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ§  Setting Up Local LLM Models\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "# Model configurations for different use cases\n",
        "model_configs = {\n",
        "    \"fast\": {\n",
        "        \"model\": \"llama3.2:1b\",\n",
        "        \"temperature\": 0.1,\n",
        "        \"description\": \"Fast, lightweight model for quick responses\"\n",
        "    },\n",
        "    \"balanced\": {\n",
        "        \"model\": \"llama3.2\",\n",
        "        \"temperature\": 0.3,\n",
        "        \"description\": \"Balanced model for general use\"\n",
        "    },\n",
        "    \"creative\": {\n",
        "        \"model\": \"llama3.2\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"description\": \"Higher temperature for creative tasks\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize models\n",
        "models = {}\n",
        "available_models = []\n",
        "\n",
        "for config_name, config in model_configs.items():\n",
        "    try:\n",
        "        model = ChatOllama(\n",
        "            model=config[\"model\"],\n",
        "            temperature=config[\"temperature\"]\n",
        "        )\n",
        "        \n",
        "        # Test the model with a simple query\n",
        "        test_response = model.invoke([HumanMessage(content=\"Hello! Respond with just 'OK' if you're working.\")])\n",
        "        \n",
        "        models[config_name] = model\n",
        "        available_models.append(config_name)\n",
        "        \n",
        "        print(f\"   âœ… {config_name}: {config['model']} (temp: {config['temperature']})\")\n",
        "        print(f\"      {config['description']}\")\n",
        "        print(f\"      Test: {test_response.content}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ {config_name}: {config['model']} - Failed\")\n",
        "        print(f\"      Error: {str(e)}\")\n",
        "        print(f\"      Try: ollama pull {config['model']}\")\n",
        "\n",
        "if available_models:\n",
        "    print(f\"\\nðŸŽ¯ Available models: {', '.join(available_models)}\")\n",
        "    # Use the first available model as default\n",
        "    default_model = models[available_models[0]]\n",
        "    print(f\"   Using '{available_models[0]}' as default\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No models available. Please install Ollama and download a model:\")\n",
        "    print(\"     1. Install Ollama: https://ollama.com\")\n",
        "    print(\"     2. Download model: ollama pull llama3.2\")\n",
        "    print(\"     3. Restart this notebook\")\n",
        "    default_model = None\n",
        "\n",
        "print(\"\\nâœ… Model setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Output Parsers and Response Formatting\n",
        "\n",
        "Output parsers help you structure and validate LLM responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“¤ Output Parsers and Response Formatting\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "# 1. String Output Parser (most common)\n",
        "str_parser = StrOutputParser()\n",
        "print(\"ðŸ”¹ String Parser:\")\n",
        "print(\"   Converts AI message to plain string\")\n",
        "\n",
        "# 2. JSON Output Parser (for structured data)\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class DataAnalysis(BaseModel):\n",
        "    \"\"\"Structured data analysis output\"\"\"\n",
        "    summary: str = Field(description=\"Brief summary of findings\")\n",
        "    insights: list = Field(description=\"List of key insights\")\n",
        "    confidence: float = Field(description=\"Confidence score 0-1\")\n",
        "    recommendations: list = Field(description=\"List of recommendations\")\n",
        "\n",
        "json_parser = JsonOutputParser(pydantic_object=DataAnalysis)\n",
        "\n",
        "print(\"\\nðŸ”¹ JSON Parser:\")\n",
        "print(\"   Parses response into structured format\")\n",
        "print(f\"   Schema: {list(DataAnalysis.__fields__.keys())}\")\n",
        "\n",
        "# 3. Custom parser example\n",
        "class ListOutputParser:\n",
        "    \"\"\"Custom parser to extract numbered lists\"\"\"\n",
        "    \n",
        "    def parse(self, text: str) -> list:\n",
        "        \"\"\"Extract numbered list items from text\"\"\"\n",
        "        import re\n",
        "        \n",
        "        # Find numbered items (1. item, 2. item, etc.)\n",
        "        pattern = r'\\d+\\.\\s*(.+?)(?=\\n\\d+\\.|$)'\n",
        "        matches = re.findall(pattern, text, re.DOTALL)\n",
        "        \n",
        "        # Clean up the matches\n",
        "        return [match.strip() for match in matches]\n",
        "\n",
        "list_parser = ListOutputParser()\n",
        "\n",
        "print(\"\\nðŸ”¹ Custom List Parser:\")\n",
        "print(\"   Extracts numbered lists from responses\")\n",
        "\n",
        "# Test parsers with sample responses\n",
        "if available_models:\n",
        "    print(\"\\nðŸ§ª Testing parsers...\")\n",
        "    \n",
        "    # Test string parser\n",
        "    test_prompt = \"List 3 benefits of machine learning in one sentence each.\"\n",
        "    try:\n",
        "        response = default_model.invoke([HumanMessage(content=test_prompt)])\n",
        "        parsed_string = str_parser.parse(response)\n",
        "        \n",
        "        print(f\"   Original response type: {type(response)}\")\n",
        "        print(f\"   Parsed string type: {type(parsed_string)}\")\n",
        "        print(f\"   Content preview: {parsed_string[:100]}...\")\n",
        "        \n",
        "        # Test custom list parser\n",
        "        parsed_list = list_parser.parse(parsed_string)\n",
        "        print(f\"   Extracted list items: {len(parsed_list)}\")\n",
        "        for i, item in enumerate(parsed_list[:3], 1):\n",
        "            print(f\"     {i}. {item[:50]}...\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   Parser test failed: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Output parsers configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Building Your First LCEL Chains\n",
        "\n",
        "Now let's put it all together using LangChain Expression Language (LCEL)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Simple Chain: prompt | model | output_parser\n",
        "\n",
        "The most basic LCEL pattern connects three components with the pipe operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ”— Building Simple LCEL Chains\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if not available_models:\n",
        "    print(\"âš ï¸  Skipping chain examples - no models available\")\n",
        "else:\n",
        "    # Chain 1: Basic explanation chain\n",
        "    explanation_prompt = ChatPromptTemplate.from_template(\n",
        "        \"Explain {concept} in simple terms with a practical example.\"\n",
        "    )\n",
        "    \n",
        "    explanation_chain = explanation_prompt | default_model | str_parser\n",
        "    \n",
        "    print(\"ðŸ”¹ Explanation Chain: prompt | model | str_parser\")\n",
        "    print(\"   Input: concept name\")\n",
        "    print(\"   Output: simple explanation\")\n",
        "    \n",
        "    # Test the chain\n",
        "    try:\n",
        "        result = explanation_chain.invoke({\"concept\": \"neural networks\"})\n",
        "        print(f\"\\n   Test Result: {result[:150]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Test failed: {e}\")\n",
        "    \n",
        "    # Chain 2: Data analysis chain\n",
        "    analysis_prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"Analyze this dataset information:\n",
        "        \n",
        "Dataset: {dataset_name}\n",
        "Rows: {rows}\n",
        "Columns: {columns}\n",
        "Target: {target_variable}\n",
        "\n",
        "Provide 3 key insights and 2 recommendations.\"\"\"\n",
        "    )\n",
        "    \n",
        "    analysis_chain = analysis_prompt | default_model | str_parser\n",
        "    \n",
        "    print(\"\\nðŸ”¹ Analysis Chain: prompt | model | str_parser\")\n",
        "    print(\"   Input: dataset metadata\")\n",
        "    print(\"   Output: insights and recommendations\")\n",
        "    \n",
        "    # Test with sample data\n",
        "    try:\n",
        "        analysis_result = analysis_chain.invoke({\n",
        "            \"dataset_name\": \"Customer Churn\",\n",
        "            \"rows\": 5000,\n",
        "            \"columns\": 12,\n",
        "            \"target_variable\": \"churn (yes/no)\"\n",
        "        })\n",
        "        print(f\"\\n   Analysis Result: {analysis_result[:200]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Analysis test failed: {e}\")\n",
        "    \n",
        "    # Chain 3: Code generation chain\n",
        "    code_prompt = ChatPromptTemplate.from_template(\n",
        "        \"Write a Python function to {task}. Include docstring and example usage.\"\n",
        "    )\n",
        "    \n",
        "    code_chain = code_prompt | default_model | str_parser\n",
        "    \n",
        "    print(\"\\nðŸ”¹ Code Generation Chain: prompt | model | str_parser\")\n",
        "    print(\"   Input: task description\")\n",
        "    print(\"   Output: Python code with documentation\")\n",
        "    \n",
        "    try:\n",
        "        code_result = code_chain.invoke({\"task\": \"calculate the mean and standard deviation of a list\"})\n",
        "        print(f\"\\n   Code Result: {code_result[:200]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Code generation test failed: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Simple chains created and tested!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Advanced Chain Composition\n",
        "\n",
        "Let's build more sophisticated chains with multiple steps and data transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ”— Advanced Chain Composition\")\n",
        "print(\"=\" * 28)\n",
        "\n",
        "if not available_models:\n",
        "    print(\"âš ï¸  Skipping advanced examples - no models available\")\n",
        "else:\n",
        "    from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "    from operator import itemgetter\n",
        "    \n",
        "    # 1. Multi-step analysis chain\n",
        "    print(\"ðŸ”¹ Multi-step Analysis Chain\")\n",
        "    \n",
        "    # Step 1: Data preprocessing function\n",
        "    def preprocess_data_info(data_dict):\n",
        "        \"\"\"Process and format data information\"\"\"\n",
        "        processed = {\n",
        "            \"formatted_info\": f\"Dataset '{data_dict['name']}' has {data_dict['rows']:,} rows and {data_dict['cols']} columns.\",\n",
        "            \"complexity\": \"high\" if data_dict['rows'] > 10000 else \"medium\" if data_dict['rows'] > 1000 else \"low\",\n",
        "            \"original\": data_dict\n",
        "        }\n",
        "        return processed\n",
        "    \n",
        "    # Step 2: Analysis prompt\n",
        "    analysis_prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"Data Information: {formatted_info}\n",
        "Complexity Level: {complexity}\n",
        "\n",
        "Provide analysis recommendations based on the complexity level:\n",
        "- Low complexity: Simple statistical analysis\n",
        "- Medium complexity: Advanced statistical methods\n",
        "- High complexity: Machine learning approaches\n",
        "\n",
        "Give 3 specific recommendations.\"\"\"\n",
        "    )\n",
        "    \n",
        "    # Build the chain\n",
        "    multi_step_chain = (\n",
        "        RunnableLambda(preprocess_data_info)\n",
        "        | analysis_prompt\n",
        "        | default_model\n",
        "        | str_parser\n",
        "    )\n",
        "    \n",
        "    print(\"   Chain: preprocess | prompt | model | parser\")\n",
        "    \n",
        "    # Test the multi-step chain\n",
        "    try:\n",
        "        test_data = {\"name\": \"Sales Data\", \"rows\": 50000, \"cols\": 25}\n",
        "        multi_result = multi_step_chain.invoke(test_data)\n",
        "        print(f\"   Result: {multi_result[:150]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Test failed: {e}\")\n",
        "    \n",
        "    # 2. Parallel processing chain\n",
        "    print(\"\\nðŸ”¹ Parallel Processing Chain\")\n",
        "    \n",
        "    from langchain_core.runnables import RunnableParallel\n",
        "    \n",
        "    # Define different analysis perspectives\n",
        "    technical_prompt = ChatPromptTemplate.from_template(\n",
        "        \"From a technical perspective, analyze: {topic}\"\n",
        "    )\n",
        "    \n",
        "    business_prompt = ChatPromptTemplate.from_template(\n",
        "        \"From a business perspective, analyze: {topic}\"\n",
        "    )\n",
        "    \n",
        "    # Create parallel chains\n",
        "    parallel_chain = RunnableParallel(\n",
        "        technical=technical_prompt | default_model | str_parser,\n",
        "        business=business_prompt | default_model | str_parser\n",
        "    )\n",
        "    \n",
        "    print(\"   Chain: parallel(technical + business analysis)\")\n",
        "    \n",
        "    try:\n",
        "        parallel_result = parallel_chain.invoke({\"topic\": \"implementing machine learning in customer service\"})\n",
        "        print(f\"   Technical: {parallel_result['technical'][:100]}...\")\n",
        "        print(f\"   Business: {parallel_result['business'][:100]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Parallel test failed: {e}\")\n",
        "    \n",
        "    # 3. Conditional chain\n",
        "    print(\"\\nðŸ”¹ Conditional Chain\")\n",
        "    \n",
        "    def route_by_complexity(data):\n",
        "        \"\"\"Route to different prompts based on data complexity\"\"\"\n",
        "        rows = data.get('rows', 0)\n",
        "        if rows > 100000:\n",
        "            return \"big_data\"\n",
        "        elif rows > 10000:\n",
        "            return \"medium_data\"\n",
        "        else:\n",
        "            return \"small_data\"\n",
        "    \n",
        "    # Different prompts for different data sizes\n",
        "    prompts = {\n",
        "        \"small_data\": ChatPromptTemplate.from_template(\"Analyze this small dataset ({rows} rows): {description}\"),\n",
        "        \"medium_data\": ChatPromptTemplate.from_template(\"Analyze this medium dataset ({rows} rows): {description}\"),\n",
        "        \"big_data\": ChatPromptTemplate.from_template(\"Analyze this large dataset ({rows} rows): {description}\")\n",
        "    }\n",
        "    \n",
        "    # This is a simplified conditional example\n",
        "    print(\"   Chain: router â†’ appropriate_prompt | model | parser\")\n",
        "    print(\"   Routes based on dataset size\")\n",
        "\n",
        "print(\"\\nâœ… Advanced chains demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Chain with Memory and Context\n",
        "\n",
        "Let's create chains that maintain context across multiple interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ§  Chains with Memory and Context\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "if not available_models:\n",
        "    print(\"âš ï¸  Skipping memory examples - no models available\")\n",
        "else:\n",
        "    # Simple conversation memory\n",
        "    class SimpleConversationMemory:\n",
        "        \"\"\"Basic conversation memory for maintaining context\"\"\"\n",
        "        \n",
        "        def __init__(self, max_messages=10):\n",
        "            self.messages = []\n",
        "            self.max_messages = max_messages\n",
        "        \n",
        "        def add_message(self, role, content):\n",
        "            self.messages.append({\"role\": role, \"content\": content})\n",
        "            # Keep only recent messages\n",
        "            if len(self.messages) > self.max_messages:\n",
        "                self.messages = self.messages[-self.max_messages:]\n",
        "        \n",
        "        def get_context(self):\n",
        "            \"\"\"Format conversation history for prompt\"\"\"\n",
        "            if not self.messages:\n",
        "                return \"No previous conversation.\"\n",
        "            \n",
        "            context = \"Previous conversation:\\n\"\n",
        "            for msg in self.messages[-5:]:  # Last 5 messages\n",
        "                context += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "            return context\n",
        "    \n",
        "    # Initialize memory\n",
        "    memory = SimpleConversationMemory()\n",
        "    \n",
        "    # Contextual prompt template\n",
        "    contextual_prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"{context}\n",
        "\n",
        "User: {user_input}\n",
        "\n",
        "As a helpful data science assistant, respond to the user's question considering the conversation history.\"\"\"\n",
        "    )\n",
        "    \n",
        "    # Contextual chain\n",
        "    def create_contextual_response(user_input):\n",
        "        \"\"\"Generate response with conversation context\"\"\"\n",
        "        context = memory.get_context()\n",
        "        \n",
        "        # Create the chain\n",
        "        chain = contextual_prompt | default_model | str_parser\n",
        "        \n",
        "        # Generate response\n",
        "        response = chain.invoke({\n",
        "            \"context\": context,\n",
        "            \"user_input\": user_input\n",
        "        })\n",
        "        \n",
        "        # Update memory\n",
        "        memory.add_message(\"user\", user_input)\n",
        "        memory.add_message(\"assistant\", response)\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    print(\"ðŸ”¹ Contextual Conversation Chain\")\n",
        "    print(\"   Maintains conversation history\")\n",
        "    print(\"   Considers previous context in responses\")\n",
        "    \n",
        "    # Test the contextual chain\n",
        "    try:\n",
        "        print(\"\\nðŸ§ª Testing contextual conversation:\")\n",
        "        \n",
        "        # First interaction\n",
        "        response1 = create_contextual_response(\"What is overfitting in machine learning?\")\n",
        "        print(f\"   Q1: What is overfitting?\")\n",
        "        print(f\"   A1: {response1[:100]}...\")\n",
        "        \n",
        "        # Second interaction (references previous)\n",
        "        response2 = create_contextual_response(\"How can I prevent it?\")\n",
        "        print(f\"\\n   Q2: How can I prevent it?\")\n",
        "        print(f\"   A2: {response2[:100]}...\")\n",
        "        \n",
        "        # Show memory state\n",
        "        print(f\"\\n   Memory contains {len(memory.messages)} messages\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   Contextual test failed: {e}\")\n",
        "    \n",
        "    # Data analysis session memory\n",
        "    print(\"\\nðŸ”¹ Data Analysis Session Memory\")\n",
        "    \n",
        "    class AnalysisSession:\n",
        "        \"\"\"Maintains state for a data analysis session\"\"\"\n",
        "        \n",
        "        def __init__(self):\n",
        "            self.dataset_info = {}\n",
        "            self.analysis_steps = []\n",
        "            self.findings = []\n",
        "        \n",
        "        def set_dataset(self, name, rows, cols, description=\"\"):\n",
        "            self.dataset_info = {\n",
        "                \"name\": name,\n",
        "                \"rows\": rows,\n",
        "                \"cols\": cols,\n",
        "                \"description\": description\n",
        "            }\n",
        "        \n",
        "        def add_step(self, step_description):\n",
        "            self.analysis_steps.append(step_description)\n",
        "        \n",
        "        def add_finding(self, finding):\n",
        "            self.findings.append(finding)\n",
        "        \n",
        "        def get_session_summary(self):\n",
        "            return {\n",
        "                \"dataset\": self.dataset_info,\n",
        "                \"steps_completed\": len(self.analysis_steps),\n",
        "                \"recent_steps\": self.analysis_steps[-3:],\n",
        "                \"key_findings\": self.findings[-3:]\n",
        "            }\n",
        "    \n",
        "    # Example session\n",
        "    session = AnalysisSession()\n",
        "    session.set_dataset(\"Customer Data\", 10000, 15, \"E-commerce customer behavior\")\n",
        "    session.add_step(\"Loaded dataset and performed initial exploration\")\n",
        "    session.add_finding(\"High correlation between purchase amount and session duration\")\n",
        "    \n",
        "    print(\"   Tracks dataset info, analysis steps, and findings\")\n",
        "    print(f\"   Session summary: {session.get_session_summary()}\")\n",
        "\n",
        "print(\"\\nâœ… Memory and context patterns demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Practical Applications\n",
        "\n",
        "Let's build some real-world applications using LangChain!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Data Analysis Assistant\n",
        "\n",
        "A comprehensive assistant for data analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ” Data Analysis Assistant\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "if not available_models:\n",
        "    print(\"âš ï¸  Skipping assistant demo - no models available\")\n",
        "else:\n",
        "    class DataAnalysisAssistant:\n",
        "        \"\"\"AI-powered data analysis assistant\"\"\"\n",
        "        \n",
        "        def __init__(self, model):\n",
        "            self.model = model\n",
        "            self.setup_chains()\n",
        "        \n",
        "        def setup_chains(self):\n",
        "            \"\"\"Initialize different analysis chains\"\"\"\n",
        "            \n",
        "            # 1. Dataset overview chain\n",
        "            self.overview_prompt = ChatPromptTemplate.from_template(\n",
        "                \"\"\"Dataset Analysis Overview:\n",
        "                \n",
        "Name: {name}\n",
        "Size: {rows} rows Ã— {cols} columns\n",
        "Description: {description}\n",
        "Missing Data: {missing_info}\n",
        "\n",
        "Provide:\n",
        "1. Initial assessment of data quality\n",
        "2. Suggested first steps for analysis\n",
        "3. Potential challenges to watch for\"\"\"\n",
        "            )\n",
        "            \n",
        "            self.overview_chain = self.overview_prompt | self.model | str_parser\n",
        "            \n",
        "            # 2. Statistical analysis chain\n",
        "            self.stats_prompt = ChatPromptTemplate.from_template(\n",
        "                \"\"\"Statistical Summary:\n",
        "                \n",
        "{statistical_summary}\n",
        "\n",
        "Interpret these statistics and identify:\n",
        "1. Key patterns in the data\n",
        "2. Outliers or anomalies\n",
        "3. Relationships between variables\n",
        "4. Recommendations for further analysis\"\"\"\n",
        "            )\n",
        "            \n",
        "            self.stats_chain = self.stats_prompt | self.model | str_parser\n",
        "            \n",
        "            # 3. Visualization recommendation chain\n",
        "            self.viz_prompt = ChatPromptTemplate.from_template(\n",
        "                \"\"\"Data Visualization Recommendations:\n",
        "                \n",
        "Dataset Type: {data_type}\n",
        "Variables: {variables}\n",
        "Analysis Goal: {goal}\n",
        "\n",
        "Recommend 3-5 specific visualizations with:\n",
        "1. Chart type and rationale\n",
        "2. Variables to include\n",
        "3. Python code snippet (matplotlib/seaborn)\"\"\"\n",
        "            )\n",
        "            \n",
        "            self.viz_chain = self.viz_prompt | self.model | str_parser\n",
        "        \n",
        "        def analyze_dataset_overview(self, dataset_info):\n",
        "            \"\"\"Generate initial dataset analysis\"\"\"\n",
        "            return self.overview_chain.invoke(dataset_info)\n",
        "        \n",
        "        def interpret_statistics(self, stats_summary):\n",
        "            \"\"\"Interpret statistical summary\"\"\"\n",
        "            return self.stats_chain.invoke({\"statistical_summary\": stats_summary})\n",
        "        \n",
        "        def recommend_visualizations(self, data_info):\n",
        "            \"\"\"Suggest appropriate visualizations\"\"\"\n",
        "            return self.viz_chain.invoke(data_info)\n",
        "    \n",
        "    # Initialize assistant\n",
        "    assistant = DataAnalysisAssistant(default_model)\n",
        "    \n",
        "    print(\"âœ… Data Analysis Assistant initialized\")\n",
        "    print(\"   Capabilities: overview, statistics, visualizations\")\n",
        "    \n",
        "    # Test the assistant\n",
        "    try:\n",
        "        print(\"\\nðŸ§ª Testing assistant capabilities:\")\n",
        "        \n",
        "        # Test dataset overview\n",
        "        test_dataset = {\n",
        "            \"name\": \"E-commerce Customer Behavior\",\n",
        "            \"rows\": 15000,\n",
        "            \"cols\": 12,\n",
        "            \"description\": \"Customer purchase history and demographics\",\n",
        "            \"missing_info\": \"Age column has 5% missing values\"\n",
        "        }\n",
        "        \n",
        "        overview = assistant.analyze_dataset_overview(test_dataset)\n",
        "        print(f\"   Overview: {overview[:150]}...\")\n",
        "        \n",
        "        # Test visualization recommendations\n",
        "        viz_request = {\n",
        "            \"data_type\": \"customer behavior\",\n",
        "            \"variables\": \"age, purchase_amount, session_duration, category_preference\",\n",
        "            \"goal\": \"understand customer segmentation patterns\"\n",
        "        }\n",
        "        \n",
        "        viz_recommendations = assistant.recommend_visualizations(viz_request)\n",
        "        print(f\"\\n   Visualizations: {viz_recommendations[:150]}...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   Assistant test failed: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Data Analysis Assistant demo complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Code Generation and Review Assistant\n",
        "\n",
        "An assistant that helps with data science code generation and review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ’» Code Generation and Review Assistant\")\n",
        "print(\"=\" * 38)\n",
        "\n",
        "if not available_models:\n",
        "    print(\"âš ï¸  Skipping code assistant demo - no models available\")\n",
        "else:\n",
        "    class CodeAssistant:\n",
        "        \"\"\"AI assistant for data science code tasks\"\"\"\n",
        "        \n",
        "        def __init__(self, model):\n",
        "            self.model = model\n",
        "            self.setup_chains()\n",
        "        \n",
        "        def setup_chains(self):\n",
        "            \"\"\"Setup code-related chains\"\"\"\n",
        "            \n",
        "            # Code generation chain\n",
        "            self.code_gen_prompt = ChatPromptTemplate.from_template(\n",
        "                \"\"\"Generate Python code for the following task:\n",
        "                \n",
        "Task: {task}\n",
        "Requirements: {requirements}\n",
        "Libraries: {libraries}\n",
        "\n",
        "Provide:\n",
        "1. Complete, working Python code\n",
        "2. Proper docstrings and comments\n",
        "3. Example usage\n",
        "4. Error handling where appropriate\"\"\"\n",
        "            )\n",
        "            \n",
        "            self.code_gen_chain = self.code_gen_prompt | self.model | str_parser\n",
        "            \n",
        "            # Code review chain\n",
        "            self.code_review_prompt = ChatPromptTemplate.from_template(\n",
        "                \"\"\"Review this Python code for data science:\n",
        "                \n",
        "```python\n",
        "{code}\n",
        "```\n",
        "\n",
        "Provide feedback on:\n",
        "1. Code quality and best practices\n",
        "2. Potential bugs or issues\n",
        "3. Performance considerations\n",
        "4. Suggestions for improvement\n",
        "5. Data science best practices\"\"\"\n",
        "            )\n",
        "            \n",
        "            self.code_review_chain = self.code_review_prompt | self.model | str_parser\n",
        "            \n",
        "            # Debugging chain\n",
        "            self.debug_prompt = ChatPromptTemplate.from_template(\n",
        "                \"\"\"Debug this Python code error:\n",
        "                \n",
        "Code:\n",
        "```python\n",
        "{code}\n",
        "```\n",
        "\n",
        "Error:\n",
        "{error}\n",
        "\n",
        "Provide:\n",
        "1. Explanation of what's causing the error\n",
        "2. Corrected code\n",
        "3. Prevention tips for similar errors\"\"\"\n",
        "            )\n",
        "            \n",
        "            self.debug_chain = self.debug_prompt | self.model | str_parser\n",
        "        \n",
        "        def generate_code(self, task, requirements=\"\", libraries=\"pandas, numpy, matplotlib\"):\n",
        "            \"\"\"Generate code for a specific task\"\"\"\n",
        "            return self.code_gen_chain.invoke({\n",
        "                \"task\": task,\n",
        "                \"requirements\": requirements,\n",
        "                \"libraries\": libraries\n",
        "            })\n",
        "        \n",
        "        def review_code(self, code):\n",
        "            \"\"\"Review and provide feedback on code\"\"\"\n",
        "            return self.code_review_chain.invoke({\"code\": code})\n",
        "        \n",
        "        def debug_code(self, code, error):\n",
        "            \"\"\"Help debug code errors\"\"\"\n",
        "            return self.debug_chain.invoke({\"code\": code, \"error\": error})\n",
        "    \n",
        "    # Initialize code assistant\n",
        "    code_assistant = CodeAssistant(default_model)\n",
        "    \n",
        "    print(\"âœ… Code Assistant initialized\")\n",
        "    print(\"   Capabilities: generation, review, debugging\")\n",
        "    \n",
        "    # Test the code assistant\n",
        "    try:\n",
        "        print(\"\\nðŸ§ª Testing code assistant:\")\n",
        "        \n",
        "        # Test code generation\n",
        "        code_task = \"Create a function to detect outliers in a dataset using the IQR method\"\n",
        "        requirements = \"Function should handle both pandas DataFrame and numpy arrays\"\n",
        "        \n",
        "        generated_code = code_assistant.generate_code(code_task, requirements)\n",
        "        print(f\"   Generated code: {generated_code[:200]}...\")\n",
        "        \n",
        "        # Test code review\n",
        "        sample_code = \"\"\"\n",
        "def analyze_data(df):\n",
        "    mean = df.mean()\n",
        "    std = df.std()\n",
        "    return mean, std\n",
        "\"\"\"\n",
        "        \n",
        "        code_review = code_assistant.review_code(sample_code)\n",
        "        print(f\"\\n   Code review: {code_review[:200]}...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   Code assistant test failed: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Code Assistant demo complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Best Practices and Production Tips\n",
        "\n",
        "Key recommendations for building robust LangChain applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ­ LangChain Best Practices and Production Tips\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "best_practices = {\n",
        "    \"Prompt Engineering\": [\n",
        "        \"Use clear, specific instructions\",\n",
        "        \"Provide examples in prompts when helpful\",\n",
        "        \"Structure prompts with clear sections\",\n",
        "        \"Test prompts with different inputs\",\n",
        "        \"Version control your prompt templates\"\n",
        "    ],\n",
        "    \"Chain Design\": [\n",
        "        \"Keep chains focused on single responsibilities\",\n",
        "        \"Use intermediate steps for complex workflows\",\n",
        "        \"Implement proper error handling\",\n",
        "        \"Add logging and monitoring\",\n",
        "        \"Make chains testable and debuggable\"\n",
        "    ],\n",
        "    \"Model Management\": [\n",
        "        \"Choose appropriate model sizes for your use case\",\n",
        "        \"Implement fallback strategies\",\n",
        "        \"Monitor model performance and costs\",\n",
        "        \"Cache responses when appropriate\",\n",
        "        \"Consider fine-tuning for specific domains\"\n",
        "    ],\n",
        "    \"Performance\": [\n",
        "        \"Use async operations for concurrent requests\",\n",
        "        \"Implement request batching\",\n",
        "        \"Set appropriate timeouts\",\n",
        "        \"Monitor response times and success rates\",\n",
        "        \"Optimize prompt length and complexity\"\n",
        "    ],\n",
        "    \"Security\": [\n",
        "        \"Validate and sanitize user inputs\",\n",
        "        \"Implement rate limiting\",\n",
        "        \"Protect sensitive data in prompts\",\n",
        "        \"Use secure model hosting\",\n",
        "        \"Log security-relevant events\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, practices in best_practices.items():\n",
        "    print(f\"\\nðŸŽ¯ {category}:\")\n",
        "    for practice in practices:\n",
        "        print(f\"   â€¢ {practice}\")\n",
        "\n",
        "# Example production-ready chain pattern\n",
        "print(\"\\nðŸ”§ Production-Ready Chain Pattern:\")\n",
        "\n",
        "class ProductionChain:\n",
        "    \"\"\"Example of a production-ready LangChain implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, model, max_retries=3, timeout=30):\n",
        "        self.model = model\n",
        "        self.max_retries = max_retries\n",
        "        self.timeout = timeout\n",
        "        self.setup_chain()\n",
        "    \n",
        "    def setup_chain(self):\n",
        "        \"\"\"Setup the chain with error handling\"\"\"\n",
        "        self.prompt = ChatPromptTemplate.from_template(\n",
        "            \"Analyze the following data and provide insights: {data}\"\n",
        "        )\n",
        "        self.chain = self.prompt | self.model | str_parser\n",
        "    \n",
        "    def analyze_with_retry(self, data):\n",
        "        \"\"\"Execute analysis with retry logic\"\"\"\n",
        "        import time\n",
        "        \n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                # Add input validation\n",
        "                if not data or len(str(data)) > 10000:\n",
        "                    raise ValueError(\"Invalid input data\")\n",
        "                \n",
        "                # Execute chain\n",
        "                result = self.chain.invoke({\"data\": data})\n",
        "                \n",
        "                # Validate output\n",
        "                if len(result) < 10:\n",
        "                    raise ValueError(\"Response too short\")\n",
        "                \n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"result\": result,\n",
        "                    \"attempt\": attempt + 1\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    return {\n",
        "                        \"success\": False,\n",
        "                        \"error\": str(e),\n",
        "                        \"attempts\": attempt + 1\n",
        "                    }\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "print(\"   âœ… Includes error handling, retries, and validation\")\n",
        "print(\"   âœ… Input/output validation\")\n",
        "print(\"   âœ… Exponential backoff for retries\")\n",
        "print(\"   âœ… Structured error responses\")\n",
        "\n",
        "print(\"\\nðŸ“š Additional Resources:\")\n",
        "resources = [\n",
        "    \"LangChain Documentation: https://python.langchain.com/\",\n",
        "    \"LCEL Guide: https://python.langchain.com/docs/expression_language/\",\n",
        "    \"Ollama Models: https://ollama.com/library\",\n",
        "    \"Prompt Engineering Guide: https://www.promptingguide.ai/\"\n",
        "]\n",
        "\n",
        "for resource in resources:\n",
        "    print(f\"   ðŸ“– {resource}\")\n",
        "\n",
        "print(\"\\nâœ… Best practices overview complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Practice Exercises\n",
        "\n",
        "Now it's your turn! Try these exercises to reinforce your learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Build a Data Science Tutor Chain\n",
        "\n",
        "Create a chain that explains data science concepts with examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ¯ EXERCISE 1: Your solution here!\n",
        "print(\"ðŸ’» Exercise 1: Data Science Tutor Chain\")\n",
        "print(\"=\" * 38)\n",
        "\n",
        "print(\"ðŸ“‹ Requirements:\")\n",
        "print(\"   1. Create a prompt that explains concepts at different levels (beginner, intermediate, advanced)\")\n",
        "print(\"   2. Include practical examples and use cases\")\n",
        "print(\"   3. Add follow-up questions to test understanding\")\n",
        "print(\"   4. Test with concepts like 'cross-validation', 'feature engineering', 'bias-variance tradeoff'\")\n",
        "\n",
        "if available_models:\n",
        "    print(\"\\nðŸ”§ Build your tutor chain here:\")\n",
        "    \n",
        "    # Your code here!\n",
        "    # Hint: Use a prompt template with variables for concept, level, and context\n",
        "    \n",
        "    print(\"   Add your implementation above!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Install Ollama and download a model to complete this exercise\")\n",
        "\n",
        "print(\"\\nâœ… Exercise 1 space ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Create a Model Comparison Chain\n",
        "\n",
        "Build a chain that compares different machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ¯ EXERCISE 2: Your solution here!\n",
        "print(\"ðŸ’» Exercise 2: Model Comparison Chain\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "print(\"ðŸ“‹ Requirements:\")\n",
        "print(\"   1. Create a chain that compares ML models for a given dataset/problem\")\n",
        "print(\"   2. Consider factors: accuracy, interpretability, training time, overfitting risk\")\n",
        "print(\"   3. Provide recommendations based on dataset characteristics\")\n",
        "print(\"   4. Test with different scenarios (large dataset, small dataset, high-dimensional, etc.)\")\n",
        "\n",
        "if available_models:\n",
        "    print(\"\\nðŸ”§ Build your model comparison chain here:\")\n",
        "    \n",
        "    # Your code here!\n",
        "    # Hint: Use parallel chains to analyze different aspects simultaneously\n",
        "    \n",
        "    print(\"   Add your implementation above!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Install Ollama and download a model to complete this exercise\")\n",
        "\n",
        "print(\"\\nâœ… Exercise 2 space ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Advanced Chain with Error Handling\n",
        "\n",
        "Create a robust chain with comprehensive error handling and fallbacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ¯ EXERCISE 3: Your solution here!\n",
        "print(\"ðŸ’» Exercise 3: Robust Chain with Error Handling\")\n",
        "print(\"=\" * 47)\n",
        "\n",
        "print(\"ðŸ“‹ Requirements:\")\n",
        "print(\"   1. Build a chain that gracefully handles various error conditions\")\n",
        "print(\"   2. Implement retry logic with exponential backoff\")\n",
        "print(\"   3. Add input validation and output verification\")\n",
        "print(\"   4. Provide meaningful error messages and fallback responses\")\n",
        "print(\"   5. Include logging for debugging and monitoring\")\n",
        "\n",
        "if available_models:\n",
        "    print(\"\\nðŸ”§ Build your robust chain here:\")\n",
        "    \n",
        "    # Your code here!\n",
        "    # Hint: Use try-catch blocks, validation functions, and fallback strategies\n",
        "    \n",
        "    print(\"   Add your implementation above!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Install Ollama and download a model to complete this exercise\")\n",
        "\n",
        "print(\"\\nâœ… Exercise 3 space ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ‰ Summary and Next Steps\n",
        "\n",
        "Congratulations! You've mastered the fundamentals of LangChain and LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸŽ“ LANGCHAIN FUNDAMENTALS COMPLETE!\")\n",
        "print(\"=\" * 36)\n",
        "\n",
        "print(\"ðŸ† What You've Learned:\")\n",
        "skills_learned = [\n",
        "    \"âœ… LangChain core components (Prompts, Models, Parsers)\",\n",
        "    \"âœ… LCEL syntax and chain composition\",\n",
        "    \"âœ… Local LLM integration with Ollama\",\n",
        "    \"âœ… Advanced chain patterns and error handling\",\n",
        "    \"âœ… Memory and context management\",\n",
        "    \"âœ… Practical AI application development\",\n",
        "    \"âœ… Production best practices\"\n",
        "]\n",
        "\n",
        "for skill in skills_learned:\n",
        "    print(f\"   {skill}\")\n",
        "\n",
        "print(\"\\nðŸ› ï¸ Key Concepts Mastered:\")\n",
        "concepts = {\n",
        "    \"Chain Patterns\": [\"Simple chains\", \"Parallel processing\", \"Conditional routing\", \"Multi-step workflows\"],\n",
        "    \"LCEL Operations\": [\"Pipe operator |\", \"RunnablePassthrough\", \"RunnableParallel\", \"RunnableLambda\"],\n",
        "    \"Production Skills\": [\"Error handling\", \"Retry logic\", \"Input validation\", \"Performance optimization\"]\n",
        "}\n",
        "\n",
        "for category, skill_list in concepts.items():\n",
        "    print(f\"   ðŸŽ¯ {category}: {', '.join(skill_list)}\")\n",
        "\n",
        "print(\"\\nðŸ”® What's Next:\")\n",
        "next_topics = [\n",
        "    \"ðŸ¤– LangChain Agents and Tools\",\n",
        "    \"ðŸ—ƒï¸ Vector Databases and Embeddings\",\n",
        "    \"ðŸ” Retrieval-Augmented Generation (RAG)\",\n",
        "    \"ðŸ’¾ Advanced Memory Patterns\",\n",
        "    \"ðŸŒ Building Full AI Applications\",\n",
        "    \"ðŸ“Š LangSmith for Monitoring and Debugging\"\n",
        "]\n",
        "\n",
        "for topic in next_topics:\n",
        "    print(f\"   {topic}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Practice Recommendations:\")\n",
        "practice_tasks = [\n",
        "    \"ðŸ”„ Complete the exercises above with different prompts and models\",\n",
        "    \"ðŸŽ¨ Build a creative writing assistant using multiple LLM models\",\n",
        "    \"ðŸ“ˆ Create a financial analysis chain with data processing\",\n",
        "    \"ðŸ” Develop a research assistant that summarizes multiple sources\",\n",
        "    \"ðŸ—ï¸ Design a multi-agent system for complex workflows\"\n",
        "]\n",
        "\n",
        "for task in practice_tasks:\n",
        "    print(f\"   {task}\")\n",
        "\n",
        "print(\"\\nðŸ“š Additional Learning:\")\n",
        "learning_resources = [\n",
        "    \"ðŸ“– Explore LangChain community integrations\",\n",
        "    \"ðŸŽ¥ Watch LangChain YouTube tutorials\",\n",
        "    \"ðŸ’» Contribute to open-source LangChain projects\",\n",
        "    \"ðŸ† Build and share your own LangChain applications\",\n",
        "    \"ðŸ“ Write about your LangChain experiences\"\n",
        "]\n",
        "\n",
        "for resource in learning_resources:\n",
        "    print(f\"   {resource}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Self-Assessment:\")\n",
        "assessment_questions = [\n",
        "    \"â“ Can you explain the difference between a prompt template and a chain?\",\n",
        "    \"â“ When would you use parallel vs sequential chain composition?\",\n",
        "    \"â“ How do you handle errors in production LangChain applications?\",\n",
        "    \"â“ What are the trade-offs between local and API-based models?\",\n",
        "    \"â“ How do you optimize chain performance for large-scale applications?\"\n",
        "]\n",
        "\n",
        "for question in assessment_questions:\n",
        "    print(f\"   {question}\")\n",
        "\n",
        "print(\"\\nðŸŒŸ Remember:\")\n",
        "reminders = [\n",
        "    \"ðŸ§  Start with simple chains and gradually add complexity\",\n",
        "    \"ðŸ”§ Always test your chains with various inputs\",\n",
        "    \"ðŸ“Š Monitor performance and costs in production\",\n",
        "    \"ðŸ”’ Implement proper security and validation\",\n",
        "    \"ðŸ“ˆ Iterate and improve based on user feedback\"\n",
        "]\n",
        "\n",
        "for reminder in reminders:\n",
        "    print(f\"   {reminder}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ You're now ready to build powerful AI applications with LangChain!\")\n",
        "print(\"ðŸ† - INRIVA AI Academy Team\")\n",
        "\n",
        "# Save learning progress\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "progress = {\n",
        "    'module': 'langchain_fundamentals',\n",
        "    'completed': True,\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'skills_learned': [skill.replace('âœ… ', '') for skill in skills_learned],\n",
        "    'concepts_mastered': {k: v for k, v in concepts.items()},\n",
        "    'next_steps': [topic.replace('ðŸ¤– ', '').replace('ðŸ—ƒï¸ ', '').replace('ðŸ” ', '').replace('ðŸ’¾ ', '').replace('ðŸŒ ', '').replace('ðŸ“Š ', '') for topic in next_topics],\n",
        "    'exercises_completed': 'Available for practice',\n",
        "    'models_used': available_models if 'available_models' in globals() else []\n",
        "}\n",
        "\n",
        "print(f\"\\nðŸ’¾ Learning progress saved: {len(json.dumps(progress))} characters\")\n",
        "print(\"ðŸ“‹ Ready for advanced LangChain topics in the next module!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}