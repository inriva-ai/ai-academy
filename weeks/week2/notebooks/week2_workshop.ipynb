{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2 Interactive Workshop: Advanced Data Processing & LangChain Fundamentals\n",
        "\n",
        "Welcome to Week 2! This interactive workshop combines advanced data preprocessing with LangChain fundamentals to build powerful hybrid ML + LLM pipelines.\n",
        "\n",
        "## 🎯 Workshop Objectives\n",
        "\n",
        "By the end of this session, you'll be able to:\n",
        "- Master advanced Metaflow preprocessing patterns\n",
        "- Understand and use LangChain Expression Language (LCEL)\n",
        "- Set up and work with local LLMs using Ollama\n",
        "- Build hybrid workflows combining traditional ML with LLM capabilities\n",
        "- Process text data with sophisticated NLP techniques\n",
        "\n",
        "## ⏰ Workshop Timeline (90 minutes)\n",
        "\n",
        "### Part 1: Advanced Data Preprocessing (45 minutes)\n",
        "1. **Missing Data Strategies** (10 min) - Advanced imputation techniques\n",
        "2. **Feature Engineering** (15 min) - Creating predictive features\n",
        "3. **Scaling and Validation** (10 min) - Pipeline robustness\n",
        "4. **Text Data Handling** (10 min) - NLP preprocessing fundamentals\n",
        "\n",
        "### Part 2: LangChain Introduction (30 minutes)\n",
        "1. **Installation and Setup** (5 min) - LangChain and Ollama\n",
        "2. **First LCEL Chain** (10 min) - prompt | model | output_parser\n",
        "3. **Local LLM Integration** (10 min) - Working with Ollama models\n",
        "4. **Chain Composition** (5 min) - Building complex workflows\n",
        "\n",
        "### Part 3: Integration Workshop (15 minutes)\n",
        "1. **Hybrid Pipelines** (10 min) - Combining Metaflow + LangChain\n",
        "2. **Text Analysis** (5 min) - LLM-powered data insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📋 Prerequisites Check\n",
        "\n",
        "Let's verify your environment is ready for the workshop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def check_environment():\n",
        "    \"\"\"Comprehensive environment verification for Week 2 workshop\"\"\"\n",
        "    print(\"🔍 WEEK 2 ENVIRONMENT CHECK\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Check Python packages\n",
        "    required_packages = {\n",
        "        'metaflow': ['metaflow', '2.7+'],\n",
        "        'pandas': ['pandas', '1.3+'],\n",
        "        'numpy': ['numpy', '1.20+'],\n",
        "        'sklearn': ['scikit-learn', '1.0+'],\n",
        "        'langchain': ['langchain', '0.1+'],\n",
        "        'langchain_community': ['langchain_community', '0.0.10+'],\n",
        "        'matplotlib': ['matplotlib', '3.3+'],\n",
        "        'seaborn': ['seaborn', '0.11+']\n",
        "    }\n",
        "    \n",
        "    print(\"📦 Checking Python packages...\")\n",
        "    missing_packages = []\n",
        "    \n",
        "    for lib, package in required_packages.items():\n",
        "        try:\n",
        "            __import__(lib)\n",
        "            print(f\"   ✅ {package[0]} {package[1]} - OK\")\n",
        "        except ImportError:\n",
        "            print(f\"   ❌ {package[0]} {package[1]} - MISSING\")\n",
        "            missing_packages.append(f\"{package[0]} {package[1]}\")\n",
        "    \n",
        "    if missing_packages:\n",
        "        print(f\"\\n⚠️  Install missing packages: pip install {' '.join(missing_packages)}\")\n",
        "        return False\n",
        "    \n",
        "    # Check data files\n",
        "    print(\"\\n📊 Checking data files...\")\n",
        "    data_files = [\n",
        "        '../data/titanic.csv',\n",
        "        '../data/customer_reviews.csv',\n",
        "        '../data/financial_data.json'\n",
        "    ]\n",
        "    \n",
        "    for file_path in data_files:\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"   ✅ {os.path.basename(file_path)}\")\n",
        "        else:\n",
        "            print(f\"   ⚠️  {os.path.basename(file_path)} - Not found (will use sample data)\")\n",
        "    \n",
        "    # Check Ollama\n",
        "    print(\"\\n🧠 Checking Ollama installation...\")\n",
        "    try:\n",
        "        result = subprocess.run(['ollama', '--version'], \n",
        "                              capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            print(\"   ✅ Ollama installed\")\n",
        "            print(\"   💡 Download model with: ollama pull llama3.2\")\n",
        "        else:\n",
        "            print(\"   ❌ Ollama command failed\")\n",
        "    except (subprocess.TimeoutExpired, FileNotFoundError):\n",
        "        print(\"   ❌ Ollama not found - install from ollama.com\")\n",
        "        print(\"      This is required for LLM exercises\")\n",
        "    \n",
        "    print(\"\\n🎯 Environment check complete!\")\n",
        "    print(\"   Ready to start the workshop!\")\n",
        "    return True\n",
        "\n",
        "check_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Advanced Data Preprocessing with Metaflow (45 minutes)\n",
        "\n",
        "Let's start by setting up our imports and loading sample data for preprocessing exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete imports for advanced data preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, MinMaxScaler, RobustScaler,\n",
        "    LabelEncoder, OneHotEncoder\n",
        ")\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from metaflow import FlowSpec, step, Parameter, IncludeFile, catch\n",
        "import re\n",
        "import string\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"viridis\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"📊 Data preprocessing environment ready!\")\n",
        "print(\"🎯 Let's build advanced preprocessing pipelines!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Loading and Exploring Our Datasets\n",
        "\n",
        "We'll work with three datasets to practice different preprocessing techniques:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets (with fallback to sample data)\n",
        "def load_workshop_data():\n",
        "    \"\"\"Load datasets for preprocessing workshop\"\"\"\n",
        "    \n",
        "    print(\"📥 Loading workshop datasets...\")\n",
        "    \n",
        "    # Dataset 1: Titanic (structured data with missing values)\n",
        "    try:\n",
        "        titanic = pd.read_csv('../data/titanic.csv')\n",
        "        print(\"   ✅ Loaded Titanic dataset\")\n",
        "    except FileNotFoundError:\n",
        "        # Create sample titanic-like data\n",
        "        np.random.seed(42)\n",
        "        n_samples = 800\n",
        "        titanic = pd.DataFrame({\n",
        "            'PassengerId': range(1, n_samples + 1),\n",
        "            'Survived': np.random.choice([0, 1], n_samples, p=[0.6, 0.4]),\n",
        "            'Pclass': np.random.choice([1, 2, 3], n_samples, p=[0.2, 0.3, 0.5]),\n",
        "            'Name': [f'Passenger {i}' for i in range(1, n_samples + 1)],\n",
        "            'Sex': np.random.choice(['male', 'female'], n_samples, p=[0.65, 0.35]),\n",
        "            'Age': np.random.normal(30, 12, n_samples),\n",
        "            'SibSp': np.random.poisson(0.5, n_samples),\n",
        "            'Parch': np.random.poisson(0.3, n_samples),\n",
        "            'Ticket': [f'TICKET{i}' for i in range(1, n_samples + 1)],\n",
        "            'Fare': np.random.lognormal(3, 1, n_samples),\n",
        "            'Cabin': [f'C{i}' if np.random.random() > 0.7 else None for i in range(n_samples)],\n",
        "            'Embarked': np.random.choice(['C', 'Q', 'S'], n_samples, p=[0.2, 0.1, 0.7])\n",
        "        })\n",
        "        # Introduce missing values\n",
        "        titanic.loc[np.random.choice(titanic.index, 150, replace=False), 'Age'] = np.nan\n",
        "        titanic.loc[np.random.choice(titanic.index, 50, replace=False), 'Embarked'] = np.nan\n",
        "        print(\"   ✅ Created sample Titanic-like dataset\")\n",
        "    \n",
        "    # Dataset 2: Customer Reviews (text data)\n",
        "    try:\n",
        "        reviews = pd.read_csv('../data/customer_reviews.csv')\n",
        "        print(\"   ✅ Loaded customer reviews dataset\")\n",
        "    except FileNotFoundError:\n",
        "        # Create sample review data\n",
        "        sample_reviews = [\n",
        "            \"Great product! Highly recommend to everyone.\",\n",
        "            \"Terrible quality. Broke after one day.\",\n",
        "            \"Average product, nothing special but works fine.\",\n",
        "            \"Amazing customer service and fast delivery!\",\n",
        "            \"Not worth the money. Poor build quality.\",\n",
        "            \"Excellent value for money. Very satisfied!\",\n",
        "            \"Disappointed with the purchase. Returns policy unclear.\",\n",
        "            \"Perfect for my needs. Would buy again.\"\n",
        "        ]\n",
        "        \n",
        "        np.random.seed(42)\n",
        "        n_reviews = 500\n",
        "        reviews = pd.DataFrame({\n",
        "            'review_id': range(1, n_reviews + 1),\n",
        "            'review_text': np.random.choice(sample_reviews, n_reviews),\n",
        "            'rating': np.random.choice([1, 2, 3, 4, 5], n_reviews, p=[0.1, 0.1, 0.2, 0.3, 0.3]),\n",
        "            'product_category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books'], n_reviews)\n",
        "        })\n",
        "        print(\"   ✅ Created sample customer reviews dataset\")\n",
        "    \n",
        "    return titanic, reviews\n",
        "\n",
        "# Load the data\n",
        "titanic_df, reviews_df = load_workshop_data()\n",
        "\n",
        "print(f\"\\n📊 Dataset Overview:\")\n",
        "print(f\"   🚢 Titanic: {titanic_df.shape[0]} rows, {titanic_df.shape[1]} columns\")\n",
        "print(f\"   💬 Reviews: {reviews_df.shape[0]} rows, {reviews_df.shape[1]} columns\")\n",
        "\n",
        "# Quick preview\n",
        "print(\"\\n🔍 Titanic Preview:\")\n",
        "display(titanic_df.head())\n",
        "\n",
        "print(\"\\n🔍 Reviews Preview:\")\n",
        "display(reviews_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 1.1: Missing Data Analysis (10 minutes)\n",
        "\n",
        "**Your Task**: Analyze and handle missing data in the Titanic dataset using advanced imputation techniques.\n",
        "\n",
        "**Instructions**:\n",
        "1. Identify columns with missing values and their percentages\n",
        "2. Visualize missing data patterns\n",
        "3. Implement different imputation strategies:\n",
        "   - Simple imputation (mean, median, mode)\n",
        "   - KNN imputation\n",
        "   - Custom domain-specific imputation\n",
        "4. Compare the results and choose the best strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 1.1: Your solution here!\n",
        "print(\"💻 Exercise 1.1: Missing Data Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Step 1: Identify missing values\n",
        "missing_info = titanic_df.isnull().sum()\n",
        "missing_percent = (missing_info / len(titanic_df)) * 100\n",
        "\n",
        "print(\"📊 Missing Value Analysis:\")\n",
        "for col in missing_info[missing_info > 0].index:\n",
        "    print(f\"   {col}: {missing_info[col]} ({missing_percent[col]:.1f}%)\")\n",
        "\n",
        "# Step 2: Visualize missing patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "missing_info[missing_info > 0].plot(kind='bar')\n",
        "plt.title('Missing Values Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Heatmap of missing values\n",
        "plt.imshow(titanic_df.isnull(), cmap='viridis', aspect='auto')\n",
        "plt.title('Missing Values Pattern')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Rows')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Your turn: Implement different imputation strategies!\n",
        "print(\"\\n🔧 Implement your imputation strategies below:\")\n",
        "print(\"   Hint: Try SimpleImputer and KNNImputer from sklearn\")\n",
        "print(\"   Consider domain knowledge for better imputation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 1.2: Feature Engineering (15 minutes)\n",
        "\n",
        "**Your Task**: Create meaningful features from the Titanic dataset to improve model performance.\n",
        "\n",
        "**Instructions**:\n",
        "1. Extract titles from passenger names\n",
        "2. Create family size features\n",
        "3. Bin numerical features (Age, Fare)\n",
        "4. Create interaction features\n",
        "5. Evaluate feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 1.2: Your solution here!\n",
        "print(\"💻 Exercise 1.2: Feature Engineering\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create a copy for feature engineering\n",
        "titanic_fe = titanic_df.copy()\n",
        "\n",
        "# Example: Extract titles from names\n",
        "def extract_title(name):\n",
        "    \"\"\"Extract title from passenger name\"\"\"\n",
        "    title = re.search(' ([A-Za-z]+)\\.', name)\n",
        "    if title:\n",
        "        return title.group(1)\n",
        "    return 'Unknown'\n",
        "\n",
        "titanic_fe['Title'] = titanic_fe['Name'].apply(extract_title)\n",
        "print(f\"📝 Extracted titles: {titanic_fe['Title'].value_counts().head()}\")\n",
        "\n",
        "# Your turn: Create more features!\n",
        "print(\"\\n🔧 Create additional features:\")\n",
        "print(\"   1. Family size (SibSp + Parch + 1)\")\n",
        "print(\"   2. Is alone (family size == 1)\")\n",
        "print(\"   3. Age groups (Child, Adult, Senior)\")\n",
        "print(\"   4. Fare per person (Fare / family size)\")\n",
        "print(\"   5. Deck from Cabin (first letter)\")\n",
        "\n",
        "# Add your feature engineering code here!\n",
        "\n",
        "print(\"\\n📊 Feature engineering complete!\")\n",
        "print(f\"   Original features: {titanic_df.shape[1]}\")\n",
        "print(f\"   New features: {titanic_fe.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 1.3: Scaling and Pipeline Validation (10 minutes)\n",
        "\n",
        "**Your Task**: Build a robust preprocessing pipeline with proper scaling and validation.\n",
        "\n",
        "**Instructions**:\n",
        "1. Compare different scaling techniques\n",
        "2. Handle categorical variables properly\n",
        "3. Create a complete preprocessing pipeline\n",
        "4. Validate pipeline robustness with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 1.3: Your solution here!\n",
        "print(\"💻 Exercise 1.3: Scaling and Pipeline Validation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare sample data for scaling comparison\n",
        "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "sample_data = titanic_df[numerical_features].dropna()\n",
        "\n",
        "# Compare scaling techniques\n",
        "scalers = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    'RobustScaler': RobustScaler()\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, (name, scaler) in enumerate(scalers.items(), 1):\n",
        "    scaled_data = scaler.fit_transform(sample_data)\n",
        "    \n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.boxplot(scaled_data, labels=numerical_features)\n",
        "    plt.title(f'{name}')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🔧 Build your complete preprocessing pipeline:\")\n",
        "print(\"   1. Handle missing values\")\n",
        "print(\"   2. Encode categorical variables\")\n",
        "print(\"   3. Scale numerical features\")\n",
        "print(\"   4. Add feature engineering\")\n",
        "print(\"   5. Validate with cross-validation\")\n",
        "\n",
        "# Add your pipeline code here!\n",
        "\n",
        "print(\"\\n✅ Pipeline validation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 1.4: Text Data Preprocessing (10 minutes)\n",
        "\n",
        "**Your Task**: Process the customer reviews dataset using NLP techniques.\n",
        "\n",
        "**Instructions**:\n",
        "1. Clean and normalize text data\n",
        "2. Remove stopwords and special characters\n",
        "3. Apply stemming/lemmatization\n",
        "4. Create TF-IDF features\n",
        "5. Analyze text patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 1.4: Your solution here!\n",
        "print(\"💻 Exercise 1.4: Text Data Preprocessing\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "# Basic text preprocessing function\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text data\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Apply basic cleaning\n",
        "reviews_df['clean_text'] = reviews_df['review_text'].apply(clean_text)\n",
        "\n",
        "print(\"📝 Text Cleaning Example:\")\n",
        "for i in range(3):\n",
        "    print(f\"   Original: {reviews_df['review_text'].iloc[i]}\")\n",
        "    print(f\"   Cleaned:  {reviews_df['clean_text'].iloc[i]}\")\n",
        "    print()\n",
        "\n",
        "# Your turn: Add more sophisticated preprocessing!\n",
        "print(\"🔧 Add advanced text preprocessing:\")\n",
        "print(\"   1. Remove stopwords\")\n",
        "print(\"   2. Apply stemming or lemmatization\")\n",
        "print(\"   3. Create TF-IDF features\")\n",
        "print(\"   4. Extract sentiment features\")\n",
        "print(\"   5. Analyze word frequencies\")\n",
        "\n",
        "# Create TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "tfidf_features = vectorizer.fit_transform(reviews_df['clean_text'])\n",
        "\n",
        "print(f\"\\n📊 TF-IDF Matrix: {tfidf_features.shape}\")\n",
        "print(f\"   Features: {len(vectorizer.get_feature_names_out())}\")\n",
        "print(f\"   Top features: {vectorizer.get_feature_names_out()[:10]}\")\n",
        "\n",
        "# Add your advanced text processing here!\n",
        "\n",
        "print(\"\\n✅ Text preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: LangChain Introduction (30 minutes)\n",
        "\n",
        "Now let's dive into LangChain and build our first chains using LCEL (LangChain Expression Language)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangChain imports and setup\n",
        "try:\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_community.llms import Ollama\n",
        "    from langchain_community.chat_models import ChatOllama\n",
        "    from langchain_core.runnables import RunnablePassthrough\n",
        "    from langchain_core.messages import HumanMessage, SystemMessage\n",
        "    \n",
        "    print(\"✅ LangChain imports successful!\")\n",
        "    \n",
        "    # Test Ollama connection\n",
        "    try:\n",
        "        llm = ChatOllama(model=\"llama3.2\", temperature=0.1)\n",
        "        test_response = llm.invoke([HumanMessage(content=\"Hello! Just testing connection.\")])\n",
        "        print(\"✅ Ollama connection successful!\")\n",
        "        print(f\"   Model: llama3.2\")\n",
        "        print(f\"   Test response: {test_response.content[:50]}...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Ollama connection failed: {e}\")\n",
        "        print(\"   Make sure Ollama is running and llama3.2 is downloaded\")\n",
        "        print(\"   Run: ollama pull llama3.2\")\n",
        "        \n",
        "except ImportError as e:\n",
        "    print(f\"❌ LangChain import failed: {e}\")\n",
        "    print(\"   Install with: pip install langchain langchain-community\")\n",
        "\n",
        "print(\"\\n🧠 LangChain environment ready!\")\n",
        "print(\"🎯 Let's build some chains!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 2.1: Your First LCEL Chain (10 minutes)\n",
        "\n",
        "**Your Task**: Build a simple prompt | model | output_parser chain using LCEL syntax.\n",
        "\n",
        "**Instructions**:\n",
        "1. Create a chat prompt template\n",
        "2. Set up the Ollama model\n",
        "3. Add an output parser\n",
        "4. Chain them together with LCEL\n",
        "5. Test with different inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 2.1: Your solution here!\n",
        "print(\"💻 Exercise 2.1: Building Your First LCEL Chain\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Step 1: Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful data science assistant. Provide clear, concise explanations.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Step 2: Set up the model\n",
        "model = ChatOllama(model=\"llama3.2\", temperature=0.1)\n",
        "\n",
        "# Step 3: Add output parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Step 4: Create the chain using LCEL\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "print(\"🔗 Chain created: prompt | model | output_parser\")\n",
        "\n",
        "# Step 5: Test the chain\n",
        "test_questions = [\n",
        "    \"What is the difference between bias and variance in machine learning?\",\n",
        "    \"Explain what cross-validation is in simple terms.\",\n",
        "    \"What are the main steps in a data preprocessing pipeline?\"\n",
        "]\n",
        "\n",
        "print(\"\\n🧪 Testing the chain:\")\n",
        "for i, question in enumerate(test_questions[:1], 1):  # Test first question\n",
        "    print(f\"\\n   Question {i}: {question}\")\n",
        "    try:\n",
        "        response = chain.invoke({\"question\": question})\n",
        "        print(f\"   Answer: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Error: {e}\")\n",
        "\n",
        "# Your turn: Try the other questions and create your own!\n",
        "print(\"\\n🔧 Your turn:\")\n",
        "print(\"   1. Test the remaining questions\")\n",
        "print(\"   2. Create your own data science questions\")\n",
        "print(\"   3. Experiment with different temperature values\")\n",
        "print(\"   4. Modify the system prompt\")\n",
        "\n",
        "print(\"\\n✅ First LCEL chain complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 2.2: Advanced Chain Composition (10 minutes)\n",
        "\n",
        "**Your Task**: Build more complex chains with multiple steps and data processing.\n",
        "\n",
        "**Instructions**:\n",
        "1. Create a data analysis chain\n",
        "2. Add data preprocessing steps\n",
        "3. Combine statistical analysis with LLM insights\n",
        "4. Build a chain that processes our customer reviews\n",
        "5. Create a summary and recommendations chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 2.2: Your solution here!\n",
        "print(\"💻 Exercise 2.2: Advanced Chain Composition\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Create a data analysis chain\n",
        "analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a data analyst. Analyze the provided data and give insights.\"),\n",
        "    (\"human\", \"Analyze this dataset summary: {data_summary}\\n\\nProvide key insights and recommendations.\")\n",
        "])\n",
        "\n",
        "# Create a function to summarize our review data\n",
        "def summarize_reviews(reviews_df):\n",
        "    \"\"\"Create a statistical summary of reviews\"\"\"\n",
        "    summary = {\n",
        "        'total_reviews': len(reviews_df),\n",
        "        'avg_rating': reviews_df['rating'].mean(),\n",
        "        'rating_distribution': reviews_df['rating'].value_counts().to_dict(),\n",
        "        'categories': reviews_df['product_category'].value_counts().to_dict(),\n",
        "        'common_words': ' '.join(reviews_df['review_text']).lower().split()\n",
        "    }\n",
        "    \n",
        "    # Get most common words (simple approach)\n",
        "    from collections import Counter\n",
        "    word_counts = Counter(summary['common_words'])\n",
        "    summary['top_words'] = dict(word_counts.most_common(10))\n",
        "    del summary['common_words']  # Remove the large list\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# Create the analysis chain\n",
        "analysis_chain = (\n",
        "    RunnablePassthrough()\n",
        "    | analysis_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "# Test the chain with our review data\n",
        "print(\"📊 Analyzing customer reviews...\")\n",
        "review_summary = summarize_reviews(reviews_df)\n",
        "print(f\"   Summary: {review_summary}\")\n",
        "\n",
        "try:\n",
        "    insights = analysis_chain.invoke({\"data_summary\": str(review_summary)})\n",
        "    print(f\"\\n🧠 LLM Insights:\\n{insights}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Error: {e}\")\n",
        "\n",
        "# Your turn: Build more complex chains!\n",
        "print(\"\\n🔧 Build your own advanced chains:\")\n",
        "print(\"   1. Sentiment analysis chain for individual reviews\")\n",
        "print(\"   2. Product recommendation chain\")\n",
        "print(\"   3. Multi-step analysis pipeline\")\n",
        "print(\"   4. Combine numerical and text analysis\")\n",
        "\n",
        "# Add your advanced chain code here!\n",
        "\n",
        "print(\"\\n✅ Advanced chain composition complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 2.3: LLM Model Comparison (10 minutes)\n",
        "\n",
        "**Your Task**: Compare different local LLM models and their outputs.\n",
        "\n",
        "**Instructions**:\n",
        "1. Set up multiple Ollama models\n",
        "2. Create a comparison chain\n",
        "3. Test the same prompt across models\n",
        "4. Analyze differences in responses\n",
        "5. Choose the best model for your use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 2.3: Your solution here!\n",
        "print(\"💻 Exercise 2.3: LLM Model Comparison\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Available models to test (install with: ollama pull <model>)\n",
        "available_models = [\n",
        "    \"llama3.2:1b\",   # Lightweight\n",
        "    \"llama3.2\",      # Standard\n",
        "    \"phi3\",          # Alternative\n",
        "]\n",
        "\n",
        "# Test prompt\n",
        "test_prompt = \"Explain the bias-variance tradeoff in machine learning in 2-3 sentences.\"\n",
        "\n",
        "print(f\"🧪 Testing prompt: {test_prompt}\")\n",
        "print(\"\\n📋 Model Comparison:\")\n",
        "\n",
        "# Test each available model\n",
        "model_responses = {}\n",
        "\n",
        "for model_name in available_models:\n",
        "    try:\n",
        "        print(f\"\\n🤖 Testing {model_name}...\")\n",
        "        \n",
        "        # Create model instance\n",
        "        test_model = ChatOllama(model=model_name, temperature=0.1)\n",
        "        \n",
        "        # Simple chain\n",
        "        simple_chain = test_model | StrOutputParser()\n",
        "        \n",
        "        # Get response\n",
        "        response = simple_chain.invoke([HumanMessage(content=test_prompt)])\n",
        "        model_responses[model_name] = response\n",
        "        \n",
        "        print(f\"   ✅ Response: {response}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error with {model_name}: {e}\")\n",
        "        print(f\"      Try: ollama pull {model_name}\")\n",
        "\n",
        "# Your turn: Analyze the differences!\n",
        "print(\"\\n🔧 Analysis tasks:\")\n",
        "print(\"   1. Compare response quality and accuracy\")\n",
        "print(\"   2. Measure response length and detail\")\n",
        "print(\"   3. Test with different types of prompts\")\n",
        "print(\"   4. Consider speed vs. quality tradeoffs\")\n",
        "print(\"   5. Choose the best model for your use case\")\n",
        "\n",
        "# Simple comparison\n",
        "if model_responses:\n",
        "    print(\"\\n📊 Quick Analysis:\")\n",
        "    for model, response in model_responses.items():\n",
        "        print(f\"   {model}: {len(response)} characters\")\n",
        "\n",
        "print(\"\\n✅ Model comparison complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Integration Workshop - Hybrid Pipelines (15 minutes)\n",
        "\n",
        "Now let's combine everything: Metaflow data processing with LangChain analysis!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 3.1: Building a Hybrid Pipeline (10 minutes)\n",
        "\n",
        "**Your Task**: Create a Metaflow pipeline that incorporates LangChain for intelligent data analysis.\n",
        "\n",
        "**Instructions**:\n",
        "1. Design a Metaflow flow with data preprocessing\n",
        "2. Add LangChain analysis steps\n",
        "3. Combine statistical analysis with LLM insights\n",
        "4. Generate automated reports\n",
        "5. Test the complete pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 3.1: Your solution here!\n",
        "print(\"💻 Exercise 3.1: Building a Hybrid Pipeline\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Define a hybrid Metaflow + LangChain pipeline\n",
        "class HybridAnalysisPipeline(FlowSpec):\n",
        "    \"\"\"\n",
        "    A hybrid pipeline combining Metaflow data processing \n",
        "    with LangChain LLM analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset_type = Parameter('dataset', default='reviews',\n",
        "                            help='Dataset to analyze: reviews or titanic')\n",
        "    \n",
        "    @step\n",
        "    def start(self):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline and load data\n",
        "        \"\"\"\n",
        "        print(\"🚀 Starting hybrid analysis pipeline\")\n",
        "        print(f\"   Dataset: {self.dataset_type}\")\n",
        "        \n",
        "        # Load appropriate dataset\n",
        "        if self.dataset_type == 'reviews':\n",
        "            self.data = reviews_df.copy()\n",
        "            print(f\"   Loaded {len(self.data)} reviews\")\n",
        "        else:\n",
        "            self.data = titanic_df.copy()\n",
        "            print(f\"   Loaded {len(self.data)} passenger records\")\n",
        "        \n",
        "        self.next(self.preprocess_data)\n",
        "    \n",
        "    @step\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Apply data preprocessing\n",
        "        \"\"\"\n",
        "        print(\"🔧 Preprocessing data...\")\n",
        "        \n",
        "        if self.dataset_type == 'reviews':\n",
        "            # Text preprocessing\n",
        "            self.data['clean_text'] = self.data['review_text'].apply(clean_text)\n",
        "            \n",
        "            # Create summary statistics\n",
        "            self.stats = {\n",
        "                'total_reviews': len(self.data),\n",
        "                'avg_rating': self.data['rating'].mean(),\n",
        "                'rating_distribution': self.data['rating'].value_counts().to_dict()\n",
        "            }\n",
        "        else:\n",
        "            # Handle missing values\n",
        "            self.data['Age'].fillna(self.data['Age'].median(), inplace=True)\n",
        "            self.data['Embarked'].fillna(self.data['Embarked'].mode()[0], inplace=True)\n",
        "            \n",
        "            # Create summary statistics\n",
        "            self.stats = {\n",
        "                'survival_rate': self.data['Survived'].mean(),\n",
        "                'avg_age': self.data['Age'].mean(),\n",
        "                'class_distribution': self.data['Pclass'].value_counts().to_dict()\n",
        "            }\n",
        "        \n",
        "        print(f\"   Preprocessing complete: {self.stats}\")\n",
        "        self.next(self.llm_analysis)\n",
        "    \n",
        "    @step \n",
        "    def llm_analysis(self):\n",
        "        \"\"\"\n",
        "        Use LangChain for intelligent analysis\n",
        "        \"\"\"\n",
        "        print(\"🧠 Running LLM analysis...\")\n",
        "        \n",
        "        try:\n",
        "            # Create analysis prompt\n",
        "            analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", \"You are an expert data analyst. Provide insights and recommendations.\"),\n",
        "                (\"human\", \"Analyze this data summary and provide 3 key insights: {data_summary}\")\n",
        "            ])\n",
        "            \n",
        "            # Create analysis chain\n",
        "            analysis_chain = analysis_prompt | model | output_parser\n",
        "            \n",
        "            # Generate insights\n",
        "            self.llm_insights = analysis_chain.invoke({\"data_summary\": str(self.stats)})\n",
        "            print(f\"   LLM Analysis: {self.llm_insights[:100]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   LLM analysis failed: {e}\")\n",
        "            self.llm_insights = \"LLM analysis unavailable\"\n",
        "        \n",
        "        self.next(self.generate_report)\n",
        "    \n",
        "    @step\n",
        "    def generate_report(self):\n",
        "        \"\"\"\n",
        "        Generate final analysis report\n",
        "        \"\"\"\n",
        "        print(\"📊 Generating final report...\")\n",
        "        \n",
        "        self.report = {\n",
        "            'dataset': self.dataset_type,\n",
        "            'data_shape': self.data.shape,\n",
        "            'statistics': self.stats,\n",
        "            'llm_insights': self.llm_insights,\n",
        "            'timestamp': pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        print(\"   Report generated successfully!\")\n",
        "        self.next(self.end)\n",
        "    \n",
        "    @step\n",
        "    def end(self):\n",
        "        \"\"\"\n",
        "        Pipeline completion\n",
        "        \"\"\"\n",
        "        print(\"🎉 Hybrid pipeline complete!\")\n",
        "        print(f\"   Final report: {len(str(self.report))} characters\")\n",
        "\n",
        "print(\"✅ Hybrid pipeline class defined!\")\n",
        "print(\"\\n🔧 Your turn:\")\n",
        "print(\"   1. Add more sophisticated preprocessing steps\")\n",
        "print(\"   2. Include multiple LLM analysis stages\")\n",
        "print(\"   3. Add data visualization generation\")\n",
        "print(\"   4. Implement error handling and logging\")\n",
        "print(\"   5. Create automated report formatting\")\n",
        "\n",
        "# Test the pipeline (comment out if Ollama not available)\n",
        "print(\"\\n🧪 Testing hybrid pipeline...\")\n",
        "print(\"   (Uncomment the code below to test)\")\n",
        "\n",
        "# Uncomment to test:\n",
        "# if __name__ == '__main__':\n",
        "#     HybridAnalysisPipeline()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercise 3.2: Production Considerations (5 minutes)\n",
        "\n",
        "**Your Task**: Discuss and implement production-ready patterns for hybrid pipelines.\n",
        "\n",
        "**Instructions**:\n",
        "1. Error handling and fallback strategies\n",
        "2. Model versioning and updates\n",
        "3. Monitoring and logging\n",
        "4. Scalability considerations\n",
        "5. Cost optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 EXERCISE 3.2: Your solution here!\n",
        "print(\"💻 Exercise 3.2: Production Considerations\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "print(\"🏭 Production-Ready Patterns:\")\n",
        "\n",
        "production_patterns = {\n",
        "    \"Error Handling\": [\n",
        "        \"Graceful LLM failures with fallback analysis\",\n",
        "        \"Retry logic for API calls\",\n",
        "        \"Data validation checkpoints\",\n",
        "        \"Circuit breaker patterns\"\n",
        "    ],\n",
        "    \"Monitoring\": [\n",
        "        \"Track LLM response times and quality\",\n",
        "        \"Monitor data drift in inputs\",\n",
        "        \"Log pipeline execution metrics\",\n",
        "        \"Alert on analysis anomalies\"\n",
        "    ],\n",
        "    \"Scalability\": [\n",
        "        \"Batch processing for large datasets\",\n",
        "        \"Async LLM calls for parallel processing\",\n",
        "        \"Caching for repeated analyses\",\n",
        "        \"Resource management and limits\"\n",
        "    ],\n",
        "    \"Cost Optimization\": [\n",
        "        \"Local models vs. API costs\",\n",
        "        \"Smart prompt engineering\",\n",
        "        \"Result caching strategies\",\n",
        "        \"Model size vs. accuracy tradeoffs\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, items in production_patterns.items():\n",
        "    print(f\"\\n   🎯 {category}:\")\n",
        "    for item in items:\n",
        "        print(f\"      • {item}\")\n",
        "\n",
        "# Example production-ready pattern\n",
        "print(\"\\n🔧 Example: Robust LLM Analysis Function\")\n",
        "\n",
        "def robust_llm_analysis(data_summary, max_retries=3, timeout=30):\n",
        "    \"\"\"\n",
        "    Production-ready LLM analysis with error handling\n",
        "    \"\"\"\n",
        "    import time\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Your LLM call here\n",
        "            # result = llm_chain.invoke({\"data\": data_summary})\n",
        "            \n",
        "            # Simulated response for demo\n",
        "            time.sleep(0.1)  # Simulate processing time\n",
        "            result = f\"Analysis attempt {attempt + 1}: Key insights from data summary\"\n",
        "            \n",
        "            # Validate response quality\n",
        "            if len(result) > 10:  # Basic validation\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'analysis': result,\n",
        "                    'attempt': attempt + 1\n",
        "                }\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   Attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'analysis': 'Automated statistical analysis only',\n",
        "                    'error': str(e)\n",
        "                }\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "# Test the robust function\n",
        "test_result = robust_llm_analysis(\"Sample data summary\")\n",
        "print(f\"   Test result: {test_result}\")\n",
        "\n",
        "print(\"\\n🔧 Your turn: Implement production patterns:\")\n",
        "print(\"   1. Add comprehensive logging\")\n",
        "print(\"   2. Implement result caching\")\n",
        "print(\"   3. Add performance monitoring\")\n",
        "print(\"   4. Create configuration management\")\n",
        "print(\"   5. Design automated testing\")\n",
        "\n",
        "print(\"\\n✅ Production considerations complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 Workshop Summary & Next Steps\n",
        "\n",
        "Congratulations! You've completed the Week 2 interactive workshop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🎓 WEEK 2 WORKSHOP COMPLETE!\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "print(\"🏆 What You've Accomplished:\")\n",
        "accomplishments = [\n",
        "    \"✅ Mastered advanced data preprocessing techniques\",\n",
        "    \"✅ Built sophisticated feature engineering pipelines\",\n",
        "    \"✅ Learned LangChain Expression Language (LCEL)\",\n",
        "    \"✅ Set up and used local LLMs with Ollama\",\n",
        "    \"✅ Created hybrid ML + LLM workflows\",\n",
        "    \"✅ Processed text data with NLP techniques\",\n",
        "    \"✅ Built production-ready patterns\"\n",
        "]\n",
        "\n",
        "for achievement in accomplishments:\n",
        "    print(f\"   {achievement}\")\n",
        "\n",
        "print(\"\\n🛠️ Key Skills Developed:\")\n",
        "skills = {\n",
        "    \"Data Processing\": [\"Advanced imputation\", \"Feature engineering\", \"Text preprocessing\", \"Pipeline validation\"],\n",
        "    \"LangChain/LLMs\": [\"LCEL syntax\", \"Chain composition\", \"Local model setup\", \"Prompt engineering\"],\n",
        "    \"Integration\": [\"Hybrid pipelines\", \"Error handling\", \"Production patterns\", \"Monitoring strategies\"]\n",
        "}\n",
        "\n",
        "for category, skill_list in skills.items():\n",
        "    print(f\"   🎯 {category}: {', '.join(skill_list)}\")\n",
        "\n",
        "print(\"\\n🚀 Coming in Week 3:\")\n",
        "week3_topics = [\n",
        "    \"Advanced LangChain patterns and agents\",\n",
        "    \"Vector databases and retrieval systems\",\n",
        "    \"RAG (Retrieval-Augmented Generation)\",\n",
        "    \"Building intelligent applications\",\n",
        "    \"End-to-end AI system deployment\"\n",
        "]\n",
        "\n",
        "for topic in week3_topics:\n",
        "    print(f\"   🔮 {topic}\")\n",
        "\n",
        "print(\"\\n💡 Practice Recommendations:\")\n",
        "practice_items = [\n",
        "    \"🔄 Apply preprocessing techniques to your own datasets\",\n",
        "    \"⚙️ Experiment with different LLM models and prompts\",\n",
        "    \"📊 Build domain-specific analysis chains\",\n",
        "    \"🏗️ Create production-ready pipeline templates\",\n",
        "    \"📚 Explore advanced LangChain documentation\"\n",
        "]\n",
        "\n",
        "for item in practice_items:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\n📋 Self-Study Checklist:\")\n",
        "checklist = [\n",
        "    \"□ Complete the additional exercises in /exercises/\",\n",
        "    \"□ Set up additional Ollama models for comparison\",\n",
        "    \"□ Build a custom preprocessing pipeline for your domain\",\n",
        "    \"□ Create a LangChain chain for a specific business problem\",\n",
        "    \"□ Review vector database concepts for Week 3\"\n",
        "]\n",
        "\n",
        "for item in checklist:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\n🎖️ Workshop Feedback:\")\n",
        "print(\"   💭 What was your favorite part of today's workshop?\")\n",
        "print(\"   🤔 Which concepts need more practice?\")\n",
        "print(\"   💡 What real-world applications are you excited to build?\")\n",
        "\n",
        "print(\"\\n🎉 Excellent work! You're ready for advanced AI applications!\")\n",
        "print(\"🏆 - INRIVA AI Academy Team\")\n",
        "\n",
        "# Save progress\n",
        "import json\n",
        "progress = {\n",
        "    'workshop': 'week2_interactive',\n",
        "    'completed': True,\n",
        "    'timestamp': pd.Timestamp.now().isoformat(),\n",
        "    'skills_learned': [skill for skill_list in skills.values() for skill in skill_list],\n",
        "    'next_steps': week3_topics\n",
        "}\n",
        "\n",
        "print(f\"\\n💾 Progress saved: {len(json.dumps(progress))} characters\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
