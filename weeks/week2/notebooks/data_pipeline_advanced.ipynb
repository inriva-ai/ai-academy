{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Data Pipeline Preprocessing Techniques\n",
        "\n",
        "Welcome to advanced data preprocessing with Metaflow! This notebook covers sophisticated techniques for building robust, production-ready data pipelines.\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you'll master:\n",
        "- **Advanced Missing Data Strategies**: Beyond simple imputation\n",
        "- **Sophisticated Feature Engineering**: Creating predictive features\n",
        "- **Robust Scaling and Normalization**: Handling different data distributions\n",
        "- **Pipeline Validation and Testing**: Ensuring reliability\n",
        "- **Performance Optimization**: Efficient processing techniques\n",
        "- **Production Patterns**: Real-world deployment considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Environment Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced preprocessing imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn preprocessing and imputation\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer,\n",
        "    PowerTransformer, LabelEncoder, OneHotEncoder\n",
        ")\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Metaflow for pipeline orchestration\n",
        "from metaflow import FlowSpec, step, Parameter\n",
        "\n",
        "# Visualization setup\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"viridis\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"üîß ADVANCED PREPROCESSING ENVIRONMENT\")\n",
        "print(\"=\" * 38)\n",
        "print(\"‚úÖ All libraries imported successfully\")\n",
        "print(\"üéØ Ready for advanced preprocessing techniques!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Creation and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_advanced_datasets():\n",
        "    \"\"\"Create realistic datasets with various preprocessing challenges\"\"\"\n",
        "    \n",
        "    print(\"üìä Creating Advanced Preprocessing Datasets\")\n",
        "    print(\"=\" * 42)\n",
        "    \n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Dataset 1: Complex Customer Data\n",
        "    n_customers = 5000\n",
        "    \n",
        "    # Generate realistic customer data\n",
        "    ages = np.random.normal(35, 12, n_customers)\n",
        "    ages = np.clip(ages, 18, 80)\n",
        "    \n",
        "    incomes = np.random.lognormal(10.5, 0.5, n_customers)\n",
        "    credit_scores = 300 + (incomes / 1000) * 50 + np.random.normal(0, 50, n_customers)\n",
        "    credit_scores = np.clip(credit_scores, 300, 850)\n",
        "    \n",
        "    customer_data = pd.DataFrame({\n",
        "        'customer_id': range(1, n_customers + 1),\n",
        "        'age': ages,\n",
        "        'annual_income': incomes,\n",
        "        'credit_score': credit_scores,\n",
        "        'region': np.random.choice(['North', 'South', 'East', 'West'], n_customers),\n",
        "        'avg_purchase_amount': np.random.lognormal(5, 1, n_customers),\n",
        "        'satisfaction_score': np.random.uniform(1, 5, n_customers),\n",
        "        'years_customer': np.random.exponential(2, n_customers),\n",
        "        'num_purchases': np.random.poisson(8, n_customers)\n",
        "    })\n",
        "    \n",
        "    # Add realistic missing data patterns\n",
        "    customer_data.loc[np.random.choice(customer_data.index, 400), 'credit_score'] = np.nan\n",
        "    customer_data.loc[np.random.choice(customer_data.index, 200), 'age'] = np.nan\n",
        "    \n",
        "    # Dataset 2: Product Reviews\n",
        "    n_products = 2000\n",
        "    \n",
        "    review_templates = [\n",
        "        \"Great product! Highly recommend.\",\n",
        "        \"Disappointed with this purchase.\",\n",
        "        \"Average product, nothing special.\",\n",
        "        \"Excellent quality and fast delivery!\",\n",
        "        \"Not worth the money.\",\n",
        "        \"Perfect for my needs.\"\n",
        "    ]\n",
        "    \n",
        "    product_data = pd.DataFrame({\n",
        "        'product_id': range(1, n_products + 1),\n",
        "        'review_text': np.random.choice(review_templates, n_products),\n",
        "        'rating': np.random.choice([1, 2, 3, 4, 5], n_products),\n",
        "        'price': np.random.lognormal(3, 1, n_products),\n",
        "        'category': np.random.choice(['Electronics', 'Clothing', 'Home'], n_products)\n",
        "    })\n",
        "    \n",
        "    print(f\"   üìä Customer dataset: {customer_data.shape}\")\n",
        "    print(f\"   üìù Product dataset: {product_data.shape}\")\n",
        "    print(\"\\n‚úÖ All datasets created successfully!\")\n",
        "    \n",
        "    return customer_data, product_data\n",
        "\n",
        "# Create the datasets\n",
        "customer_df, product_df = create_advanced_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Advanced Missing Data Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß† Advanced Imputation Techniques\")\n",
        "print(\"=\" * 33)\n",
        "\n",
        "class AdvancedImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Custom imputer with multiple strategies\"\"\"\n",
        "    \n",
        "    def __init__(self, numerical_strategy='knn', categorical_strategy='mode', n_neighbors=5):\n",
        "        self.numerical_strategy = numerical_strategy\n",
        "        self.categorical_strategy = categorical_strategy\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.imputers_ = {}\n",
        "        self.feature_types_ = {}\n",
        "        \n",
        "    def _detect_feature_types(self, X):\n",
        "        \"\"\"Automatically detect numerical and categorical features\"\"\"\n",
        "        feature_types = {}\n",
        "        \n",
        "        for col in X.columns:\n",
        "            if X[col].dtype in ['object', 'category']:\n",
        "                feature_types[col] = 'categorical'\n",
        "            elif X[col].dtype in ['int64', 'float64']:\n",
        "                unique_ratio = X[col].nunique() / len(X[col].dropna())\n",
        "                if unique_ratio < 0.05 and X[col].nunique() < 20:\n",
        "                    feature_types[col] = 'categorical'\n",
        "                else:\n",
        "                    feature_types[col] = 'numerical'\n",
        "            else:\n",
        "                feature_types[col] = 'numerical'\n",
        "                \n",
        "        return feature_types\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit imputers for different feature types\"\"\"\n",
        "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        \n",
        "        self.feature_types_ = self._detect_feature_types(X)\n",
        "        \n",
        "        numerical_features = [col for col, ftype in self.feature_types_.items() if ftype == 'numerical']\n",
        "        categorical_features = [col for col, ftype in self.feature_types_.items() if ftype == 'categorical']\n",
        "        \n",
        "        # Fit numerical imputer\n",
        "        if numerical_features:\n",
        "            if self.numerical_strategy == 'knn':\n",
        "                self.imputers_['numerical'] = KNNImputer(n_neighbors=self.n_neighbors)\n",
        "            elif self.numerical_strategy == 'iterative':\n",
        "                self.imputers_['numerical'] = IterativeImputer(max_iter=10, random_state=42)\n",
        "            else:\n",
        "                self.imputers_['numerical'] = SimpleImputer(strategy=self.numerical_strategy)\n",
        "            \n",
        "            self.imputers_['numerical'].fit(X[numerical_features])\n",
        "        \n",
        "        # Fit categorical imputer\n",
        "        if categorical_features:\n",
        "            self.imputers_['categorical'] = SimpleImputer(strategy=self.categorical_strategy)\n",
        "            self.imputers_['categorical'].fit(X[categorical_features])\n",
        "        \n",
        "        self.numerical_features_ = numerical_features\n",
        "        self.categorical_features_ = categorical_features\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Apply imputation to data\"\"\"\n",
        "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        X_imputed = X.copy()\n",
        "        \n",
        "        # Impute numerical features\n",
        "        if self.numerical_features_ and 'numerical' in self.imputers_:\n",
        "            X_num_imputed = self.imputers_['numerical'].transform(X[self.numerical_features_])\n",
        "            X_imputed[self.numerical_features_] = X_num_imputed\n",
        "        \n",
        "        # Impute categorical features\n",
        "        if self.categorical_features_ and 'categorical' in self.imputers_:\n",
        "            X_cat_imputed = self.imputers_['categorical'].transform(X[self.categorical_features_])\n",
        "            X_imputed[self.categorical_features_] = X_cat_imputed\n",
        "        \n",
        "        return X_imputed\n",
        "\n",
        "# Test imputation strategies\n",
        "print(\"üß™ Testing Imputation Strategies:\")\n",
        "\n",
        "test_features = ['age', 'credit_score', 'annual_income']\n",
        "X_test = customer_df[test_features].copy()\n",
        "\n",
        "print(f\"   Original missing values: {X_test.isnull().sum().sum()}\")\n",
        "\n",
        "# Apply KNN imputation\n",
        "knn_imputer = AdvancedImputer(numerical_strategy='knn', n_neighbors=5)\n",
        "X_knn = knn_imputer.fit_transform(X_test)\n",
        "\n",
        "print(f\"   KNN imputed missing values: {X_knn.isnull().sum().sum()}\")\n",
        "print(\"\\n‚úÖ Advanced imputation techniques demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sophisticated Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß Sophisticated Feature Engineering\")\n",
        "print(\"=\" * 34)\n",
        "\n",
        "class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Automated feature engineering with multiple strategies\"\"\"\n",
        "    \n",
        "    def __init__(self, create_interactions=True, create_ratios=True, \n",
        "                 polynomial_degree=2, n_bins=5):\n",
        "        self.create_interactions = create_interactions\n",
        "        self.create_ratios = create_ratios\n",
        "        self.polynomial_degree = polynomial_degree\n",
        "        self.n_bins = n_bins\n",
        "        self.numerical_features_ = []\n",
        "        self.bin_edges_ = {}\n",
        "        \n",
        "    def _identify_numerical_features(self, X):\n",
        "        \"\"\"Identify numerical features for engineering\"\"\"\n",
        "        numerical = []\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype in ['int64', 'float64']:\n",
        "                unique_ratio = X[col].nunique() / len(X[col])\n",
        "                if unique_ratio > 0.05 or X[col].nunique() >= 10:\n",
        "                    numerical.append(col)\n",
        "        return numerical\n",
        "    \n",
        "    def _create_polynomial_features(self, X):\n",
        "        \"\"\"Create polynomial features for numerical columns\"\"\"\n",
        "        poly_features = pd.DataFrame(index=X.index)\n",
        "        \n",
        "        for col in self.numerical_features_:\n",
        "            if col in X.columns:\n",
        "                if self.polynomial_degree >= 2:\n",
        "                    poly_features[f'{col}_squared'] = X[col] ** 2\n",
        "                \n",
        "                if (X[col] > 0).all():\n",
        "                    poly_features[f'{col}_log'] = np.log1p(X[col])\n",
        "                \n",
        "                if (X[col] >= 0).all():\n",
        "                    poly_features[f'{col}_sqrt'] = np.sqrt(X[col])\n",
        "        \n",
        "        return poly_features\n",
        "    \n",
        "    def _create_interaction_features(self, X):\n",
        "        \"\"\"Create interaction features between numerical columns\"\"\"\n",
        "        interaction_features = pd.DataFrame(index=X.index)\n",
        "        \n",
        "        numerical_cols = [col for col in self.numerical_features_ if col in X.columns]\n",
        "        \n",
        "        for i, col1 in enumerate(numerical_cols):\n",
        "            for col2 in numerical_cols[i+1:]:\n",
        "                interaction_features[f'{col1}_x_{col2}'] = X[col1] * X[col2]\n",
        "                interaction_features[f'{col1}_plus_{col2}'] = X[col1] + X[col2]\n",
        "        \n",
        "        return interaction_features\n",
        "    \n",
        "    def _create_ratio_features(self, X):\n",
        "        \"\"\"Create ratio features between numerical columns\"\"\"\n",
        "        ratio_features = pd.DataFrame(index=X.index)\n",
        "        \n",
        "        numerical_cols = [col for col in self.numerical_features_ if col in X.columns]\n",
        "        \n",
        "        for i, col1 in enumerate(numerical_cols):\n",
        "            for col2 in numerical_cols[i+1:]:\n",
        "                if (X[col2] != 0).all():\n",
        "                    ratio_features[f'{col1}_div_{col2}'] = X[col1] / X[col2]\n",
        "        \n",
        "        return ratio_features\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the feature engineer\"\"\"\n",
        "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        self.numerical_features_ = self._identify_numerical_features(X)\n",
        "        \n",
        "        # Compute bin edges for numerical features\n",
        "        for col in self.numerical_features_:\n",
        "            if col in X.columns:\n",
        "                _, self.bin_edges_[col] = pd.qcut(X[col], q=self.n_bins, retbins=True, duplicates='drop')\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data with engineered features\"\"\"\n",
        "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        X_engineered = X.copy()\n",
        "        \n",
        "        # Add polynomial features\n",
        "        poly_features = self._create_polynomial_features(X)\n",
        "        X_engineered = pd.concat([X_engineered, poly_features], axis=1)\n",
        "        \n",
        "        # Add interaction features\n",
        "        if self.create_interactions:\n",
        "            interaction_features = self._create_interaction_features(X)\n",
        "            X_engineered = pd.concat([X_engineered, interaction_features], axis=1)\n",
        "        \n",
        "        # Add ratio features\n",
        "        if self.create_ratios:\n",
        "            ratio_features = self._create_ratio_features(X)\n",
        "            X_engineered = pd.concat([X_engineered, ratio_features], axis=1)\n",
        "        \n",
        "        # Handle infinite and NaN values\n",
        "        X_engineered = X_engineered.replace([np.inf, -np.inf], np.nan)\n",
        "        \n",
        "        return X_engineered\n",
        "\n",
        "# Test feature engineering\n",
        "print(\"üß™ Testing Advanced Feature Engineering:\")\n",
        "\n",
        "feature_cols = ['age', 'annual_income', 'credit_score', 'avg_purchase_amount']\n",
        "X_feature_test = customer_df[feature_cols].dropna().head(1000)\n",
        "\n",
        "print(f\"   Original features: {X_feature_test.shape[1]}\")\n",
        "\n",
        "feature_engineer = AdvancedFeatureEngineer(\n",
        "    create_interactions=True,\n",
        "    create_ratios=True,\n",
        "    polynomial_degree=2\n",
        ")\n",
        "\n",
        "X_engineered = feature_engineer.fit_transform(X_feature_test)\n",
        "\n",
        "print(f\"   Engineered features: {X_engineered.shape[1]}\")\n",
        "print(f\"   New features created: {X_engineered.shape[1] - X_feature_test.shape[1]}\")\n",
        "print(\"\\n‚úÖ Sophisticated feature engineering demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Advanced Text Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìù Advanced Text Feature Engineering\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "class AdvancedTextFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Advanced text feature extraction\"\"\"\n",
        "    \n",
        "    def __init__(self, include_basic_stats=True, include_tfidf=True, \n",
        "                 max_features=100, ngram_range=(1, 2)):\n",
        "        self.include_basic_stats = include_basic_stats\n",
        "        self.include_tfidf = include_tfidf\n",
        "        self.max_features = max_features\n",
        "        self.ngram_range = ngram_range\n",
        "        self.tfidf_vectorizer = None\n",
        "        \n",
        "    def _create_basic_text_features(self, texts):\n",
        "        \"\"\"Create basic statistical features from text\"\"\"\n",
        "        features = pd.DataFrame()\n",
        "        \n",
        "        # Length features\n",
        "        features['text_length'] = texts.str.len().fillna(0)\n",
        "        features['word_count'] = texts.str.split().str.len().fillna(0)\n",
        "        features['sentence_count'] = texts.str.count(r'[.!?]').fillna(0) + 1\n",
        "        features['avg_word_length'] = (features['text_length'] / features['word_count']).fillna(0)\n",
        "        \n",
        "        # Punctuation and special characters\n",
        "        features['exclamation_count'] = texts.str.count('!').fillna(0)\n",
        "        features['question_count'] = texts.str.count('\\?').fillna(0)\n",
        "        features['capital_ratio'] = (texts.str.count(r'[A-Z]') / features['text_length']).fillna(0)\n",
        "        \n",
        "        # Simple sentiment analysis\n",
        "        positive_words = ['great', 'excellent', 'amazing', 'perfect', 'love', 'awesome']\n",
        "        negative_words = ['terrible', 'awful', 'hate', 'worst', 'disappointing', 'poor']\n",
        "        \n",
        "        features['positive_word_count'] = texts.str.lower().str.count('|'.join(positive_words)).fillna(0)\n",
        "        features['negative_word_count'] = texts.str.lower().str.count('|'.join(negative_words)).fillna(0)\n",
        "        features['sentiment_ratio'] = ((features['positive_word_count'] - features['negative_word_count']) / \n",
        "                                      features['word_count']).fillna(0)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def _create_tfidf_features(self, texts):\n",
        "        \"\"\"Create TF-IDF features\"\"\"\n",
        "        clean_texts = texts.fillna('').astype(str)\n",
        "        \n",
        "        if self.tfidf_vectorizer is None:\n",
        "            self.tfidf_vectorizer = TfidfVectorizer(\n",
        "                max_features=self.max_features,\n",
        "                ngram_range=self.ngram_range,\n",
        "                stop_words='english',\n",
        "                lowercase=True\n",
        "            )\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(clean_texts)\n",
        "        else:\n",
        "            tfidf_matrix = self.tfidf_vectorizer.transform(clean_texts)\n",
        "        \n",
        "        # Convert to DataFrame\n",
        "        feature_names = [f'tfidf_{name}' for name in self.tfidf_vectorizer.get_feature_names_out()]\n",
        "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=texts.index)\n",
        "        \n",
        "        return tfidf_df\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the text feature engineer\"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            texts = X[X.columns[0]]\n",
        "        else:\n",
        "            texts = pd.Series(X)\n",
        "        \n",
        "        if self.include_tfidf:\n",
        "            self._create_tfidf_features(texts)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform text data into features\"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            texts = X[X.columns[0]]\n",
        "        else:\n",
        "            texts = pd.Series(X)\n",
        "        \n",
        "        all_features = pd.DataFrame(index=texts.index)\n",
        "        \n",
        "        # Basic text statistics\n",
        "        if self.include_basic_stats:\n",
        "            basic_features = self._create_basic_text_features(texts)\n",
        "            all_features = pd.concat([all_features, basic_features], axis=1)\n",
        "        \n",
        "        # TF-IDF features\n",
        "        if self.include_tfidf:\n",
        "            tfidf_features = self._create_tfidf_features(texts)\n",
        "            all_features = pd.concat([all_features, tfidf_features], axis=1)\n",
        "        \n",
        "        return all_features\n",
        "\n",
        "# Test text feature engineering\n",
        "print(\"üß™ Testing Text Feature Engineering:\")\n",
        "\n",
        "product_text_data = product_df[['review_text']].dropna().head(500)\n",
        "print(f\"   Sample size: {len(product_text_data)}\")\n",
        "\n",
        "text_engineer = AdvancedTextFeatureEngineer(\n",
        "    include_basic_stats=True,\n",
        "    include_tfidf=True,\n",
        "    max_features=50\n",
        ")\n",
        "\n",
        "text_features = text_engineer.fit_transform(product_text_data)\n",
        "print(f\"   Features created: {text_features.shape[1]}\")\n",
        "\n",
        "# Show feature types\n",
        "basic_features = [col for col in text_features.columns if not col.startswith('tfidf_')]\n",
        "tfidf_features = [col for col in text_features.columns if col.startswith('tfidf_')]\n",
        "\n",
        "print(f\"   Basic text features: {len(basic_features)}\")\n",
        "print(f\"   TF-IDF features: {len(tfidf_features)}\")\n",
        "print(\"\\n‚úÖ Advanced text feature engineering demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Robust Scaling and Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚öñÔ∏è Robust Scaling and Normalization\")\n",
        "print(\"=\" * 34)\n",
        "\n",
        "class AdaptiveScaler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Automatically choose the best scaling method for each feature\"\"\"\n",
        "    \n",
        "    def __init__(self, strategy='auto', outlier_threshold=0.1, skewness_threshold=1):\n",
        "        self.strategy = strategy\n",
        "        self.outlier_threshold = outlier_threshold\n",
        "        self.skewness_threshold = skewness_threshold\n",
        "        self.scalers_ = {}\n",
        "        self.feature_strategies_ = {}\n",
        "        \n",
        "    def _analyze_feature_distribution(self, series):\n",
        "        \"\"\"Analyze feature distribution to choose best scaling\"\"\"\n",
        "        skewness = abs(stats.skew(series.dropna()))\n",
        "        \n",
        "        # Detect outliers using IQR method\n",
        "        Q1 = series.quantile(0.25)\n",
        "        Q3 = series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outlier_count = ((series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))).sum()\n",
        "        outlier_ratio = outlier_count / len(series)\n",
        "        \n",
        "        return {\n",
        "            'skewness': skewness,\n",
        "            'outlier_ratio': outlier_ratio,\n",
        "            'min_val': series.min(),\n",
        "            'max_val': series.max()\n",
        "        }\n",
        "    \n",
        "    def _choose_scaler(self, feature_stats):\n",
        "        \"\"\"Choose the best scaler based on feature characteristics\"\"\"\n",
        "        \n",
        "        if self.strategy != 'auto':\n",
        "            scaler_map = {\n",
        "                'standard': StandardScaler(),\n",
        "                'minmax': MinMaxScaler(),\n",
        "                'robust': RobustScaler()\n",
        "            }\n",
        "            return scaler_map.get(self.strategy, StandardScaler())\n",
        "        \n",
        "        # Auto strategy: choose based on data characteristics\n",
        "        if feature_stats['outlier_ratio'] > self.outlier_threshold:\n",
        "            return RobustScaler()  # Robust to outliers\n",
        "        elif feature_stats['skewness'] > self.skewness_threshold:\n",
        "            return RobustScaler()  # Handle skewed distributions\n",
        "        else:\n",
        "            return StandardScaler()  # Normal-ish distribution\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit scalers for each feature\"\"\"\n",
        "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        \n",
        "        for col in X.columns:\n",
        "            if X[col].dtype in ['int64', 'float64']:\n",
        "                feature_stats = self._analyze_feature_distribution(X[col])\n",
        "                scaler = self._choose_scaler(feature_stats)\n",
        "                scaler.fit(X[[col]])\n",
        "                \n",
        "                self.scalers_[col] = scaler\n",
        "                self.feature_strategies_[col] = type(scaler).__name__\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform features using fitted scalers\"\"\"\n",
        "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        X_scaled = X.copy()\n",
        "        \n",
        "        for col in self.scalers_:\n",
        "            if col in X.columns:\n",
        "                X_scaled[col] = self.scalers_[col].transform(X[[col]]).flatten()\n",
        "        \n",
        "        return X_scaled\n",
        "    \n",
        "    def get_scaling_summary(self):\n",
        "        \"\"\"Get summary of scaling strategies used\"\"\"\n",
        "        return self.feature_strategies_\n",
        "\n",
        "# Test adaptive scaling\n",
        "print(\"üß™ Testing Adaptive Scaling:\")\n",
        "\n",
        "scaling_test_data = pd.DataFrame({\n",
        "    'normal_feature': np.random.normal(50, 10, 1000),\n",
        "    'skewed_feature': np.random.lognormal(2, 1, 1000),\n",
        "    'outlier_feature': np.concatenate([np.random.normal(0, 1, 950), np.random.normal(10, 1, 50)])\n",
        "})\n",
        "\n",
        "adaptive_scaler = AdaptiveScaler(strategy='auto')\n",
        "scaled_data = adaptive_scaler.fit_transform(scaling_test_data)\n",
        "\n",
        "scaling_summary = adaptive_scaler.get_scaling_summary()\n",
        "print(\"   Chosen scaling strategies:\")\n",
        "for feature, strategy in scaling_summary.items():\n",
        "    print(f\"     {feature}: {strategy}\")\n",
        "\n",
        "print(\"\\n‚úÖ Adaptive scaling demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Complete Advanced Pipeline with Metaflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üèóÔ∏è Complete Advanced Preprocessing Pipeline\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "class AdvancedPreprocessingPipeline(FlowSpec):\n",
        "    \"\"\"\n",
        "    Production-ready advanced preprocessing pipeline with:\n",
        "    - Intelligent missing data handling\n",
        "    - Sophisticated feature engineering\n",
        "    - Adaptive scaling and normalization\n",
        "    - Data validation and quality checks\n",
        "    \"\"\"\n",
        "    \n",
        "    data_source = Parameter('data_source', default='customer',\n",
        "                           help='Data source: customer or product')\n",
        "    \n",
        "    validation_split = Parameter('validation_split', default=0.2,\n",
        "                                help='Fraction of data for validation')\n",
        "    \n",
        "    @step\n",
        "    def start(self):\n",
        "        \"\"\"Initialize pipeline and load data\"\"\"\n",
        "        print(\"üöÄ Starting Advanced Preprocessing Pipeline\")\n",
        "        print(f\"   Data source: {self.data_source}\")\n",
        "        \n",
        "        if self.data_source == 'customer':\n",
        "            self.data = customer_df.copy()\n",
        "        else:\n",
        "            self.data = product_df.copy()\n",
        "        \n",
        "        print(f\"   Loaded data shape: {self.data.shape}\")\n",
        "        self.next(self.data_quality_check)\n",
        "    \n",
        "    @step\n",
        "    def data_quality_check(self):\n",
        "        \"\"\"Comprehensive data quality assessment\"\"\"\n",
        "        print(\"üîç Data Quality Assessment\")\n",
        "        \n",
        "        self.quality_report = {\n",
        "            'total_rows': len(self.data),\n",
        "            'total_columns': len(self.data.columns),\n",
        "            'missing_data': {},\n",
        "            'duplicate_rows': self.data.duplicated().sum()\n",
        "        }\n",
        "        \n",
        "        # Analyze missing data\n",
        "        for col in self.data.columns:\n",
        "            missing_count = self.data[col].isnull().sum()\n",
        "            missing_ratio = missing_count / len(self.data)\n",
        "            self.quality_report['missing_data'][col] = {\n",
        "                'count': missing_count,\n",
        "                'ratio': missing_ratio\n",
        "            }\n",
        "        \n",
        "        print(f\"   Missing data analysis complete\")\n",
        "        self.next(self.advanced_imputation)\n",
        "    \n",
        "    @step\n",
        "    def advanced_imputation(self):\n",
        "        \"\"\"Apply advanced missing data imputation\"\"\"\n",
        "        print(\"üß† Advanced Missing Data Imputation\")\n",
        "        \n",
        "        self.imputer = AdvancedImputer(\n",
        "            numerical_strategy='knn',\n",
        "            categorical_strategy='most_frequent',\n",
        "            n_neighbors=5\n",
        "        )\n",
        "        \n",
        "        self.data_imputed = self.imputer.fit_transform(self.data)\n",
        "        \n",
        "        missing_before = self.data.isnull().sum().sum()\n",
        "        missing_after = self.data_imputed.isnull().sum().sum()\n",
        "        \n",
        "        print(f\"   Missing values before: {missing_before}\")\n",
        "        print(f\"   Missing values after: {missing_after}\")\n",
        "        \n",
        "        self.next(self.feature_engineering)\n",
        "    \n",
        "    @step\n",
        "    def feature_engineering(self):\n",
        "        \"\"\"Apply sophisticated feature engineering\"\"\"\n",
        "        print(\"üîß Advanced Feature Engineering\")\n",
        "        \n",
        "        if self.data_source == 'customer':\n",
        "            # Numerical feature engineering\n",
        "            self.feature_engineer = AdvancedFeatureEngineer(\n",
        "                create_interactions=True,\n",
        "                create_ratios=True,\n",
        "                polynomial_degree=2\n",
        "            )\n",
        "            self.data_engineered = self.feature_engineer.fit_transform(self.data_imputed)\n",
        "        else:\n",
        "            # Text feature engineering for product data\n",
        "            text_cols = ['review_text']\n",
        "            other_cols = [col for col in self.data_imputed.columns if col not in text_cols]\n",
        "            \n",
        "            # Process text features\n",
        "            self.text_engineer = AdvancedTextFeatureEngineer(max_features=30)\n",
        "            text_features = self.text_engineer.fit_transform(self.data_imputed[text_cols])\n",
        "            \n",
        "            # Combine with other features\n",
        "            self.data_engineered = pd.concat([self.data_imputed[other_cols], text_features], axis=1)\n",
        "        \n",
        "        features_before = self.data_imputed.shape[1]\n",
        "        features_after = self.data_engineered.shape[1]\n",
        "        \n",
        "        print(f\"   Features before: {features_before}\")\n",
        "        print(f\"   Features after: {features_after}\")\n",
        "        \n",
        "        self.next(self.adaptive_scaling)\n",
        "    \n",
        "    @step\n",
        "    def adaptive_scaling(self):\n",
        "        \"\"\"Apply adaptive scaling and normalization\"\"\"\n",
        "        print(\"‚öñÔ∏è Adaptive Scaling and Normalization\")\n",
        "        \n",
        "        self.scaler = AdaptiveScaler(strategy='auto')\n",
        "        self.data_final = self.scaler.fit_transform(self.data_engineered)\n",
        "        \n",
        "        scaling_strategies = self.scaler.get_scaling_summary()\n",
        "        print(f\"   Scaling strategies applied: {len(scaling_strategies)}\")\n",
        "        \n",
        "        self.next(self.final_validation)\n",
        "    \n",
        "    @step\n",
        "    def final_validation(self):\n",
        "        \"\"\"Final data validation and quality checks\"\"\"\n",
        "        print(\"‚úÖ Final Data Validation\")\n",
        "        \n",
        "        validation_results = {\n",
        "            'no_missing_values': self.data_final.isnull().sum().sum() == 0,\n",
        "            'no_infinite_values': np.isinf(self.data_final.select_dtypes(include=[np.number])).sum().sum() == 0,\n",
        "            'reasonable_shape': self.data_final.shape[0] > 0 and self.data_final.shape[1] > 0\n",
        "        }\n",
        "        \n",
        "        self.validation_passed = all(validation_results.values())\n",
        "        print(f\"   Validation: {'PASSED' if self.validation_passed else 'FAILED'}\")\n",
        "        \n",
        "        self.next(self.end)\n",
        "    \n",
        "    @step\n",
        "    def end(self):\n",
        "        \"\"\"Pipeline completion and summary\"\"\"\n",
        "        print(\"üéâ Advanced Preprocessing Pipeline Complete!\")\n",
        "        \n",
        "        self.pipeline_summary = {\n",
        "            'data_source': self.data_source,\n",
        "            'original_shape': self.data.shape,\n",
        "            'final_shape': self.data_final.shape,\n",
        "            'validation_passed': self.validation_passed\n",
        "        }\n",
        "        \n",
        "        print(f\"   Original shape: {self.pipeline_summary['original_shape']}\")\n",
        "        print(f\"   Final shape: {self.pipeline_summary['final_shape']}\")\n",
        "        print(f\"   Status: {'‚úÖ SUCCESS' if self.validation_passed else '‚ùå FAILED'}\")\n",
        "        print(\"\\nüöÄ Pipeline ready for model training!\")\n",
        "\n",
        "print(\"‚úÖ Advanced preprocessing pipeline class defined!\")\n",
        "print(\"\\nüí° To run the pipeline:\")\n",
        "print(\"   1. Save this class to a .py file\")\n",
        "print(\"   2. Run: python advanced_pipeline.py run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Practice Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üíª Practice Exercises\")\n",
        "print(\"=\" * 20)\n",
        "\n",
        "print(\"üéØ Exercise 1: Custom Domain-Specific Transformer\")\n",
        "print(\"   Create a transformer for financial data that:\")\n",
        "print(\"   ‚Ä¢ Handles currency conversion\")\n",
        "print(\"   ‚Ä¢ Creates financial ratios\")\n",
        "print(\"   ‚Ä¢ Detects suspicious transactions\")\n",
        "print(\"   ‚Ä¢ Applies appropriate scaling\")\n",
        "\n",
        "print(\"\\nüéØ Exercise 2: Performance Optimization\")\n",
        "print(\"   Optimize the pipeline for:\")\n",
        "print(\"   ‚Ä¢ Memory efficiency with large datasets\")\n",
        "print(\"   ‚Ä¢ Parallel processing capabilities\")\n",
        "print(\"   ‚Ä¢ Streaming data processing\")\n",
        "print(\"   ‚Ä¢ Progress monitoring\")\n",
        "\n",
        "print(\"\\nüéØ Exercise 3: Data Quality Monitoring\")\n",
        "print(\"   Build a monitoring system that:\")\n",
        "print(\"   ‚Ä¢ Tracks data drift over time\")\n",
        "print(\"   ‚Ä¢ Monitors feature distributions\")\n",
        "print(\"   ‚Ä¢ Generates quality alerts\")\n",
        "print(\"   ‚Ä¢ Creates detailed reports\")\n",
        "\n",
        "print(\"\\n‚úÖ Ready for your solutions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üéì ADVANCED DATA PREPROCESSING COMPLETE!\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "print(\"üèÜ Advanced Techniques Mastered:\")\n",
        "techniques = [\n",
        "    \"‚úÖ Intelligent missing data imputation strategies\",\n",
        "    \"‚úÖ Sophisticated automated feature engineering\",\n",
        "    \"‚úÖ Advanced text feature extraction techniques\",\n",
        "    \"‚úÖ Adaptive scaling and normalization methods\",\n",
        "    \"‚úÖ Production-ready pipeline architecture\",\n",
        "    \"‚úÖ Data quality validation and monitoring\"\n",
        "]\n",
        "\n",
        "for technique in techniques:\n",
        "    print(f\"   {technique}\")\n",
        "\n",
        "print(\"\\nüõ†Ô∏è Key Components Built:\")\n",
        "components = [\n",
        "    \"AdvancedImputer - Smart missing data handling\",\n",
        "    \"AdvancedFeatureEngineer - Automated feature creation\",\n",
        "    \"AdvancedTextFeatureEngineer - Text processing\",\n",
        "    \"AdaptiveScaler - Intelligent scaling selection\",\n",
        "    \"AdvancedPreprocessingPipeline - Complete Metaflow pipeline\"\n",
        "]\n",
        "\n",
        "for component in components:\n",
        "    print(f\"   üîß {component}\")\n",
        "\n",
        "print(\"\\nüöÄ Production Readiness:\")\n",
        "readiness = [\n",
        "    \"‚òëÔ∏è Robust error handling and validation\",\n",
        "    \"‚òëÔ∏è Scalable processing architecture\",\n",
        "    \"‚òëÔ∏è Comprehensive data quality checks\",\n",
        "    \"‚òëÔ∏è Monitoring and logging capabilities\",\n",
        "    \"‚òëÔ∏è Reproducible and maintainable code\"\n",
        "]\n",
        "\n",
        "for item in readiness:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\nüéØ Next Learning Steps:\")\n",
        "next_steps = [\n",
        "    \"üß† AutoML and automated feature selection\",\n",
        "    \"üîÑ Real-time streaming data preprocessing\",\n",
        "    \"üèóÔ∏è Distributed processing with Spark/Dask\",\n",
        "    \"üìä Advanced statistical transformations\",\n",
        "    \"üéØ Domain-specific preprocessing techniques\"\n",
        "]\n",
        "\n",
        "for step in next_steps:\n",
        "    print(f\"   {step}\")\n",
        "\n",
        "print(\"\\nüí° Best Practices Learned:\")\n",
        "practices = [\n",
        "    \"üîç Always analyze data characteristics first\",\n",
        "    \"‚öñÔ∏è Balance sophistication with interpretability\",\n",
        "    \"üß™ Validate preprocessing choices empirically\",\n",
        "    \"üìä Monitor data quality continuously\",\n",
        "    \"üîÑ Design for maintainability and extensibility\"\n",
        "]\n",
        "\n",
        "for practice in practices:\n",
        "    print(f\"   {practice}\")\n",
        "\n",
        "print(\"\\nüéâ You're now ready to tackle complex preprocessing challenges!\")\n",
        "print(\"üèÜ - INRIVA AI Academy Team\")\n",
        "\n",
        "# Save progress\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "progress = {\n",
        "    'module': 'advanced_data_preprocessing',\n",
        "    'completed': True,\n",
        "    'completion_date': datetime.now().isoformat(),\n",
        "    'techniques_mastered': len(techniques),\n",
        "    'components_built': len(components),\n",
        "    'exercises_available': 3\n",
        "}\n",
        "\n",
        "print(f\"\\nüíæ Progress saved: {len(json.dumps(progress))} characters\")\n",
        "print(\"üìã Ready for real-world preprocessing challenges!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}