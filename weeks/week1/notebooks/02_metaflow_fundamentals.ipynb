{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaflow Fundamentals - Week 1 Workshop\n",
    "\n",
    "Welcome to the core of our MLOps journey! In this notebook, we'll explore Metaflow - the foundation that makes machine learning workflows reproducible, scalable, and production-ready.\n",
    "\n",
    "## What is Metaflow?\n",
    "\n",
    "Metaflow solves three critical problems in ML development:\n",
    "1. **Versioning**: Your code, data, and results\n",
    "2. **Scalability**: From laptop to cloud seamlessly  \n",
    "3. **Reproducibility**: Anyone can rerun your exact experiment\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Your First Metaflow Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Metaflow components\n",
    "from metaflow import FlowSpec, step\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class WorkshopIntroFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    Our first Metaflow workflow - demonstrates core concepts\n",
    "    \"\"\"\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Initialize our workflow with sample data\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Starting our first Metaflow workflow!\")\n",
    "        \n",
    "        # Create sample data\n",
    "        np.random.seed(42)\n",
    "        self.sample_data = {\n",
    "            'values': np.random.normal(100, 15, 1000),\n",
    "            'categories': np.random.choice(['A', 'B', 'C'], 1000),\n",
    "            'timestamps': pd.date_range('2024-01-01', periods=1000)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(self.sample_data['values'])} data points\")\n",
    "        self.next(self.process_data)\n",
    "    \n",
    "    @step  \n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Process our data and calculate statistics\n",
    "        \"\"\"\n",
    "        print(\"üîß Processing data...\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(self.sample_data)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        self.statistics = {\n",
    "            'mean': df['values'].mean(),\n",
    "            'std': df['values'].std(),\n",
    "            'count_by_category': df['categories'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        print(f\"üìä Statistics calculated:\")\n",
    "        print(f\"   Mean: {self.statistics['mean']:.2f}\")\n",
    "        print(f\"   Std: {self.statistics['std']:.2f}\")\n",
    "        \n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        Finalize workflow\n",
    "        \"\"\"\n",
    "        print(\"üéâ Workflow completed successfully!\")\n",
    "        print(f\"üìã Final statistics: {self.statistics}\")\n",
    "\n",
    "# Note: We define the flow here, but will run it from command line\n",
    "print(\"‚úÖ WorkshopIntroFlow defined successfully!\")\n",
    "print(\"üí° To run this flow, save it as a .py file and use: python flow_file.py run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Metaflow Structure\n",
    "\n",
    "Every Metaflow workflow has:\n",
    "- **FlowSpec**: The main class that defines your workflow\n",
    "- **@step**: Decorators that mark individual processing steps\n",
    "- **self.next()**: Defines the flow between steps\n",
    "- **Artifacts**: Data that persists between steps (self.variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the flow structure in more detail\n",
    "from metaflow import FlowSpec, step, Parameter\n",
    "\n",
    "class DetailedFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    A more detailed flow showing Metaflow features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters allow customization when running\n",
    "    sample_size = Parameter('sample_size', \n",
    "                           help='Number of samples to generate',\n",
    "                           default=100)\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Generate data with configurable size\n",
    "        \"\"\"\n",
    "        print(f\"üéØ Generating {self.sample_size} samples\")\n",
    "        \n",
    "        # Generate data using the parameter\n",
    "        np.random.seed(42)\n",
    "        self.data = np.random.normal(0, 1, self.sample_size)\n",
    "        \n",
    "        # Store metadata about our data\n",
    "        self.metadata = {\n",
    "            'created_at': pd.Timestamp.now(),\n",
    "            'sample_size': len(self.data),\n",
    "            'data_type': 'normal_distribution'\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Data generated: shape {self.data.shape}\")\n",
    "        self.next(self.analyze)\n",
    "    \n",
    "    @step\n",
    "    def analyze(self):\n",
    "        \"\"\"\n",
    "        Analyze the generated data\n",
    "        \"\"\"\n",
    "        print(\"üìä Analyzing data...\")\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        self.analysis = {\n",
    "            'mean': float(np.mean(self.data)),\n",
    "            'std': float(np.std(self.data)),\n",
    "            'min': float(np.min(self.data)),\n",
    "            'max': float(np.max(self.data)),\n",
    "            'percentiles': {\n",
    "                '25th': float(np.percentile(self.data, 25)),\n",
    "                '50th': float(np.percentile(self.data, 50)),\n",
    "                '75th': float(np.percentile(self.data, 75))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"   Mean: {self.analysis['mean']:.3f}\")\n",
    "        print(f\"   Std: {self.analysis['std']:.3f}\")\n",
    "        print(f\"   Range: [{self.analysis['min']:.3f}, {self.analysis['max']:.3f}]\")\n",
    "        \n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        Summarize results\n",
    "        \"\"\"\n",
    "        print(\"üéâ Analysis complete!\")\n",
    "        \n",
    "        # Create final summary\n",
    "        self.summary = {\n",
    "            'workflow': 'DetailedFlow',\n",
    "            'parameters': {'sample_size': self.sample_size},\n",
    "            'metadata': self.metadata,\n",
    "            'results': self.analysis\n",
    "        }\n",
    "        \n",
    "        print(f\"üìã Summary created with {len(self.summary)} sections\")\n",
    "\n",
    "print(\"‚úÖ DetailedFlow defined successfully!\")\n",
    "print(\"üí° This flow demonstrates parameters and comprehensive data tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accessing Flow Results\n",
    "\n",
    "One of Metaflow's superpowers is that you can access results from any previous run programmatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates how to access flow results\n",
    "# Note: This will only work after you've actually run a flow\n",
    "\n",
    "from metaflow import Flow\n",
    "\n",
    "# Function to safely demonstrate flow access\n",
    "def demonstrate_flow_access():\n",
    "    try:\n",
    "        # Try to access a flow (this will fail if no flows have been run)\n",
    "        # flow = Flow('WorkshopIntroFlow')\n",
    "        # latest_run = flow.latest_run\n",
    "        \n",
    "        print(\"üìã How to access flow results:\")\n",
    "        print(\"\")\n",
    "        print(\"# Get a specific flow\")\n",
    "        print(\"flow = Flow('WorkshopIntroFlow')\")\n",
    "        print(\"\")\n",
    "        print(\"# Get the latest run\")\n",
    "        print(\"latest_run = flow.latest_run\")\n",
    "        print(\"\")\n",
    "        print(\"# Check if run was successful\")\n",
    "        print(\"if latest_run.successful:\")\n",
    "        print(\"    print('Run completed successfully!')\")\n",
    "        print(\"\")\n",
    "        print(\"# Access artifacts from specific steps\")\n",
    "        print(\"end_step = latest_run['end']\")\n",
    "        print(\"statistics = end_step.task.data.statistics\")\n",
    "        print(\"\")\n",
    "        print(\"# Access all artifacts\")\n",
    "        print(\"for artifact_name in end_step.task.data:\")\n",
    "        print(\"    print(f'Artifact: {artifact_name}')\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Flow access patterns demonstrated!\")\n",
    "        print(\"üí° Run a flow first, then use these patterns to access results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è  No flows run yet: {e}\")\n",
    "        print(\"This is expected - run a flow first to see real results!\")\n",
    "\n",
    "demonstrate_flow_access()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metaflow Best Practices\n",
    "\n",
    "Let's look at a production-ready flow that follows best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import FlowSpec, step, Parameter, catch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class ProductionReadyFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    A production-ready flow demonstrating best practices:\n",
    "    - Comprehensive error handling\n",
    "    - Detailed logging\n",
    "    - Parameter validation\n",
    "    - Artifact organization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Well-documented parameters with validation\n",
    "    data_size = Parameter('data_size',\n",
    "                         help='Number of data points to generate (10-10000)',\n",
    "                         default=1000,\n",
    "                         type=int)\n",
    "    \n",
    "    noise_level = Parameter('noise_level',\n",
    "                           help='Noise level for data generation (0.1-2.0)',\n",
    "                           default=0.5,\n",
    "                           type=float)\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Initialize workflow with parameter validation and logging\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Starting ProductionReadyFlow\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Parameters:\")\n",
    "        print(f\"  data_size: {self.data_size}\")\n",
    "        print(f\"  noise_level: {self.noise_level}\")\n",
    "        \n",
    "        # Parameter validation\n",
    "        if not (10 <= self.data_size <= 10000):\n",
    "            raise ValueError(f\"data_size must be between 10 and 10000, got {self.data_size}\")\n",
    "        \n",
    "        if not (0.1 <= self.noise_level <= 2.0):\n",
    "            raise ValueError(f\"noise_level must be between 0.1 and 2.0, got {self.noise_level}\")\n",
    "        \n",
    "        # Store run metadata\n",
    "        self.run_metadata = {\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'parameters': {\n",
    "                'data_size': self.data_size,\n",
    "                'noise_level': self.noise_level\n",
    "            },\n",
    "            'version': '1.0.0',\n",
    "            'description': 'Production-ready data processing workflow'\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Parameters validated and metadata stored\")\n",
    "        self.next(self.generate_data)\n",
    "    \n",
    "    @catch(var='generation_error')\n",
    "    @step\n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate synthetic data with error handling\n",
    "        \"\"\"\n",
    "        print(\"\\nüîß Generating synthetic data...\")\n",
    "        \n",
    "        try:\n",
    "            # Set seed for reproducibility\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            # Generate base signal\n",
    "            x = np.linspace(0, 4*np.pi, self.data_size)\n",
    "            signal = np.sin(x) + 0.5*np.cos(2*x)\n",
    "            \n",
    "            # Add noise\n",
    "            noise = np.random.normal(0, self.noise_level, self.data_size)\n",
    "            self.data = signal + noise\n",
    "            \n",
    "            # Create additional features\n",
    "            self.features = {\n",
    "                'x': x,\n",
    "                'signal': signal,\n",
    "                'noise': noise,\n",
    "                'final_data': self.data\n",
    "            }\n",
    "            \n",
    "            # Store generation info\n",
    "            self.generation_info = {\n",
    "                'data_points_generated': len(self.data),\n",
    "                'signal_range': [float(signal.min()), float(signal.max())],\n",
    "                'noise_std': float(noise.std()),\n",
    "                'final_data_range': [float(self.data.min()), float(self.data.max())]\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Generated {len(self.data)} data points\")\n",
    "            print(f\"   üìä Data range: [{self.data.min():.3f}, {self.data.max():.3f}]\")\n",
    "            print(f\"   üîä Noise std: {noise.std():.3f}\")\n",
    "            \n",
    "            self.generation_error = None  # No error occurred\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Data generation failed: {e}\")\n",
    "            self.generation_error = str(e)\n",
    "            # Set default empty data\n",
    "            self.data = np.array([])\n",
    "            self.features = {}\n",
    "            self.generation_info = {'error': str(e)}\n",
    "        \n",
    "        self.next(self.analyze_data)\n",
    "    \n",
    "    @step\n",
    "    def analyze_data(self):\n",
    "        \"\"\"\n",
    "        Comprehensive data analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä Analyzing data...\")\n",
    "        \n",
    "        if self.generation_error:\n",
    "            print(f\"   ‚ö†Ô∏è Skipping analysis due to generation error: {self.generation_error}\")\n",
    "            self.analysis_results = {'error': 'No data to analyze'}\n",
    "        else:\n",
    "            # Comprehensive statistical analysis\n",
    "            self.analysis_results = {\n",
    "                'basic_stats': {\n",
    "                    'mean': float(np.mean(self.data)),\n",
    "                    'std': float(np.std(self.data)),\n",
    "                    'median': float(np.median(self.data)),\n",
    "                    'min': float(np.min(self.data)),\n",
    "                    'max': float(np.max(self.data))\n",
    "                },\n",
    "                'distribution_stats': {\n",
    "                    'skewness': float(self._calculate_skewness(self.data)),\n",
    "                    'kurtosis': float(self._calculate_kurtosis(self.data))\n",
    "                },\n",
    "                'percentiles': {\n",
    "                    '10th': float(np.percentile(self.data, 10)),\n",
    "                    '25th': float(np.percentile(self.data, 25)),\n",
    "                    '75th': float(np.percentile(self.data, 75)),\n",
    "                    '90th': float(np.percentile(self.data, 90))\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"   üìà Mean: {self.analysis_results['basic_stats']['mean']:.3f}\")\n",
    "            print(f\"   üìä Std: {self.analysis_results['basic_stats']['std']:.3f}\")\n",
    "            print(f\"   üìè Range: [{self.analysis_results['basic_stats']['min']:.3f}, {self.analysis_results['basic_stats']['max']:.3f}]\")\n",
    "        \n",
    "        self.next(self.end)\n",
    "    \n",
    "    def _calculate_skewness(self, data):\n",
    "        \"\"\"Calculate skewness of data\"\"\"\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        return np.mean(((data - mean) / std) ** 3)\n",
    "    \n",
    "    def _calculate_kurtosis(self, data):\n",
    "        \"\"\"Calculate kurtosis of data\"\"\"\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        return np.mean(((data - mean) / std) ** 4) - 3\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        Finalize workflow with comprehensive summary\n",
    "        \"\"\"\n",
    "        print(\"\\nüéâ ProductionReadyFlow completed!\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Create comprehensive final report\n",
    "        self.final_report = {\n",
    "            'workflow_info': {\n",
    "                'name': 'ProductionReadyFlow',\n",
    "                'version': self.run_metadata['version'],\n",
    "                'completion_time': datetime.now().isoformat()\n",
    "            },\n",
    "            'execution_summary': {\n",
    "                'parameters_used': self.run_metadata['parameters'],\n",
    "                'data_generation_successful': self.generation_error is None,\n",
    "                'analysis_completed': 'error' not in self.analysis_results\n",
    "            },\n",
    "            'results': self.analysis_results if hasattr(self, 'analysis_results') else {},\n",
    "            'metadata': self.run_metadata\n",
    "        }\n",
    "        \n",
    "        print(\"üìã Final Report Summary:\")\n",
    "        print(f\"   ‚úÖ Workflow: {self.final_report['workflow_info']['name']}\")\n",
    "        print(f\"   üìä Data generation: {'‚úÖ Success' if self.final_report['execution_summary']['data_generation_successful'] else '‚ùå Failed'}\")\n",
    "        print(f\"   üîç Analysis: {'‚úÖ Complete' if self.final_report['execution_summary']['analysis_completed'] else '‚ùå Failed'}\")\n",
    "        \n",
    "        if self.generation_error is None:\n",
    "            print(f\"   üìà Data points processed: {len(self.data)}\")\n",
    "            print(f\"   üéØ Final data mean: {self.analysis_results['basic_stats']['mean']:.3f}\")\n",
    "        \n",
    "        print(\"\\nüíæ All artifacts saved automatically by Metaflow!\")\n",
    "\n",
    "print(\"‚úÖ ProductionReadyFlow defined successfully!\")\n",
    "print(\"üèÜ This flow demonstrates production-ready patterns:\")\n",
    "print(\"   - Parameter validation\")\n",
    "print(\"   - Error handling with @catch\")\n",
    "print(\"   - Comprehensive logging\")\n",
    "print(\"   - Detailed artifact organization\")\n",
    "print(\"   - Professional reporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running Flows and Next Steps\n",
    "\n",
    "Now you understand Metaflow fundamentals! Here's how to run these flows and what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì Metaflow Fundamentals Complete!\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nüìù What You've Learned:\")\n",
    "print(\"   ‚úÖ Metaflow FlowSpec structure\")\n",
    "print(\"   ‚úÖ Step definitions and flow control\")\n",
    "print(\"   ‚úÖ Artifact management\")\n",
    "print(\"   ‚úÖ Parameter handling\")\n",
    "print(\"   ‚úÖ Error handling with @catch\")\n",
    "print(\"   ‚úÖ Production-ready patterns\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Save these flows as .py files\")\n",
    "print(\"   2. Run them with: python flow_name.py run\")\n",
    "print(\"   3. Explore results with: python flow_name.py show\")\n",
    "print(\"   4. Access artifacts programmatically\")\n",
    "\n",
    "print(\"\\nüí° Tips for Success:\")\n",
    "print(\"   - Always use descriptive step names\")\n",
    "print(\"   - Store intermediate results as artifacts\")\n",
    "print(\"   - Add comprehensive docstrings\")\n",
    "print(\"   - Use parameters for configurable values\")\n",
    "print(\"   - Handle errors gracefully\")\n",
    "\n",
    "print(\"\\nüéØ Coming Up Next:\")\n",
    "print(\"   - Data exploration with pandas\")\n",
    "print(\"   - Visualization techniques\")\n",
    "print(\"   - Complete ML pipeline with Metaflow\")\n",
    "\n",
    "print(\"\\nüèÜ You're now ready to build production ML workflows!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}