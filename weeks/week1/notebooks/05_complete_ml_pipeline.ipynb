{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete ML Pipeline - Week 1 Workshop\n",
    "\n",
    "This is the culmination of Week 1! We'll combine everything we've learned - Metaflow, data exploration, and visualization - into a production-ready machine learning pipeline.\n",
    "\n",
    "## Learning Objectives\n",
    "- Build end-to-end ML pipelines with Metaflow\n",
    "- Implement proper data preprocessing\n",
    "- Compare multiple ML algorithms systematically\n",
    "- Create comprehensive model evaluation\n",
    "- Generate production-ready reports\n",
    "\n",
    "Let's build something amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "print(\"üöÄ Setting up Complete ML Pipeline Environment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Core ML and data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metaflow for MLOps\n",
    "from metaflow import FlowSpec, step, Parameter, catch\n",
    "\n",
    "# Utilities\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìä Ready to build production ML pipeline\")\n",
    "print(\"üéØ Target: Wine classification with comprehensive evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Complete ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteMLPipeline(FlowSpec):\n",
    "    \"\"\"\n",
    "    Production-ready ML pipeline for wine classification\n",
    "    \n",
    "    Features:\n",
    "    - Comprehensive data preprocessing\n",
    "    - Multiple algorithm comparison\n",
    "    - Cross-validation and robust evaluation\n",
    "    - Automated report generation\n",
    "    - Production-ready model artifacts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurable parameters\n",
    "    test_size = Parameter('test_size',\n",
    "                         help='Test set proportion (0.1-0.4)',\n",
    "                         default=0.2,\n",
    "                         type=float)\n",
    "    \n",
    "    random_state = Parameter('random_state',\n",
    "                           help='Random seed for reproducibility',\n",
    "                           default=42,\n",
    "                           type=int)\n",
    "    \n",
    "    cv_folds = Parameter('cv_folds',\n",
    "                        help='Number of cross-validation folds',\n",
    "                        default=5,\n",
    "                        type=int)\n",
    "    \n",
    "    models_to_test = Parameter('models',\n",
    "                              help='Comma-separated list of models',\n",
    "                              default='random_forest,logistic_regression,svm,gradient_boosting,naive_bayes')\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Initialize pipeline with data loading and validation\n",
    "        \"\"\"\n",
    "        print(\"üç∑ Starting Complete Wine Classification Pipeline\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìä Configuration:\")\n",
    "        print(f\"   Test size: {self.test_size}\")\n",
    "        print(f\"   Random state: {self.random_state}\")\n",
    "        print(f\"   CV folds: {self.cv_folds}\")\n",
    "        print(f\"   Models: {self.models_to_test}\")\n",
    "        \n",
    "        # Parameter validation\n",
    "        if not (0.1 <= self.test_size <= 0.4):\n",
    "            raise ValueError(f\"test_size must be between 0.1 and 0.4, got {self.test_size}\")\n",
    "        \n",
    "        if not (3 <= self.cv_folds <= 10):\n",
    "            raise ValueError(f\"cv_folds must be between 3 and 10, got {self.cv_folds}\")\n",
    "        \n",
    "        # Load wine dataset\n",
    "        wine_data = load_wine()\n",
    "        \n",
    "        # Store raw data and metadata\n",
    "        self.X_raw = wine_data.data\n",
    "        self.y_raw = wine_data.target\n",
    "        self.feature_names = wine_data.feature_names.tolist()\n",
    "        self.target_names = wine_data.target_names.tolist()\n",
    "        \n",
    "        # Create comprehensive dataset info\n",
    "        self.dataset_info = {\n",
    "            'n_samples': self.X_raw.shape[0],\n",
    "            'n_features': self.X_raw.shape[1],\n",
    "            'n_classes': len(np.unique(self.y_raw)),\n",
    "            'class_distribution': np.bincount(self.y_raw).tolist(),\n",
    "            'feature_names': self.feature_names,\n",
    "            'target_names': self.target_names,\n",
    "            'pipeline_start_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìà Dataset Overview:\")\n",
    "        print(f\"   Samples: {self.dataset_info['n_samples']}\")\n",
    "        print(f\"   Features: {self.dataset_info['n_features']}\")\n",
    "        print(f\"   Classes: {self.dataset_info['n_classes']} {self.target_names}\")\n",
    "        print(f\"   Distribution: {self.dataset_info['class_distribution']}\")\n",
    "        \n",
    "        self.next(self.data_exploration)\n",
    "    \n",
    "    @step\n",
    "    def data_exploration(self):\n",
    "        \"\"\"\n",
    "        Comprehensive data exploration and quality assessment\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Conducting Data Exploration...\")\n",
    "        \n",
    "        # Convert to DataFrame for analysis\n",
    "        df = pd.DataFrame(self.X_raw, columns=self.feature_names)\n",
    "        df['target'] = self.y_raw\n",
    "        \n",
    "        # Data quality assessment\n",
    "        self.data_quality = {\n",
    "            'missing_values': df.isnull().sum().sum(),\n",
    "            'duplicate_rows': df.duplicated().sum(),\n",
    "            'feature_dtypes': df.dtypes.to_dict(),\n",
    "            'memory_usage_mb': df.memory_usage().sum() / 1024 / 1024\n",
    "        }\n",
    "        \n",
    "        # Statistical analysis\n",
    "        numeric_features = df.select_dtypes(include=[np.number]).drop('target', axis=1)\n",
    "        \n",
    "        self.feature_statistics = {\n",
    "            'means': numeric_features.mean().to_dict(),\n",
    "            'stds': numeric_features.std().to_dict(),\n",
    "            'mins': numeric_features.min().to_dict(),\n",
    "            'maxs': numeric_features.max().to_dict(),\n",
    "            'skewness': numeric_features.skew().to_dict()\n",
    "        }\n",
    "        \n",
    "        # Correlation analysis\n",
    "        correlation_matrix = numeric_features.corr()\n",
    "        \n",
    "        # Find highly correlated pairs\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_val = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    high_corr_pairs.append({\n",
    "                        'feature_1': correlation_matrix.columns[i],\n",
    "                        'feature_2': correlation_matrix.columns[j],\n",
    "                        'correlation': corr_val\n",
    "                    })\n",
    "        \n",
    "        self.correlation_analysis = {\n",
    "            'correlation_matrix': correlation_matrix.to_dict(),\n",
    "            'high_correlation_pairs': high_corr_pairs,\n",
    "            'n_high_corr_pairs': len(high_corr_pairs)\n",
    "        }\n",
    "        \n",
    "        # Feature-target relationships\n",
    "        target_correlations = numeric_features.corrwith(df['target']).abs().sort_values(ascending=False)\n",
    "        \n",
    "        self.feature_importance_preview = {\n",
    "            'correlation_with_target': target_correlations.to_dict(),\n",
    "            'top_5_features': target_correlations.head(5).index.tolist(),\n",
    "            'top_5_correlations': target_correlations.head(5).values.tolist()\n",
    "        }\n",
    "        \n",
    "        print(f\"   üìä Data quality score: {self._calculate_quality_score()}/10\")\n",
    "        print(f\"   üîó High correlation pairs: {len(high_corr_pairs)}\")\n",
    "        print(f\"   üéØ Top predictive feature: {target_correlations.index[0]} ({target_correlations.iloc[0]:.3f})\")\n",
    "        \n",
    "        self.next(self.preprocessing)\n",
    "    \n",
    "    def _calculate_quality_score(self):\n",
    "        \"\"\"Calculate overall data quality score (0-10)\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # No missing values (+2)\n",
    "        if self.data_quality['missing_values'] == 0:\n",
    "            score += 2\n",
    "        \n",
    "        # No duplicates (+1)\n",
    "        if self.data_quality['duplicate_rows'] == 0:\n",
    "            score += 1\n",
    "        \n",
    "        # Reasonable number of features (+2)\n",
    "        if 5 <= self.dataset_info['n_features'] <= 50:\n",
    "            score += 2\n",
    "        \n",
    "        # Balanced classes (+2)\n",
    "        class_counts = np.array(self.dataset_info['class_distribution'])\n",
    "        balance_ratio = class_counts.min() / class_counts.max()\n",
    "        if balance_ratio > 0.7:\n",
    "            score += 2\n",
    "        elif balance_ratio > 0.5:\n",
    "            score += 1\n",
    "        \n",
    "        # Low multicollinearity (+2)\n",
    "        if self.correlation_analysis['n_high_corr_pairs'] <= 3:\n",
    "            score += 2\n",
    "        elif self.correlation_analysis['n_high_corr_pairs'] <= 6:\n",
    "            score += 1\n",
    "        \n",
    "        # Strong predictive features (+1)\n",
    "        if max(self.feature_importance_preview['top_5_correlations']) > 0.5:\n",
    "            score += 1\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    @step\n",
    "    def preprocessing(self):\n",
    "        \"\"\"\n",
    "        Comprehensive data preprocessing pipeline\n",
    "        \"\"\"\n",
    "        print(\"\\nüîß Data Preprocessing Pipeline...\")\n",
    "        \n",
    "        # Split the data with stratification\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X_raw, self.y_raw,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=self.y_raw\n",
    "        )\n",
    "        \n",
    "        # Feature scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        # Store preprocessing info\n",
    "        self.preprocessing_info = {\n",
    "            'train_samples': len(self.X_train),\n",
    "            'test_samples': len(self.X_test),\n",
    "            'train_class_distribution': np.bincount(self.y_train).tolist(),\n",
    "            'test_class_distribution': np.bincount(self.y_test).tolist(),\n",
    "            'scaler_mean': self.scaler.mean_.tolist(),\n",
    "            'scaler_scale': self.scaler.scale_.tolist(),\n",
    "            'scaling_method': 'StandardScaler'\n",
    "        }\n",
    "        \n",
    "        # Validate stratification\n",
    "        train_dist = np.array(self.preprocessing_info['train_class_distribution']) / len(self.X_train)\n",
    "        test_dist = np.array(self.preprocessing_info['test_class_distribution']) / len(self.X_test)\n",
    "        stratification_quality = 1 - np.mean(np.abs(train_dist - test_dist))\n",
    "        \n",
    "        print(f\"   üìä Train/Test split: {len(self.X_train)}/{len(self.X_test)}\")\n",
    "        print(f\"   ‚öñÔ∏è Stratification quality: {stratification_quality:.3f}\")\n",
    "        print(f\"   üìè Features scaled using {self.preprocessing_info['scaling_method']}\")\n",
    "        \n",
    "        self.next(self.model_training)\n",
    "    \n",
    "    @catch(var='training_errors')\n",
    "    @step\n",
    "    def model_training(self):\n",
    "        \"\"\"\n",
    "        Train and compare multiple ML algorithms\n",
    "        \"\"\"\n",
    "        print(\"\\nü§ñ Training Multiple ML Models...\")\n",
    "        \n",
    "        # Parse model list\n",
    "        model_names = [name.strip() for name in self.models_to_test.split(',')]\n",
    "        \n",
    "        # Define model configurations\n",
    "        model_configs = {\n",
    "            'random_forest': {\n",
    "                'model': RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "                'description': 'Ensemble of decision trees with bagging'\n",
    "            },\n",
    "            'logistic_regression': {\n",
    "                'model': LogisticRegression(\n",
    "                    random_state=self.random_state,\n",
    "                    max_iter=1000,\n",
    "                    multi_class='ovr'\n",
    "                ),\n",
    "                'description': 'Linear model with logistic function'\n",
    "            },\n",
    "            'svm': {\n",
    "                'model': SVC(\n",
    "                    random_state=self.random_state,\n",
    "                    probability=True,\n",
    "                    kernel='rbf'\n",
    "                ),\n",
    "                'description': 'Support Vector Machine with RBF kernel'\n",
    "            },\n",
    "            'gradient_boosting': {\n",
    "                'model': GradientBoostingClassifier(\n",
    "                    random_state=self.random_state,\n",
    "                    n_estimators=100\n",
    "                ),\n",
    "                'description': 'Sequential ensemble with gradient boosting'\n",
    "            },\n",
    "            'naive_bayes': {\n",
    "                'model': GaussianNB(),\n",
    "                'description': 'Probabilistic classifier assuming feature independence'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Validate requested models\n",
    "        invalid_models = set(model_names) - set(model_configs.keys())\n",
    "        if invalid_models:\n",
    "            raise ValueError(f\"Invalid models requested: {invalid_models}\")\n",
    "        \n",
    "        # Train models with cross-validation\n",
    "        self.model_results = {}\n",
    "        self.training_errors = {}\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            try:\n",
    "                print(f\"   üî® Training {model_name}...\")\n",
    "                \n",
    "                config = model_configs[model_name]\n",
    "                model = config['model']\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(\n",
    "                    model, self.X_train_scaled, self.y_train,\n",
    "                    cv=cv, scoring='accuracy', n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Fit on full training set\n",
    "                model.fit(self.X_train_scaled, self.y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                train_pred = model.predict(self.X_train_scaled)\n",
    "                test_pred = model.predict(self.X_test_scaled)\n",
    "                \n",
    "                # Probabilities (if available)\n",
    "                try:\n",
    "                    train_prob = model.predict_proba(self.X_train_scaled)\n",
    "                    test_prob = model.predict_proba(self.X_test_scaled)\n",
    "                except AttributeError:\n",
    "                    train_prob = None\n",
    "                    test_prob = None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                train_accuracy = accuracy_score(self.y_train, train_pred)\n",
    "                test_accuracy = accuracy_score(self.y_test, test_pred)\n",
    "                \n",
    "                # Store comprehensive results\n",
    "                self.model_results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'description': config['description'],\n",
    "                    'cv_scores': cv_scores.tolist(),\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'overfitting_gap': train_accuracy - test_accuracy,\n",
    "                    'train_predictions': train_pred.tolist(),\n",
    "                    'test_predictions': test_pred.tolist(),\n",
    "                    'train_probabilities': train_prob.tolist() if train_prob is not None else None,\n",
    "                    'test_probabilities': test_prob.tolist() if test_prob is not None else None\n",
    "                }\n",
    "                \n",
    "                print(f\"      CV: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n",
    "                print(f\"      Test: {test_accuracy:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Training failed: {str(e)}\")\n",
    "                self.training_errors[model_name] = str(e)\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ Successfully trained {len(self.model_results)} models\")\n",
    "        if self.training_errors:\n",
    "            print(f\"   ‚ö†Ô∏è {len(self.training_errors)} models failed to train\")\n",
    "        \n",
    "        self.next(self.model_evaluation)\n",
    "    \n",
    "    @step\n",
    "    def model_evaluation(self):\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation and comparison\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä Comprehensive Model Evaluation...\")\n",
    "        \n",
    "        if not self.model_results:\n",
    "            print(\"   ‚ùå No models to evaluate\")\n",
    "            self.evaluation_results = {}\n",
    "            self.best_model_name = None\n",
    "            self.next(self.end)\n",
    "            return\n",
    "        \n",
    "        # Find best model by CV score\n",
    "        best_model_name = max(self.model_results.keys(),\n",
    "                            key=lambda x: self.model_results[x]['cv_mean'])\n",
    "        \n",
    "        self.best_model_name = best_model_name\n",
    "        best_model_results = self.model_results[best_model_name]\n",
    "        \n",
    "        print(f\"   üèÜ Best model: {best_model_name}\")\n",
    "        print(f\"   üìà CV score: {best_model_results['cv_mean']:.3f} ¬± {best_model_results['cv_std']:.3f}\")\n",
    "        print(f\"   üéØ Test accuracy: {best_model_results['test_accuracy']:.3f}\")\n",
    "        \n",
    "        # Detailed evaluation for best model\n",
    "        best_model = best_model_results['model']\n",
    "        test_pred = np.array(best_model_results['test_predictions'])\n",
    "        \n",
    "        # Classification report\n",
    "        classification_rep = classification_report(\n",
    "            self.y_test, test_pred,\n",
    "            target_names=self.target_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        conf_matrix = confusion_matrix(self.y_test, test_pred)\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        feature_importance = None\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            feature_importance = dict(zip(\n",
    "                self.feature_names,\n",
    "                best_model.feature_importances_\n",
    "            ))\n",
    "        elif hasattr(best_model, 'coef_'):\n",
    "            # For linear models, use absolute coefficients\n",
    "            if len(best_model.coef_.shape) > 1:\n",
    "                coef_importance = np.mean(np.abs(best_model.coef_), axis=0)\n",
    "            else:\n",
    "                coef_importance = np.abs(best_model.coef_)\n",
    "            feature_importance = dict(zip(\n",
    "                self.feature_names,\n",
    "                coef_importance\n",
    "            ))\n",
    "        \n",
    "        # Model comparison summary\n",
    "        model_comparison = {}\n",
    "        for name, results in self.model_results.items():\n",
    "            model_comparison[name] = {\n",
    "                'cv_mean': results['cv_mean'],\n",
    "                'cv_std': results['cv_std'],\n",
    "                'test_accuracy': results['test_accuracy'],\n",
    "                'overfitting_gap': results['overfitting_gap'],\n",
    "                'description': results['description']\n",
    "            }\n",
    "        \n",
    "        # Store comprehensive evaluation results\n",
    "        self.evaluation_results = {\n",
    "            'best_model_name': best_model_name,\n",
    "            'best_model_metrics': {\n",
    "                'cv_score': best_model_results['cv_mean'],\n",
    "                'cv_std': best_model_results['cv_std'],\n",
    "                'test_accuracy': best_model_results['test_accuracy'],\n",
    "                'overfitting_gap': best_model_results['overfitting_gap']\n",
    "            },\n",
    "            'classification_report': classification_rep,\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'feature_importance': feature_importance,\n",
    "            'model_comparison': model_comparison,\n",
    "            'evaluation_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.next(self.generate_report)\n",
    "    \n",
    "    @step\n",
    "    def generate_report(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive pipeline report\n",
    "        \"\"\"\n",
    "        print(\"\\nüìã Generating Comprehensive Report...\")\n",
    "        \n",
    "        # Calculate pipeline metrics\n",
    "        total_samples = self.dataset_info['n_samples']\n",
    "        models_trained = len(self.model_results)\n",
    "        models_failed = len(self.training_errors) if hasattr(self, 'training_errors') else 0\n",
    "        \n",
    "        # Performance assessment\n",
    "        if self.evaluation_results:\n",
    "            best_accuracy = self.evaluation_results['best_model_metrics']['test_accuracy']\n",
    "            performance_grade = (\n",
    "                'Excellent' if best_accuracy > 0.95 else\n",
    "                'Very Good' if best_accuracy > 0.9 else\n",
    "                'Good' if best_accuracy > 0.8 else\n",
    "                'Fair' if best_accuracy > 0.7 else\n",
    "                'Poor'\n",
    "            )\n",
    "        else:\n",
    "            best_accuracy = 0\n",
    "            performance_grade = 'Failed'\n",
    "        \n",
    "        # Create executive summary\n",
    "        self.executive_summary = {\n",
    "            'pipeline_overview': {\n",
    "                'dataset_samples': total_samples,\n",
    "                'features_used': len(self.feature_names),\n",
    "                'models_trained': models_trained,\n",
    "                'models_failed': models_failed,\n",
    "                'best_model': self.best_model_name,\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'performance_grade': performance_grade\n",
    "            },\n",
    "            'data_quality_assessment': {\n",
    "                'quality_score': self._calculate_quality_score(),\n",
    "                'missing_values': self.data_quality['missing_values'],\n",
    "                'duplicate_rows': self.data_quality['duplicate_rows'],\n",
    "                'high_correlation_pairs': self.correlation_analysis['n_high_corr_pairs']\n",
    "            },\n",
    "            'model_performance': self.evaluation_results.get('model_comparison', {}),\n",
    "            'recommendations': self._generate_recommendations()\n",
    "        }\n",
    "        \n",
    "        # Create final comprehensive report\n",
    "        self.final_report = {\n",
    "            'pipeline_metadata': {\n",
    "                'pipeline_name': 'CompleteMLPipeline',\n",
    "                'version': '1.0.0',\n",
    "                'execution_timestamp': datetime.now().isoformat(),\n",
    "                'parameters_used': {\n",
    "                    'test_size': self.test_size,\n",
    "                    'random_state': self.random_state,\n",
    "                    'cv_folds': self.cv_folds,\n",
    "                    'models_to_test': self.models_to_test\n",
    "                }\n",
    "            },\n",
    "            'executive_summary': self.executive_summary,\n",
    "            'dataset_analysis': {\n",
    "                'dataset_info': self.dataset_info,\n",
    "                'data_quality': self.data_quality,\n",
    "                'feature_statistics': self.feature_statistics,\n",
    "                'correlation_analysis': self.correlation_analysis,\n",
    "                'feature_importance_preview': self.feature_importance_preview\n",
    "            },\n",
    "            'preprocessing_details': self.preprocessing_info,\n",
    "            'model_training_results': {\n",
    "                'successful_models': self.model_results,\n",
    "                'failed_models': getattr(self, 'training_errors', {}),\n",
    "                'training_summary': {\n",
    "                    'total_models_attempted': models_trained + models_failed,\n",
    "                    'successful_models': models_trained,\n",
    "                    'failed_models': models_failed,\n",
    "                    'success_rate': models_trained / (models_trained + models_failed) if (models_trained + models_failed) > 0 else 0\n",
    "                }\n",
    "            },\n",
    "            'evaluation_results': self.evaluation_results,\n",
    "            'production_readiness': self._assess_production_readiness()\n",
    "        }\n",
    "        \n",
    "        print(f\"   üìä Report generated successfully!\")\n",
    "        print(f\"   üéØ Performance grade: {performance_grade}\")\n",
    "        print(f\"   üìà Best model accuracy: {best_accuracy:.3f}\")\n",
    "        print(f\"   üîß Data quality score: {self._calculate_quality_score()}/10\")\n",
    "        \n",
    "        self.next(self.end)\n",
    "    \n",
    "    def _generate_recommendations(self):\n",
    "        \"\"\"Generate actionable recommendations based on results\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if not self.evaluation_results:\n",
    "            recommendations.append(\"Pipeline failed - review error logs and data quality\")\n",
    "            return recommendations\n",
    "        \n",
    "        # Performance recommendations\n",
    "        best_accuracy = self.evaluation_results['best_model_metrics']['test_accuracy']\n",
    "        if best_accuracy > 0.95:\n",
    "            recommendations.append(\"Excellent performance - ready for production deployment\")\n",
    "        elif best_accuracy > 0.9:\n",
    "            recommendations.append(\"Good performance - consider hyperparameter tuning for optimization\")\n",
    "        elif best_accuracy > 0.8:\n",
    "            recommendations.append(\"Fair performance - investigate feature engineering opportunities\")\n",
    "        else:\n",
    "            recommendations.append(\"Poor performance - consider data quality issues or alternative approaches\")\n",
    "        \n",
    "        # Overfitting recommendations\n",
    "        overfitting_gap = self.evaluation_results['best_model_metrics']['overfitting_gap']\n",
    "        if overfitting_gap > 0.1:\n",
    "            recommendations.append(\"Significant overfitting detected - implement regularization or reduce model complexity\")\n",
    "        \n",
    "        # Data quality recommendations\n",
    "        if self.correlation_analysis['n_high_corr_pairs'] > 5:\n",
    "            recommendations.append(\"High feature correlation detected - consider feature selection or PCA\")\n",
    "        \n",
    "        if self._calculate_quality_score() < 7:\n",
    "            recommendations.append(\"Data quality concerns - review data collection and preprocessing\")\n",
    "        \n",
    "        # Model diversity recommendations\n",
    "        if len(self.model_results) < 3:\n",
    "            recommendations.append(\"Limited model comparison - test additional algorithm types\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _assess_production_readiness(self):\n",
    "        \"\"\"Assess if the pipeline is ready for production\"\"\"\n",
    "        readiness_score = 0\n",
    "        max_score = 10\n",
    "        \n",
    "        # Performance criteria (3 points)\n",
    "        if self.evaluation_results:\n",
    "            accuracy = self.evaluation_results['best_model_metrics']['test_accuracy']\n",
    "            if accuracy > 0.9:\n",
    "                readiness_score += 3\n",
    "            elif accuracy > 0.8:\n",
    "                readiness_score += 2\n",
    "            elif accuracy > 0.7:\n",
    "                readiness_score += 1\n",
    "        \n",
    "        # Data quality (2 points)\n",
    "        quality_score = self._calculate_quality_score()\n",
    "        if quality_score >= 8:\n",
    "            readiness_score += 2\n",
    "        elif quality_score >= 6:\n",
    "            readiness_score += 1\n",
    "        \n",
    "        # Model robustness (2 points)\n",
    "        if self.evaluation_results:\n",
    "            cv_std = self.evaluation_results['best_model_metrics']['cv_std']\n",
    "            overfitting = self.evaluation_results['best_model_metrics']['overfitting_gap']\n",
    "            if cv_std < 0.05 and overfitting < 0.05:\n",
    "                readiness_score += 2\n",
    "            elif cv_std < 0.1 and overfitting < 0.1:\n",
    "                readiness_score += 1\n",
    "        \n",
    "        # Pipeline completeness (2 points)\n",
    "        if len(self.model_results) >= 3:\n",
    "            readiness_score += 1\n",
    "        if hasattr(self, 'final_report') and self.final_report:\n",
    "            readiness_score += 1\n",
    "        \n",
    "        # Error handling (1 point)\n",
    "        if len(getattr(self, 'training_errors', {})) == 0:\n",
    "            readiness_score += 1\n",
    "        \n",
    "        readiness_level = (\n",
    "            'Production Ready' if readiness_score >= 8 else\n",
    "            'Near Production' if readiness_score >= 6 else\n",
    "            'Development Stage' if readiness_score >= 4 else\n",
    "            'Early Development'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'readiness_score': readiness_score,\n",
    "            'max_score': max_score,\n",
    "            'readiness_level': readiness_level,\n",
    "            'readiness_percentage': (readiness_score / max_score) * 100\n",
    "        }\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        Finalize pipeline with summary and next steps\n",
    "        \"\"\"\n",
    "        print(\"\\nüéâ Complete ML Pipeline Finished!\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        if hasattr(self, 'final_report') and self.final_report:\n",
    "            summary = self.executive_summary['pipeline_overview']\n",
    "            quality = self.executive_summary['data_quality_assessment']\n",
    "            production = self.final_report['production_readiness']\n",
    "            \n",
    "            print(\"üìä Pipeline Execution Summary:\")\n",
    "            print(f\"   üìà Dataset: {summary['dataset_samples']} samples, {summary['features_used']} features\")\n",
    "            print(f\"   ü§ñ Models: {summary['models_trained']} trained, {summary['models_failed']} failed\")\n",
    "            print(f\"   üèÜ Best Model: {summary['best_model']} ({summary['best_accuracy']:.3f} accuracy)\")\n",
    "            print(f\"   üìã Performance: {summary['performance_grade']}\")\n",
    "            \n",
    "            print(f\"\\nüîç Data Quality Assessment:\")\n",
    "            print(f\"   üìä Quality Score: {quality['quality_score']}/10\")\n",
    "            print(f\"   ‚úÖ Missing Values: {quality['missing_values']}\")\n",
    "            print(f\"   üîó High Correlations: {quality['high_correlation_pairs']}\")\n",
    "            \n",
    "            print(f\"\\nüöÄ Production Readiness:\")\n",
    "            print(f\"   üìà Readiness Score: {production['readiness_score']}/{production['max_score']}\")\n",
    "            print(f\"   üéØ Status: {production['readiness_level']}\")\n",
    "            print(f\"   üìä Percentage: {production['readiness_percentage']:.1f}%\")\n",
    "            \n",
    "            print(f\"\\nüí° Key Recommendations:\")\n",
    "            for i, rec in enumerate(self.executive_summary['recommendations'][:3], 1):\n",
    "                print(f\"   {i}. {rec}\")\n",
    "            \n",
    "            print(f\"\\nüîó Access Results:\")\n",
    "            print(\"   from metaflow import Flow\")\n",
    "            print(\"   flow = Flow('CompleteMLPipeline')\")\n",
    "            print(\"   run = flow.latest_run\")\n",
    "            print(\"   report = run['end'].task.data.final_report\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Pipeline execution failed\")\n",
    "            print(\"Check error logs and data quality issues\")\n",
    "        \n",
    "        print(f\"\\n‚ú® All artifacts automatically saved by Metaflow!\")\n",
    "        print(\"üéì Week 1 Complete ML Pipeline Workshop Finished!\")\n",
    "\n",
    "print(\"‚úÖ CompleteMLPipeline class defined successfully!\")\n",
    "print(\"üéØ This pipeline demonstrates production-ready MLOps practices:\")\n",
    "print(\"   üìä Comprehensive data exploration and quality assessment\")\n",
    "print(\"   üîß Robust preprocessing with validation\")\n",
    "print(\"   ü§ñ Multiple algorithm comparison with cross-validation\")\n",
    "print(\"   üìà Detailed evaluation and performance metrics\")\n",
    "print(\"   üìã Automated report generation\")\n",
    "print(\"   üöÄ Production readiness assessment\")\n",
    "print(\"\")\n",
    "print(\"üí° To run this pipeline:\")\n",
    "print(\"   1. Save this code as 'complete_ml_pipeline.py'\")\n",
    "print(\"   2. Run: python complete_ml_pipeline.py run\")\n",
    "print(\"   3. Explore results: python complete_ml_pipeline.py show\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Demonstration and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ PIPELINE DEMONSTRATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# For workshop demonstration, we'll show how to run individual steps\n",
    "# and examine the pipeline structure\n",
    "\n",
    "print(\"üìã Pipeline Step Overview:\")\n",
    "pipeline_steps = [\n",
    "    (\"start\", \"Load data and validate parameters\"),\n",
    "    (\"data_exploration\", \"Comprehensive data analysis and quality assessment\"),\n",
    "    (\"preprocessing\", \"Data splitting, scaling, and preparation\"),\n",
    "    (\"model_training\", \"Train multiple ML algorithms with cross-validation\"),\n",
    "    (\"model_evaluation\", \"Evaluate models and select best performer\"),\n",
    "    (\"generate_report\", \"Create comprehensive analysis report\"),\n",
    "    (\"end\", \"Finalize pipeline and provide recommendations\")\n",
    "]\n",
    "\n",
    "for i, (step_name, description) in enumerate(pipeline_steps, 1):\n",
    "    print(f\"   {i}. {step_name}: {description}\")\n",
    "\n",
    "print(\"\\nüîß Pipeline Configuration Options:\")\n",
    "config_options = {\n",
    "    \"test_size\": \"Proportion of data for testing (0.1-0.4)\",\n",
    "    \"random_state\": \"Random seed for reproducibility\",\n",
    "    \"cv_folds\": \"Number of cross-validation folds (3-10)\",\n",
    "    \"models\": \"Comma-separated list of models to train\"\n",
    "}\n",
    "\n",
    "for param, description in config_options.items():\n",
    "    print(f\"   --{param}: {description}\")\n",
    "\n",
    "print(\"\\nüí° Example Usage Commands:\")\n",
    "print(\"   # Basic run with defaults\")\n",
    "print(\"   python complete_ml_pipeline.py run\")\n",
    "print(\"\")\n",
    "print(\"   # Custom configuration\")\n",
    "print(\"   python complete_ml_pipeline.py run --test_size 0.3 --cv_folds 10\")\n",
    "print(\"\")\n",
    "print(\"   # Specific models only\")\n",
    "print(\"   python complete_ml_pipeline.py run --models 'random_forest,svm'\")\n",
    "print(\"\")\n",
    "print(\"   # View results\")\n",
    "print(\"   python complete_ml_pipeline.py show\")\n",
    "\n",
    "# Demonstrate pipeline components with sample data\n",
    "print(\"\\nüéØ Quick Pipeline Component Demo:\")\n",
    "\n",
    "# Load sample data for demonstration\n",
    "wine_data = load_wine()\n",
    "X_sample = wine_data.data[:50]  # Use subset for demo\n",
    "y_sample = wine_data.target[:50]\n",
    "\n",
    "print(f\"   üìä Sample data loaded: {X_sample.shape[0]} samples, {X_sample.shape[1]} features\")\n",
    "\n",
    "# Demo preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, random_state=42, stratify=y_sample\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"   üîß Preprocessing: {len(X_train)} train, {len(X_test)} test samples\")\n",
    "\n",
    "# Demo model training\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    accuracy = model.score(X_test_scaled, y_test)\n",
    "    results[name] = accuracy\n",
    "    print(f\"   ü§ñ {name}: {accuracy:.3f} accuracy\")\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"   üèÜ Best model: {best_model} ({results[best_model]:.3f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline components working correctly!\")\n",
    "print(\"üöÄ Ready to run the complete pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä PIPELINE RESULTS ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# This section demonstrates how to analyze pipeline results\n",
    "# In a real scenario, you would access these from a completed Metaflow run\n",
    "\n",
    "# Simulate pipeline results for visualization\n",
    "print(\"üéØ Simulating Pipeline Results for Visualization...\")\n",
    "\n",
    "# Load full dataset for realistic analysis\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "feature_names = wine_data.feature_names\n",
    "target_names = wine_data.target_names\n",
    "\n",
    "# Comprehensive model comparison\n",
    "models_to_compare = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Split and preprocess data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train models and collect results\n",
    "model_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_accuracy = model.score(X_train_scaled, y_train)\n",
    "    test_accuracy = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'overfitting': train_accuracy - test_accuracy\n",
    "    }\n",
    "\n",
    "print(f\"‚úÖ Trained and evaluated {len(model_results)} models\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Complete ML Pipeline Results Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "model_names = list(model_results.keys())\n",
    "cv_means = [model_results[name]['cv_mean'] for name in model_names]\n",
    "cv_stds = [model_results[name]['cv_std'] for name in model_names]\n",
    "test_accs = [model_results[name]['test_accuracy'] for name in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "axes[0, 0].bar(x_pos - 0.2, cv_means, 0.4, yerr=cv_stds, \n",
    "              label='CV Score', alpha=0.7, capsize=5)\n",
    "axes[0, 0].bar(x_pos + 0.2, test_accs, 0.4, \n",
    "              label='Test Accuracy', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Models')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Model Performance Comparison')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Overfitting Analysis\n",
    "overfitting_gaps = [model_results[name]['overfitting'] for name in model_names]\n",
    "colors = ['red' if gap > 0.05 else 'orange' if gap > 0.02 else 'green' \n",
    "          for gap in overfitting_gaps]\n",
    "bars = axes[0, 1].bar(model_names, overfitting_gaps, color=colors, alpha=0.7)\n",
    "axes[0, 1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='High Overfitting')\n",
    "axes[0, 1].axhline(y=0.02, color='orange', linestyle='--', alpha=0.7, label='Moderate Overfitting')\n",
    "axes[0, 1].set_xlabel('Models')\n",
    "axes[0, 1].set_ylabel('Overfitting Gap')\n",
    "axes[0, 1].set_title('Overfitting Analysis')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Feature Importance (Random Forest)\n",
    "rf_model = models_to_compare['Random Forest']\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "feature_importance = rf_model.feature_importances_\n",
    "top_features_idx = np.argsort(feature_importance)[-8:]  # Top 8 features\n",
    "axes[0, 2].barh(range(len(top_features_idx)), feature_importance[top_features_idx])\n",
    "axes[0, 2].set_yticks(range(len(top_features_idx)))\n",
    "axes[0, 2].set_yticklabels([feature_names[i] for i in top_features_idx])\n",
    "axes[0, 2].set_xlabel('Feature Importance')\n",
    "axes[0, 2].set_title('Top Features (Random Forest)')\n",
    "axes[0, 2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Confusion Matrix (Best Model)\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['cv_mean'])\n",
    "best_model = models_to_compare[best_model_name]\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=target_names, yticklabels=target_names, ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "axes[1, 0].set_title(f'Confusion Matrix ({best_model_name})')\n",
    "\n",
    "# 5. Cross-Validation Stability\n",
    "cv_scores_all = {}\n",
    "for name, model in models_to_compare.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_scores_all[name] = scores\n",
    "\n",
    "# Box plot of CV scores\n",
    "cv_data = [cv_scores_all[name] for name in model_names]\n",
    "bp = axes[1, 1].boxplot(cv_data, labels=model_names, patch_artist=True)\n",
    "colors_box = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1, 1].set_xlabel('Models')\n",
    "axes[1, 1].set_ylabel('CV Accuracy')\n",
    "axes[1, 1].set_title('Cross-Validation Stability')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Performance Summary Table\n",
    "axes[1, 2].axis('off')\n",
    "table_data = []\n",
    "for name in model_names:\n",
    "    results = model_results[name]\n",
    "    table_data.append([\n",
    "        name,\n",
    "        f\"{results['cv_mean']:.3f}\",\n",
    "        f\"{results['test_accuracy']:.3f}\",\n",
    "        f\"{results['overfitting']:.3f}\"\n",
    "    ])\n",
    "\n",
    "table = axes[1, 2].table(cellText=table_data,\n",
    "                        colLabels=['Model', 'CV Score', 'Test Acc', 'Overfitting'],\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style the table\n",
    "for i in range(len(table_data) + 1):\n",
    "    for j in range(4):\n",
    "        cell = table[(i, j)]\n",
    "        if i == 0:  # Header
            cell.set_facecolor('#40466e')
            cell.set_text_props(weight='bold', color='white')
        else:
            # Highlight best performers
            if j == 1:  # CV Score column
                val = float(table_data[i-1][j])
                if val == max([float(row[1]) for row in table_data]):
                    cell.set_facecolor('#90EE90')  # Light green
            elif j == 2:  # Test Accuracy column
                val = float(table_data[i-1][j])
                if val == max([float(row[2]) for row in table_data]):
                    cell.set_facecolor('#90EE90')  # Light green

axes[1, 2].set_title('Performance Summary')

plt.tight_layout()
plt.show()

# Print summary
print(f"\\nüìä Pipeline Results Summary:")
print(f"   üèÜ Best Model: {best_model_name}")
print(f"   üìà Best CV Score: {model_results[best_model_name]['cv_mean']:.3f} ¬± {model_results[best_model_name]['cv_std']:.3f}")
print(f"   üéØ Test Accuracy: {model_results[best_model_name]['test_accuracy']:.3f}")
print(f"   ‚öñÔ∏è Overfitting Gap: {model_results[best_model_name]['overfitting']:.3f}")

# Performance assessment
best_accuracy = model_results[best_model_name]['test_accuracy']
if best_accuracy > 0.95:
    print(f"   ‚úÖ Excellent performance - ready for production!")
elif best_accuracy > 0.9:
    print(f"   ‚úÖ Very good performance - minor optimizations possible")
elif best_accuracy > 0.8:
    print(f"   ‚ö†Ô∏è Good performance - consider feature engineering")
else:
    print(f"   ‚ùå Performance needs improvement - review approach")

print(f"\\nüí° Key Insights:")
print(f"   üìä {len([name for name, results in model_results.items() if results['overfitting'] < 0.05])} models show good generalization")
print(f"   üéØ Feature '{feature_names[top_features_idx[-1]]}' is most important")
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production Deployment Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ PRODUCTION DEPLOYMENT READINESS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Production readiness checklist and assessment\n",
    "print(\"üìã Production Readiness Checklist:\")\n",
    "\n",
    "checklist_items = [\n",
    "    (\"Data Quality\", \"‚úÖ No missing values, clean dataset\"),\n",
    "    (\"Model Performance\", \"‚úÖ >90% accuracy on test set\"),\n",
    "    (\"Overfitting Check\", \"‚úÖ <5% gap between train/test\"),\n",
    "    (\"Cross-Validation\", \"‚úÖ Stable performance across folds\"),\n",
    "    (\"Feature Stability\", \"‚úÖ Important features identified\"),\n",
    "    (\"Error Handling\", \"‚úÖ Robust pipeline with error catching\"),\n",
    "    (\"Reproducibility\", \"‚úÖ Fixed random seeds, versioned code\"),\n",
    "    (\"Documentation\", \"‚úÖ Comprehensive reports generated\"),\n",
    "    (\"Scalability\", \"‚ö†Ô∏è Consider batch vs real-time needs\"),\n",
    "    (\"Monitoring\", \"‚ö†Ô∏è Implement performance monitoring\")\n",
    "]\n",
    "\n",
    "for category, status in checklist_items:\n",
    "    print(f\"   {status} {category}\")\n",
    "\n",
    "print(\"\\nüîß Deployment Architecture Recommendations:\")\n",
    "\n",
    "deployment_options = {\n",
    "    \"Batch Processing\": {\n",
    "        \"Use Case\": \"Large datasets, periodic predictions\",\n",
    "        \"Technology\": \"Metaflow + AWS Batch/Google Cloud Dataflow\",\n",
    "        \"Pros\": \"High throughput, cost-effective\",\n",
    "        \"Cons\": \"Not real-time\"\n",
    "    },\n",
    "    \"Real-time API\": {\n",
    "        \"Use Case\": \"Individual predictions on demand\",\n",
    "        \"Technology\": \"Flask/FastAPI + Docker + Kubernetes\",\n",
    "        \"Pros\": \"Low latency, interactive\",\n",
    "        \"Cons\": \"Higher infrastructure costs\"\n",
    "    },\n",
    "    \"Streaming\": {\n",
    "        \"Use Case\": \"Continuous data streams\",\n",
    "        \"Technology\": \"Kafka + Apache Spark + MLflow\",\n",
    "        \"Pros\": \"Real-time processing\",\n",
    "        \"Cons\": \"Complex infrastructure\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for deployment_type, details in deployment_options.items():\n",
    "    print(f\"\\n   üéØ {deployment_type}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "\n",
    "print(\"\\nüìä Model Serving Code Example:\")\n",
    "serving_code = '''\n",
    "# Example Flask API for model serving\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load trained model and scaler\n",
    "model = pickle.load(open('best_model.pkl', 'rb'))\n",
    "scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get features from request\n",
    "        features = request.json['features']\n",
    "        \n",
    "        # Preprocess\n",
    "        features_scaled = scaler.transform([features])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(features_scaled)[0]\n",
    "        probability = model.predict_proba(features_scaled)[0]\n",
    "        \n",
    "        return jsonify({\n",
    "            'prediction': int(prediction),\n",
    "            'probabilities': probability.tolist(),\n",
    "            'confidence': float(max(probability))\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "'''\n",
    "\n",
    "print(serving_code)\n",
    "\n",
    "print(\"\\nüîç Monitoring and Maintenance:\")\n",
    "monitoring_aspects = [\n",
    "    \"üìä Model Performance Metrics (accuracy, latency)\",\n",
    "    \"üìà Data Drift Detection (feature distribution changes)\",\n",
    "    \"üéØ Prediction Quality Monitoring\",\n",
    "    \"‚ö†Ô∏è Error Rate and Exception Tracking\",\n",
    "    \"üíæ Resource Usage (CPU, memory, storage)\",\n",
    "    \"üîÑ Automatic Retraining Triggers\",\n",
    "    \"üìã Audit Logging for Compliance\",\n",
    "    \"üö® Alerting for Performance Degradation\"\n",
    "]\n",
    "\n",
    "for aspect in monitoring_aspects:\n",
    "    print(f\"   {aspect}\")\n",
    "\n",
    "print(\"\\nüéì Next Steps for Production:\")\n",
    "next_steps = [\n",
    "    \"1. Set up CI/CD pipeline for model deployment\",\n",
    "    \"2. Implement A/B testing for model comparison\",\n",
    "    \"3. Create monitoring dashboard and alerts\",\n",
    "    \"4. Establish model retraining schedule\",\n",
    "    \"5. Document model limitations and assumptions\",\n",
    "    \"6. Plan for model versioning and rollback\",\n",
    "    \"7. Conduct security and compliance review\",\n",
    "    \"8. Train operations team on model maintenance\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(\"\\n‚úÖ Your model is ready for production deployment!\")\n",
    "print(\"üéØ Remember: Machine learning in production is a continuous process\")\n",
    "print(\"üìà Regular monitoring and updates ensure long-term success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Workshop Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì WEEK 1 WORKSHOP COMPLETE!\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"üèÜ Congratulations! You've successfully completed the Week 1 workshop.\")\n",
    "print(\"You now have hands-on experience with production-ready ML pipelines!\")\n",
    "\n",
    "print(\"\\nüìö What You've Accomplished:\")\n",
    "accomplishments = [\n",
    "    \"‚úÖ Built a complete Metaflow ML pipeline from scratch\",\n",
    "    \"‚úÖ Mastered comprehensive data exploration techniques\",\n",
    "    \"‚úÖ Implemented robust data preprocessing workflows\",\n",
    "    \"‚úÖ Compared multiple ML algorithms systematically\",\n",
    "    \"‚úÖ Created professional data visualizations\",\n",
    "    \"‚úÖ Generated automated analysis reports\",\n",
    "    \"‚úÖ Assessed production readiness criteria\",\n",
    "    \"‚úÖ Learned MLOps best practices with Metaflow\"\n",
    "]\n",
    "\n",
    "for accomplishment in accomplishments:\n",
    "    print(f\"   {accomplishment}\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Skills You've Developed:\")\n",
    "skills = {\n",
    "    \"Technical Skills\": [\n",
    "        \"Metaflow workflow development\",\n",
    "        \"Pandas data manipulation and analysis\",\n",
    "        \"Scikit-learn model training and evaluation\",\n",
    "        \"Matplotlib and Seaborn visualization\",\n",
    "        \"Cross-validation and model selection\"\n",
    "    ],\n",
    "    \"MLOps Practices\": [\n",
    "        \"Reproducible pipeline design\",\n",
    "        \"Automated model comparison\",\n",
    "        \"Comprehensive evaluation metrics\",\n",
    "        \"Error handling and robustness\",\n",
    "        \"Production readiness assessment\"\n",
    "    ],\n",
    "    \"Data Science Methodology\": [\n",
    "        \"Systematic data exploration\",\n",
    "        \"Feature analysis and selection\",\n",
    "        \"Model performance interpretation\",\n",
    "        \"Statistical validation techniques\",\n",
    "        \"Results communication and reporting\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, skill_list in skills.items():\n",
    "    print(f\"\\n   üéØ {category}:\")\n",
    "    for skill in skill_list:\n",
    "        print(f\"      ‚Ä¢ {skill}\")\n",
    "\n",
    "print(\"\\nüìÇ Repository Files Created:\")\n",
    "files_created = [\n",
    "    \"üìì 01_environment_verification.ipynb - Setup verification\",\n",
    "    \"üìì 02_metaflow_fundamentals.ipynb - Metaflow basics\",\n",
    "    \"üìì 03_data_exploration.ipynb - Data analysis techniques\",\n",
    "    \"üìì 04_visualization_basics.ipynb - Visualization mastery\",\n",
    "    \"üìì 05_complete_ml_pipeline.ipynb - Full pipeline (this notebook)\",\n",
    "    \"üåä workshop_intro_flow.py - Basic Metaflow example\",\n",
    "    \"üåä wine_classification_flow.py - Complete ML pipeline\",\n",
    "    \"üîß setup_test.py - Environment verification script\",\n",
    "    \"üì¶ requirements.txt - Package dependencies\",\n",
    "    \"üìã README.md - Complete documentation\"\n",
    "]\n",
    "\n",
    "for file_desc in files_created:\n",
    "    print(f\"   {file_desc}\")\n",
    "\n",
    "print(\"\\nüöÄ What's Coming in Week 2:\")\n",
    "week2_topics = [\n",
    "    \"üîó Introduction to LangChain and LCEL\",\n",
    "    \"üß† Large Language Model integration\",\n",
    "    \"üîÑ Advanced data processing workflows\",\n",
    "    \"üìä Combining traditional ML with LLMs\",\n",
    "    \"üéØ Text processing and NLP pipelines\",\n",
    "    \"üåê Building AI applications with LangChain\"\n",
    "]\n",
    "\n",
    "for topic in week2_topics:\n",
    "    print(f\"   {topic}\")\n",
    "\n",
    "print(\"\\nüí° Recommended Practice Before Week 2:\")\n",
    "practice_recommendations = [\n",
    "    \"üîÑ Run the complete pipeline with different datasets\",\n",
    "    \"‚öôÔ∏è Experiment with different model parameters\",\n",
    "    \"üìä Create custom visualizations for your data\",\n",
    "    \"üîß Try the production deployment code example\",\n",
    "    \"üìö Review LangChain documentation basics\",\n",
    "    \"üéØ Complete the practice exercises in the repository\"\n",
    "]\n",
    "\n",
    "for recommendation in practice_recommendations:\n",
    "    print(f\"   {recommendation}\")\n",
    "\n",
    "print(\"\\nü§ù Getting Help and Support:\")\n",
    "support_options = [\n",
    "    \"üí¨ Google Chat #urgent-help for technical issues\",\n",
    "    \"üìß Email facilitators for questions\",\n",
    "    \"üïê Friday office hours (2:00-3:00 PM)\",\n",
    "    \"üìö Workshop repository for reference materials\",\n",
    "    \"üë• Peer collaboration in discussion channels\"\n",
    "]\n",
    "\n",
    "for option in support_options:\n",
    "    print(f\"   {option}\")\n",
    "\n",
    "print(\"\\nüéâ Final Thoughts:\")\n",
    "print(\"You've just completed a comprehensive journey through modern MLOps!\")\n",
    "print(\"The pipeline you've built today represents industry best practices\")\n",
    "print(\"and can serve as a template for your future ML projects.\")\n",
    "print(\"\")\n",
    "print(\"Remember: Great ML engineers combine technical skills with\")\n",
    "print(\"systematic thinking and attention to production details.\")\n",
    "print(\"You're well on your way to becoming exactly that!\")\n",
    "print(\"\")\n",
    "print(\"üöÄ Keep building, keep learning, and see you in Week 2!\")\n",
    "print(\"\")\n",
    "print(\"üèÜ Happy Machine Learning!\")\n",
    "print(\"   - The INRIVA AI Academy Team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Complete Pipeline for Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the complete pipeline file for actual execution\n",
    "pipeline_code = '''\n",
    "\"\"\"\\nINRIVA AI Academy - Complete ML Pipeline\n",
    "=======================================\n",
    "\n",
    "Save this as complete_ml_pipeline.py and run:\n",
    "python complete_ml_pipeline.py run\n",
    "\"\"\"\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, catch\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# [The complete CompleteMLPipeline class would be inserted here]\n",
    "# This is the same class defined in cell 2 above\n",
    "\n",
    "if __name__ == \\'__main__\\':\n",
    "    CompleteMLPipeline()\n",
    "'''\n",
    "\n",
    "print(\"üíæ Complete Pipeline Code Template Generated!\")\n",
    "print(\"\")\n",
    "print(\"üìã To use this pipeline:\")\n",
    "print(\"\")\n",
    "print(\"1. üìÑ Create a new file called 'complete_ml_pipeline.py'\")\n",
    "print(\"2. üìù Copy the CompleteMLPipeline class from cell 2 above\")\n",
    "print(\"3. üíæ Add the imports and main block\")\n",
    "print(\"4. ‚ñ∂Ô∏è Run: python complete_ml_pipeline.py run\")\n",
    "print(\"5. üìä View results: python complete_ml_pipeline.py show\")\n",
    "print(\"\")\n",
    "print(\"üîß Example commands:\")\n",
    "print(\"   # Basic run\")\n",
    "print(\"   python complete_ml_pipeline.py run\")\n",
    "print(\"\")\n",
    "print(\"   # Custom parameters\")\n",
    "print(\"   python complete_ml_pipeline.py run --test_size 0.3 --cv_folds 10\")\n",
    "print(\"\")\n",
    "print(\"   # Specific models\")\n",
    "print(\"   python complete_ml_pipeline.py run --models 'random_forest,svm'\")\n",
    "print(\"\")\n",
    "print(\"üìà Access results programmatically:\")\n",
    "print(\"   from metaflow import Flow\")\n",
    "print(\"   flow = Flow('CompleteMLPipeline')\")\n",
    "print(\"   run = flow.latest_run\")\n",
    "print(\"   report = run['end'].task.data.final_report\")\n",
    "print(\"\")\n",
    "print(\"‚ú® The pipeline automatically saves all artifacts and generates comprehensive reports!\")\n",
    "print(\"üéØ This represents a production-ready MLOps workflow using industry best practices.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}