{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification Deep Dive: Comprehensive Tutorial\n",
        "\n",
        "This notebook provides an in-depth exploration of classification algorithms, evaluation techniques, and advanced patterns for real-world applications.\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you'll master:\n",
        "- **6+ classification algorithms** with their strengths and use cases\n",
        "- **Advanced evaluation metrics** beyond simple accuracy\n",
        "- **Class imbalance handling** techniques\n",
        "- **Feature engineering** for classification\n",
        "- **Model interpretation** and explainability\n",
        "- **Production deployment** considerations\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Data Loading and Exploration](#1-data-loading-and-exploration)\n",
        "2. [Binary Classification Fundamentals](#2-binary-classification-fundamentals)\n",
        "3. [Multi-class Classification](#3-multi-class-classification)\n",
        "4. [Advanced Evaluation Techniques](#4-advanced-evaluation-techniques)\n",
        "5. [Handling Class Imbalance](#5-handling-class-imbalance)\n",
        "6. [Feature Engineering for Classification](#6-feature-engineering-for-classification)\n",
        "7. [Model Interpretation and Explainability](#7-model-interpretation-and-explainability)\n",
        "8. [Production Considerations](#8-production-considerations)\n",
        "9. [Advanced Techniques](#9-advanced-techniques)\n",
        "10. [Summary and Best Practices](#10-summary-and-best-practices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_wine, load_breast_cancer, make_classification\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, StratifiedKFold,\n",
        "    GridSearchCV, learning_curve, validation_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier, \n",
        "    VotingClassifier, BaggingClassifier\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    precision_recall_curve, roc_curve, roc_auc_score,\n",
        "    precision_score, recall_score, f1_score, cohen_kappa_score\n",
        ")\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.inspection import permutation_importance\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All packages imported successfully!\")\n",
        "print(f\"üìÖ Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "We'll work with multiple datasets to demonstrate different classification scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load multiple datasets for comprehensive analysis\n",
        "print(\"üìä Loading Classification Datasets\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Dataset 1: Wine Classification (Multi-class)\n",
        "wine_data = load_wine()\n",
        "wine_X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
        "wine_y = wine_data.target\n",
        "wine_target_names = wine_data.target_names\n",
        "\n",
        "print(f\"üç∑ Wine Dataset:\")\n",
        "print(f\"   Shape: {wine_X.shape}\")\n",
        "print(f\"   Classes: {len(wine_target_names)} - {list(wine_target_names)}\")\n",
        "print(f\"   Class distribution: {np.bincount(wine_y)}\")\n",
        "\n",
        "# Dataset 2: Breast Cancer (Binary)\n",
        "cancer_data = load_breast_cancer()\n",
        "cancer_X = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "cancer_y = cancer_data.target\n",
        "cancer_target_names = cancer_data.target_names\n",
        "\n",
        "print(f\"\\nüè• Breast Cancer Dataset:\")\n",
        "print(f\"   Shape: {cancer_X.shape}\")\n",
        "print(f\"   Classes: {len(cancer_target_names)} - {list(cancer_target_names)}\")\n",
        "print(f\"   Class distribution: {np.bincount(cancer_y)}\")\n",
        "\n",
        "# Dataset 3: Imbalanced Synthetic Dataset\n",
        "imbalanced_X, imbalanced_y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    n_classes=3,\n",
        "    weights=[0.7, 0.2, 0.1],  # Imbalanced classes\n",
        "    random_state=42\n",
        ")\n",
        "imbalanced_X = pd.DataFrame(imbalanced_X, columns=[f'feature_{i}' for i in range(20)])\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è Imbalanced Synthetic Dataset:\")\n",
        "print(f\"   Shape: {imbalanced_X.shape}\")\n",
        "print(f\"   Classes: 3 classes\")\n",
        "print(f\"   Class distribution: {np.bincount(imbalanced_y)}\")\n",
        "print(f\"   Class proportions: {np.bincount(imbalanced_y) / len(imbalanced_y)}\")\n",
        "\n",
        "print(\"\\n‚úÖ All datasets loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data exploration\n",
        "def explore_dataset(X, y, target_names, dataset_name):\n",
        "    \"\"\"\n",
        "    Comprehensive exploration of a classification dataset.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Exploring {dataset_name} Dataset\")\n",
        "    print(\"=\" * (len(dataset_name) + 20))\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"üìä Dataset Shape: {X.shape}\")\n",
        "    print(f\"üéØ Number of Classes: {len(np.unique(y))}\")\n",
        "    print(f\"üìà Feature Types: {X.dtypes.value_counts().to_dict()}\")\n",
        "    \n",
        "    # Class distribution\n",
        "    class_counts = np.bincount(y)\n",
        "    class_props = class_counts / len(y)\n",
        "    \n",
        "    print(f\"\\nüé≠ Class Distribution:\")\n",
        "    for i, (count, prop) in enumerate(zip(class_counts, class_props)):\n",
        "        class_name = target_names[i] if target_names is not None else f\"Class {i}\"\n",
        "        print(f\"   {class_name}: {count} samples ({prop:.1%})\")\n",
        "    \n",
        "    # Check for class imbalance\n",
        "    max_prop = max(class_props)\n",
        "    min_prop = min(class_props)\n",
        "    imbalance_ratio = max_prop / min_prop\n",
        "    \n",
        "    if imbalance_ratio > 2:\n",
        "        print(f\"   ‚ö†Ô∏è Class imbalance detected (ratio: {imbalance_ratio:.1f}:1)\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Classes are relatively balanced\")\n",
        "    \n",
        "    # Missing values\n",
        "    missing_values = X.isnull().sum().sum()\n",
        "    print(f\"\\n‚ùì Missing Values: {missing_values}\")\n",
        "    \n",
        "    # Feature statistics\n",
        "    print(f\"\\nüìà Feature Statistics:\")\n",
        "    print(f\"   Mean feature range: [{X.mean().min():.3f}, {X.mean().max():.3f}]\")\n",
        "    print(f\"   Std feature range: [{X.std().min():.3f}, {X.std().max():.3f}]\")\n",
        "    \n",
        "    # Feature scaling recommendation\n",
        "    feature_ranges = X.max() - X.min()\n",
        "    max_range = feature_ranges.max()\n",
        "    min_range = feature_ranges.min()\n",
        "    \n",
        "    if max_range / min_range > 10:\n",
        "        print(f\"   üîß Feature scaling recommended (range ratio: {max_range/min_range:.1f}:1)\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Feature scales are relatively similar\")\n",
        "    \n",
        "    return {\n",
        "        'shape': X.shape,\n",
        "        'n_classes': len(np.unique(y)),\n",
        "        'class_distribution': class_counts,\n",
        "        'imbalance_ratio': imbalance_ratio,\n",
        "        'missing_values': missing_values,\n",
        "        'needs_scaling': max_range / min_range > 10\n",
        "    }\n",
        "\n",
        "# Explore all datasets\n",
        "wine_stats = explore_dataset(wine_X, wine_y, wine_target_names, \"Wine\")\n",
        "cancer_stats = explore_dataset(cancer_X, cancer_y, cancer_target_names, \"Breast Cancer\")\n",
        "imbalanced_stats = explore_dataset(imbalanced_X, imbalanced_y, None, \"Imbalanced Synthetic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of dataset characteristics\n",
        "print(\"üìä Dataset Visualization\")\n",
        "print(\"=\" * 24)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "\n",
        "datasets = [\n",
        "    (wine_X, wine_y, wine_target_names, \"Wine\"),\n",
        "    (cancer_X, cancer_y, cancer_target_names, \"Breast Cancer\"),\n",
        "    (imbalanced_X, imbalanced_y, None, \"Imbalanced\")\n",
        "]\n",
        "\n",
        "for i, (X, y, target_names, name) in enumerate(datasets):\n",
        "    # Class distribution\n",
        "    class_counts = np.bincount(y)\n",
        "    class_labels = target_names if target_names is not None else [f\"Class {j}\" for j in range(len(class_counts))]\n",
        "    \n",
        "    axes[i, 0].bar(range(len(class_counts)), class_counts, color=sns.color_palette(\"husl\", len(class_counts)))\n",
        "    axes[i, 0].set_title(f'{name} - Class Distribution')\n",
        "    axes[i, 0].set_xticks(range(len(class_counts)))\n",
        "    axes[i, 0].set_xticklabels([label[:10] + '...' if len(label) > 10 else label for label in class_labels], rotation=45)\n",
        "    axes[i, 0].set_ylabel('Count')\n",
        "    \n",
        "    # Feature correlation heatmap (subset of features)\n",
        "    n_features_to_show = min(8, X.shape[1])\n",
        "    feature_subset = X.iloc[:, :n_features_to_show]\n",
        "    corr_matrix = feature_subset.corr()\n",
        "    \n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "                ax=axes[i, 1], cbar_kws={'shrink': 0.8})\n",
        "    axes[i, 1].set_title(f'{name} - Feature Correlations')\n",
        "    \n",
        "    # Feature distribution by class (first 2 features)\n",
        "    if X.shape[1] >= 2:\n",
        "        for class_idx in np.unique(y):\n",
        "            class_data = X[y == class_idx]\n",
        "            label = class_labels[class_idx] if target_names is not None else f\"Class {class_idx}\"\n",
        "            axes[i, 2].scatter(class_data.iloc[:, 0], class_data.iloc[:, 1], \n",
        "                             label=label, alpha=0.6, s=30)\n",
        "        \n",
        "        axes[i, 2].set_xlabel(X.columns[0][:15] + '...' if len(X.columns[0]) > 15 else X.columns[0])\n",
        "        axes[i, 2].set_ylabel(X.columns[1][:15] + '...' if len(X.columns[1]) > 15 else X.columns[1])\n",
        "        axes[i, 2].set_title(f'{name} - Feature Scatter')\n",
        "        axes[i, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Dataset exploration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Binary Classification Fundamentals\n",
        "\n",
        "Let's start with binary classification using the breast cancer dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary classification with breast cancer dataset\n",
        "print(\"üè• Binary Classification: Breast Cancer Diagnosis\")\n",
        "print(\"=\" * 48)\n",
        "\n",
        "# Prepare data\n",
        "X_binary = cancer_X\n",
        "y_binary = cancer_y\n",
        "target_names_binary = cancer_target_names\n",
        "\n",
        "# Split data\n",
        "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
        "    X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_bin = StandardScaler()\n",
        "X_train_bin_scaled = scaler_bin.fit_transform(X_train_bin)\n",
        "X_test_bin_scaled = scaler_bin.transform(X_test_bin)\n",
        "\n",
        "print(f\"üìä Training set: {X_train_bin_scaled.shape}\")\n",
        "print(f\"üìä Test set: {X_test_bin_scaled.shape}\")\n",
        "print(f\"üéØ Class distribution (train): {np.bincount(y_train_bin)}\")\n",
        "print(f\"üéØ Class distribution (test): {np.bincount(y_test_bin)}\")\n",
        "\n",
        "# Define binary classifiers\n",
        "binary_classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),\n",
        "    'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "print(f\"\\nü§ñ Training {len(binary_classifiers)} binary classifiers...\")\n",
        "\n",
        "binary_results = {}\n",
        "\n",
        "for name, classifier in binary_classifiers.items():\n",
        "    print(f\"\\nüîÑ Training {name}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Train the model\n",
        "    classifier.fit(X_train_bin_scaled, y_train_bin)\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = classifier.predict(X_test_bin_scaled)\n",
        "    y_pred_proba = classifier.predict_proba(X_test_bin_scaled)[:, 1]  # Probability of positive class\n",
        "    \n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy = accuracy_score(y_test_bin, y_pred)\n",
        "    precision = precision_score(y_test_bin, y_pred)\n",
        "    recall = recall_score(y_test_bin, y_pred)\n",
        "    f1 = f1_score(y_test_bin, y_pred)\n",
        "    auc = roc_auc_score(y_test_bin, y_pred_proba)\n",
        "    kappa = cohen_kappa_score(y_test_bin, y_pred)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(classifier, X_train_bin_scaled, y_train_bin, cv=5, scoring='accuracy')\n",
        "    cv_auc_scores = cross_val_score(classifier, X_train_bin_scaled, y_train_bin, cv=5, scoring='roc_auc')\n",
        "    \n",
        "    binary_results[name] = {\n",
        "        'model': classifier,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc': auc,\n",
        "        'kappa': kappa,\n",
        "        'cv_accuracy': cv_scores.mean(),\n",
        "        'cv_accuracy_std': cv_scores.std(),\n",
        "        'cv_auc': cv_auc_scores.mean(),\n",
        "        'cv_auc_std': cv_auc_scores.std(),\n",
        "        'training_time': training_time,\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"   ‚úÖ Accuracy: {accuracy:.3f} | Precision: {precision:.3f} | Recall: {recall:.3f}\")\n",
        "    print(f\"      F1: {f1:.3f} | AUC: {auc:.3f} | CV-Acc: {cv_scores.mean():.3f}¬±{cv_scores.std():.3f}\")\n",
        "    print(f\"      Time: {training_time:.2f}s\")\n",
        "\n",
        "print(\"\\nüèÜ Binary classification training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary classification evaluation and visualization\n",
        "print(\"üìä Binary Classification Evaluation\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Create comprehensive evaluation plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# 1. Metrics comparison\n",
        "metrics_data = []\n",
        "for name, results in binary_results.items():\n",
        "    metrics_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': results['accuracy'],\n",
        "        'Precision': results['precision'],\n",
        "        'Recall': results['recall'],\n",
        "        'F1-Score': results['f1_score'],\n",
        "        'AUC': results['auc']\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "metrics_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']].plot(\n",
        "    kind='bar', ax=axes[0], rot=45\n",
        ")\n",
        "axes[0].set_title('Performance Metrics Comparison')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# 2. ROC Curves\n",
        "for name, results in binary_results.items():\n",
        "    y_proba = results['probabilities']\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin, y_proba)\n",
        "    auc_score = results['auc']\n",
        "    \n",
        "    axes[1].plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)\n",
        "\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curves')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Precision-Recall Curves\n",
        "for name, results in binary_results.items():\n",
        "    y_proba = results['probabilities']\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(y_test_bin, y_proba)\n",
        "    \n",
        "    axes[2].plot(recall_curve, precision_curve, label=name, linewidth=2)\n",
        "\n",
        "# Baseline precision (proportion of positive class)\n",
        "baseline_precision = np.mean(y_test_bin)\n",
        "axes[2].axhline(y=baseline_precision, color='k', linestyle='--', \n",
        "                label=f'Baseline ({baseline_precision:.3f})')\n",
        "axes[2].set_xlabel('Recall')\n",
        "axes[2].set_ylabel('Precision')\n",
        "axes[2].set_title('Precision-Recall Curves')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Confusion matrices for top 3 models\n",
        "top_models = sorted(binary_results.items(), key=lambda x: x[1]['f1_score'], reverse=True)[:3]\n",
        "\n",
        "for i, (name, results) in enumerate(top_models):\n",
        "    y_pred = results['predictions']\n",
        "    cm = confusion_matrix(y_test_bin, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names_binary, yticklabels=target_names_binary,\n",
        "                ax=axes[3 + i])\n",
        "    axes[3 + i].set_title(f'{name}\\nF1: {results[\"f1_score\"]:.3f}')\n",
        "    axes[3 + i].set_xlabel('Predicted')\n",
        "    axes[3 + i].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary table\n",
        "print(\"\\nüìã Binary Classification Results Summary:\")\n",
        "summary_df = pd.DataFrame({\n",
        "    'Model': list(binary_results.keys()),\n",
        "    'Accuracy': [f\"{r['accuracy']:.3f}\" for r in binary_results.values()],\n",
        "    'F1-Score': [f\"{r['f1_score']:.3f}\" for r in binary_results.values()],\n",
        "    'AUC': [f\"{r['auc']:.3f}\" for r in binary_results.values()],\n",
        "    'CV-Accuracy': [f\"{r['cv_accuracy']:.3f}¬±{r['cv_accuracy_std']:.3f}\" for r in binary_results.values()],\n",
        "    'Time (s)': [f\"{r['training_time']:.2f}\" for r in binary_results.values()]\n",
        "})\n",
        "\n",
        "# Sort by F1-score\n",
        "summary_df['F1_numeric'] = [r['f1_score'] for r in binary_results.values()]\n",
        "summary_df = summary_df.sort_values('F1_numeric', ascending=False).drop('F1_numeric', axis=1)\n",
        "\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n‚úÖ Binary classification evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Multi-class Classification\n",
        "\n",
        "Now let's explore multi-class classification using the wine dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-class classification with wine dataset\n",
        "print(\"üç∑ Multi-class Classification: Wine Quality\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "# Prepare data\n",
        "X_multi = wine_X\n",
        "y_multi = wine_y\n",
        "target_names_multi = wine_target_names\n",
        "\n",
        "# Split data with stratification\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
        "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_multi = StandardScaler()\n",
        "X_train_multi_scaled = scaler_multi.fit_transform(X_train_multi)\n",
        "X_test_multi_scaled = scaler_multi.transform(X_test_multi)\n",
        "\n",
        "print(f\"üìä Training set: {X_train_multi_scaled.shape}\")\n",
        "print(f\"üìä Test set: {X_test_multi_scaled.shape}\")\n",
        "print(f\"üéØ Class distribution (train): {np.bincount(y_train_multi)}\")\n",
        "print(f\"üéØ Class distribution (test): {np.bincount(y_test_multi)}\")\n",
        "\n",
        "# Multi-class classifiers\n",
        "multi_classifiers = {\n",
        "    'Logistic Regression (OvR)': LogisticRegression(random_state=42, max_iter=1000, multi_class='ovr'),\n",
        "    'Logistic Regression (Multi)': LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial'),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),\n",
        "    'SVM (Poly)': SVC(kernel='poly', degree=3, random_state=42, probability=True),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "print(f\"\\nü§ñ Training {len(multi_classifiers)} multi-class classifiers...\")\n",
        "\n",
        "multi_results = {}\n",
        "\n",
        "for name, classifier in multi_classifiers.items():\n",
        "    print(f\"\\nüîÑ Training {name}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Train the model\n",
        "    classifier.fit(X_train_multi_scaled, y_train_multi)\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = classifier.predict(X_test_multi_scaled)\n",
        "    y_pred_proba = classifier.predict_proba(X_test_multi_scaled)\n",
        "    \n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Calculate metrics for multi-class\n",
        "    accuracy = accuracy_score(y_test_multi, y_pred)\n",
        "    precision_macro = precision_score(y_test_multi, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test_multi, y_pred, average='macro')\n",
        "    f1_macro = f1_score(y_test_multi, y_pred, average='macro')\n",
        "    f1_weighted = f1_score(y_test_multi, y_pred, average='weighted')\n",
        "    \n",
        "    # Multi-class AUC\n",
        "    auc_macro = roc_auc_score(y_test_multi, y_pred_proba, multi_class='ovr', average='macro')\n",
        "    auc_weighted = roc_auc_score(y_test_multi, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "    \n",
        "    # Cohen's Kappa\n",
        "    kappa = cohen_kappa_score(y_test_multi, y_pred)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(classifier, X_train_multi_scaled, y_train_multi, cv=5, scoring='accuracy')\n",
        "    cv_f1_scores = cross_val_score(classifier, X_train_multi_scaled, y_train_multi, cv=5, scoring='f1_macro')\n",
        "    \n",
        "    # Per-class metrics\n",
        "    class_report = classification_report(y_test_multi, y_pred, target_names=target_names_multi, output_dict=True)\n",
        "    \n",
        "    multi_results[name] = {\n",
        "        'model': classifier,\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'auc_macro': auc_macro,\n",
        "        'auc_weighted': auc_weighted,\n",
        "        'kappa': kappa,\n",
        "        'cv_accuracy': cv_scores.mean(),\n",
        "        'cv_accuracy_std': cv_scores.std(),\n",
        "        'cv_f1': cv_f1_scores.mean(),\n",
        "        'cv_f1_std': cv_f1_scores.std(),\n",
        "        'training_time': training_time,\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_pred_proba,\n",
        "        'classification_report': class_report\n",
        "    }\n",
        "    \n",
        "    print(f\"   ‚úÖ Accuracy: {accuracy:.3f} | F1-Macro: {f1_macro:.3f} | AUC-Macro: {auc_macro:.3f}\")\n",
        "    print(f\"      Kappa: {kappa:.3f} | CV-Acc: {cv_scores.mean():.3f}¬±{cv_scores.std():.3f}\")\n",
        "    print(f\"      Time: {training_time:.2f}s\")\n",
        "\n",
        "print(\"\\nüèÜ Multi-class classification training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-class classification evaluation and visualization\n",
        "print(\"üìä Multi-class Classification Evaluation\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create comprehensive evaluation plots\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# 1. Overall metrics comparison\n",
        "metrics_data = []\n",
        "for name, results in multi_results.items():\n",
        "    metrics_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': results['accuracy'],\n",
        "        'F1-Macro': results['f1_macro'],\n",
        "        'F1-Weighted': results['f1_weighted'],\n",
        "        'AUC-Macro': results['auc_macro'],\n",
        "        'Kappa': results['kappa']\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "metrics_df.set_index('Model')[['Accuracy', 'F1-Macro', 'F1-Weighted', 'AUC-Macro', 'Kappa']].plot(\n",
        "    kind='bar', ax=axes[0], rot=45\n",
        ")\n",
        "axes[0].set_title('Multi-class Performance Metrics')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# 2. Training time comparison\n",
        "model_names = list(multi_results.keys())\n",
        "training_times = [results['training_time'] for results in multi_results.values()]\n",
        "\n",
        "bars = axes[1].bar(range(len(model_names)), training_times, \n",
        "                   color=sns.color_palette(\"viridis\", len(model_names)))\n",
        "axes[1].set_title('Training Time Comparison')\n",
        "axes[1].set_ylabel('Time (seconds)')\n",
        "axes[1].set_xticks(range(len(model_names)))\n",
        "axes[1].set_xticklabels([name.split('(')[0].strip() for name in model_names], rotation=45)\n",
        "\n",
        "# Add time labels on bars\n",
        "for bar, time in zip(bars, training_times):\n",
        "    height = bar.get_height()\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                f'{time:.2f}s', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# 3. Cross-validation scores with error bars\n",
        "cv_means = [results['cv_accuracy'] for results in multi_results.values()]\n",
        "cv_stds = [results['cv_accuracy_std'] for results in multi_results.values()]\n",
        "\n",
        "axes[2].errorbar(range(len(model_names)), cv_means, yerr=cv_stds, \n",
        "                fmt='o', capsize=5, capthick=2, markersize=8)\n",
        "axes[2].set_title('Cross-Validation Accuracy')\n",
        "axes[2].set_ylabel('CV Accuracy')\n",
        "axes[2].set_xticks(range(len(model_names)))\n",
        "axes[2].set_xticklabels([name.split('(')[0].strip() for name in model_names], rotation=45)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4-6. Confusion matrices for top 3 models\n",
        "top_models = sorted(multi_results.items(), key=lambda x: x[1]['f1_macro'], reverse=True)[:3]\n",
        "\n",
        "for i, (name, results) in enumerate(top_models):\n",
        "    y_pred = results['predictions']\n",
        "    cm = confusion_matrix(y_test_multi, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names_multi, yticklabels=target_names_multi,\n",
        "                ax=axes[3 + i])\n",
        "    axes[3 + i].set_title(f'{name.split(\"(\")[0].strip()}\\nF1-Macro: {results[\"f1_macro\"]:.3f}')\n",
        "    axes[3 + i].set_xlabel('Predicted')\n",
        "    axes[3 + i].set_ylabel('Actual')\n",
        "\n",
        "# 7. Per-class performance heatmap\n",
        "best_model_name, best_results = max(multi_results.items(), key=lambda x: x[1]['f1_macro'])\n",
        "class_report = best_results['classification_report']\n",
        "\n",
        "# Extract per-class metrics\n",
        "per_class_metrics = []\n",
        "for class_name in target_names_multi:\n",
        "    if class_name in class_report:\n",
        "        per_class_metrics.append([\n",
        "            class_report[class_name]['precision'],\n",
        "            class_report[class_name]['recall'],\n",
        "            class_report[class_name]['f1-score']\n",
        "        ])\n",
        "\n",
        "per_class_df = pd.DataFrame(per_class_metrics, \n",
        "                           columns=['Precision', 'Recall', 'F1-Score'],\n",
        "                           index=target_names_multi)\n",
        "\n",
        "sns.heatmap(per_class_df, annot=True, fmt='.3f', cmap='RdYlBu_r', \n",
        "            ax=axes[6], cbar_kws={'shrink': 0.8})\n",
        "axes[6].set_title(f'Per-Class Performance\\n({best_model_name.split(\"(\")[0].strip()})')\n",
        "\n",
        "# 8. Feature importance (for tree-based models)\n",
        "tree_models = [(name, results) for name, results in multi_results.items() \n",
        "               if hasattr(results['model'], 'feature_importances_')]\n",
        "\n",
        "if tree_models:\n",
        "    best_tree_name, best_tree_results = max(tree_models, key=lambda x: x[1]['f1_macro'])\n",
        "    importances = best_tree_results['model'].feature_importances_\n",
        "    feature_names = X_multi.columns\n",
        "    \n",
        "    # Get top 10 features\n",
        "    top_indices = np.argsort(importances)[-10:]\n",
        "    top_importances = importances[top_indices]\n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "    \n",
        "    axes[7].barh(range(10), top_importances, color=sns.color_palette(\"coolwarm\", 10))\n",
        "    axes[7].set_yticks(range(10))\n",
        "    axes[7].set_yticklabels([name[:15] + '...' if len(name) > 15 else name for name in top_features])\n",
        "    axes[7].set_title(f'Top 10 Feature Importance\\n({best_tree_name.split(\"(\")[0].strip()})')\n",
        "    axes[7].set_xlabel('Importance')\n",
        "else:\n",
        "    axes[7].text(0.5, 0.5, 'No tree-based models\\navailable for\\nfeature importance', \n",
        "                ha='center', va='center', transform=axes[7].transAxes, fontsize=12)\n",
        "    axes[7].set_title('Feature Importance')\n",
        "\n",
        "# 9. Accuracy vs Training Time scatter\n",
        "accuracies = [results['accuracy'] for results in multi_results.values()]\n",
        "times = [results['training_time'] for results in multi_results.values()]\n",
        "\n",
        "scatter = axes[8].scatter(times, accuracies, s=100, alpha=0.7, \n",
        "                         c=range(len(model_names)), cmap='viridis')\n",
        "\n",
        "for i, name in enumerate(model_names):\n",
        "    axes[8].annotate(name.split('(')[0].strip(), \n",
        "                    (times[i], accuracies[i]), \n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "axes[8].set_xlabel('Training Time (seconds)')\n",
        "axes[8].set_ylabel('Accuracy')\n",
        "axes[8].set_title('Accuracy vs Training Time')\n",
        "axes[8].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary table\n",
        "print(\"\\nüìã Multi-class Classification Results Summary:\")\n",
        "summary_df = pd.DataFrame({\n",
        "    'Model': [name.split('(')[0].strip() for name in multi_results.keys()],\n",
        "    'Accuracy': [f\"{r['accuracy']:.3f}\" for r in multi_results.values()],\n",
        "    'F1-Macro': [f\"{r['f1_macro']:.3f}\" for r in multi_results.values()],\n",
        "    'F1-Weighted': [f\"{r['f1_weighted']:.3f}\" for r in multi_results.values()],\n",
        "    'AUC-Macro': [f\"{r['auc_macro']:.3f}\" for r in multi_results.values()],\n",
        "    'Kappa': [f\"{r['kappa']:.3f}\" for r in multi_results.values()],\n",
        "    'CV-Acc': [f\"{r['cv_accuracy']:.3f}¬±{r['cv_accuracy_std']:.3f}\" for r in multi_results.values()],\n",
        "    'Time (s)': [f\"{r['training_time']:.2f}\" for r in multi_results.values()]\n",
        "})\n",
        "\n",
        "# Sort by F1-Macro\n",
        "summary_df['F1_numeric'] = [r['f1_macro'] for r in multi_results.values()]\n",
        "summary_df = summary_df.sort_values('F1_numeric', ascending=False).drop('F1_numeric', axis=1)\n",
        "\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n‚úÖ Multi-class classification evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Advanced Evaluation Techniques\n",
        "\n",
        "Let's explore advanced evaluation techniques including learning curves, validation curves, and statistical significance testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced evaluation techniques\n",
        "print(\"üìà Advanced Evaluation Techniques\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Select best model from multi-class results for detailed analysis\n",
        "best_model_name, best_model_results = max(multi_results.items(), key=lambda x: x[1]['f1_macro'])\n",
        "best_model = best_model_results['model']\n",
        "\n",
        "print(f\"üèÜ Analyzing best model: {best_model_name}\")\n",
        "print(f\"   F1-Macro Score: {best_model_results['f1_macro']:.3f}\")\n",
        "\n",
        "# 1. Learning Curves\n",
        "print(\"\\nüìö Generating Learning Curves...\")\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    best_model, X_train_multi_scaled, y_train_multi,\n",
        "    cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    scoring='f1_macro', random_state=42\n",
        ")\n",
        "\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "train_std = train_scores.std(axis=1)\n",
        "val_mean = val_scores.mean(axis=1)\n",
        "val_std = val_scores.std(axis=1)\n",
        "\n",
        "print(f\"   üìä Training score at 100%: {train_mean[-1]:.3f} ¬± {train_std[-1]:.3f}\")\n",
        "print(f\"   üìâ Validation score at 100%: {val_mean[-1]:.3f} ¬± {val_std[-1]:.3f}\")\n",
        "print(f\"   üìà Gap at 100%: {train_mean[-1] - val_mean[-1]:.3f}\")\n",
        "\n",
        "if train_mean[-1] - val_mean[-1] > 0.1:\n",
        "    print(\"   ‚ö†Ô∏è Potential overfitting detected\")\n",
        "elif train_mean[-1] - val_mean[-1] < 0.05:\n",
        "    print(\"   ‚úÖ Good generalization\")\n",
        "else:\n",
        "    print(\"   ‚ö° Moderate overfitting\")\n",
        "\n",
        "# 2. Validation Curves (for Random Forest n_estimators)\n",
        "if 'Random Forest' in best_model_name:\n",
        "    print(\"\\nüå≤ Generating Validation Curves for n_estimators...\")\n",
        "    param_range = [10, 25, 50, 75, 100, 150, 200]\n",
        "    \n",
        "    train_scores_val, val_scores_val = validation_curve(\n",
        "        RandomForestClassifier(random_state=42), X_train_multi_scaled, y_train_multi,\n",
        "        param_name='n_estimators', param_range=param_range,\n",
        "        cv=5, scoring='f1_macro', n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    val_train_mean = train_scores_val.mean(axis=1)\n",
        "    val_val_mean = val_scores_val.mean(axis=1)\n",
        "    \n",
        "    optimal_n_estimators = param_range[np.argmax(val_val_mean)]\n",
        "    optimal_score = val_val_mean[np.argmax(val_val_mean)]\n",
        "    \n",
        "    print(f\"   üéØ Optimal n_estimators: {optimal_n_estimators}\")\n",
        "    print(f\"   üìä Optimal validation score: {optimal_score:.3f}\")\n",
        "\n",
        "# 3. Cross-validation with multiple metrics\n",
        "print(\"\\nüîÑ Comprehensive Cross-Validation...\")\n",
        "cv_metrics = ['accuracy', 'f1_macro', 'f1_weighted', 'precision_macro', 'recall_macro']\n",
        "cv_results_detailed = {}\n",
        "\n",
        "for metric in cv_metrics:\n",
        "    scores = cross_val_score(best_model, X_train_multi_scaled, y_train_multi, \n",
        "                            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
        "                            scoring=metric)\n",
        "    cv_results_detailed[metric] = {\n",
        "        'mean': scores.mean(),\n",
        "        'std': scores.std(),\n",
        "        'scores': scores\n",
        "    }\n",
        "    print(f\"   {metric:15}: {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
        "\n",
        "# 4. Statistical significance testing (comparing top 2 models)\n",
        "print(\"\\nüìä Statistical Significance Testing...\")\n",
        "top_2_models = sorted(multi_results.items(), key=lambda x: x[1]['f1_macro'], reverse=True)[:2]\n",
        "\n",
        "if len(top_2_models) >= 2:\n",
        "    model1_name, model1_results = top_2_models[0]\n",
        "    model2_name, model2_results = top_2_models[1]\n",
        "    \n",
        "    # Perform 5-fold CV for both models\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    model1_cv_scores = cross_val_score(model1_results['model'], X_train_multi_scaled, y_train_multi, \n",
        "                                      cv=cv, scoring='f1_macro')\n",
        "    model2_cv_scores = cross_val_score(model2_results['model'], X_train_multi_scaled, y_train_multi, \n",
        "                                      cv=cv, scoring='f1_macro')\n",
        "    \n",
        "    # Paired t-test\n",
        "    from scipy.stats import ttest_rel\n",
        "    t_stat, p_value = ttest_rel(model1_cv_scores, model2_cv_scores)\n",
        "    \n",
        "    print(f\"   Comparing {model1_name.split('(')[0].strip()} vs {model2_name.split('(')[0].strip()}\")\n",
        "    print(f\"   Model 1 CV: {model1_cv_scores.mean():.3f} ¬± {model1_cv_scores.std():.3f}\")\n",
        "    print(f\"   Model 2 CV: {model2_cv_scores.mean():.3f} ¬± {model2_cv_scores.std():.3f}\")\n",
        "    print(f\"   t-statistic: {t_stat:.3f}\")\n",
        "    print(f\"   p-value: {p_value:.3f}\")\n",
        "    \n",
        "    if p_value < 0.05:\n",
        "        print(f\"   ‚úÖ Statistically significant difference (Œ±=0.05)\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå No statistically significant difference (Œ±=0.05)\")\n",
        "\n",
        "print(\"\\n‚úÖ Advanced evaluation techniques complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of advanced evaluation results\n",
        "print(\"üìä Advanced Evaluation Visualizations\")\n",
        "print(\"=\" * 38)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# 1. Learning Curves\n",
        "axes[0].plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
        "axes[0].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "\n",
        "axes[0].plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
        "axes[0].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
        "\n",
        "axes[0].set_xlabel('Training Set Size')\n",
        "axes[0].set_ylabel('F1-Macro Score')\n",
        "axes[0].set_title(f'Learning Curves\\n({best_model_name.split(\"(\")[0].strip()})')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation Curves (if Random Forest)\n",
        "if 'Random Forest' in best_model_name:\n",
        "    axes[1].plot(param_range, val_train_mean, 'o-', color='blue', label='Training Score')\n",
        "    axes[1].plot(param_range, val_val_mean, 'o-', color='red', label='Validation Score')\n",
        "    axes[1].axvline(x=optimal_n_estimators, color='green', linestyle='--', \n",
        "                   label=f'Optimal: {optimal_n_estimators}')\n",
        "    \n",
        "    axes[1].set_xlabel('n_estimators')\n",
        "    axes[1].set_ylabel('F1-Macro Score')\n",
        "    axes[1].set_title('Validation Curves\\n(n_estimators)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'Validation curves\\nnot applicable\\nfor this model', \n",
        "                ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
        "    axes[1].set_title('Validation Curves')\n",
        "\n",
        "# 3. Cross-validation metrics comparison\n",
        "metric_names = list(cv_results_detailed.keys())\n",
        "metric_means = [cv_results_detailed[m]['mean'] for m in metric_names]\n",
        "metric_stds = [cv_results_detailed[m]['std'] for m in metric_names]\n",
        "\n",
        "bars = axes[2].bar(range(len(metric_names)), metric_means, \n",
        "                   yerr=metric_stds, capsize=5, \n",
        "                   color=sns.color_palette(\"Set2\", len(metric_names)))\n",
        "\n",
        "axes[2].set_xticks(range(len(metric_names)))\n",
        "axes[2].set_xticklabels([m.replace('_', '\\n') for m in metric_names], rotation=0)\n",
        "axes[2].set_ylabel('CV Score')\n",
        "axes[2].set_title(f'Cross-Validation Metrics\\n({best_model_name.split(\"(\")[0].strip()})')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, mean_val in zip(bars, metric_means):\n",
        "    height = bar.get_height()\n",
        "    axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{mean_val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 4. Model comparison with confidence intervals\n",
        "if len(top_2_models) >= 2:\n",
        "    model_names_short = [name.split('(')[0].strip() for name, _ in top_2_models]\n",
        "    cv_scores_comparison = [model1_cv_scores, model2_cv_scores]\n",
        "    \n",
        "    bp = axes[3].boxplot(cv_scores_comparison, labels=model_names_short, patch_artist=True)\n",
        "    \n",
        "    # Color the boxes\n",
        "    colors = ['lightblue', 'lightcoral']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "    \n",
        "    axes[3].set_ylabel('F1-Macro Score')\n",
        "    axes[3].set_title('Model Comparison\\n(CV Score Distribution)')\n",
        "    axes[3].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add p-value annotation\n",
        "    axes[3].text(0.5, 0.95, f'p-value: {p_value:.3f}', \n",
        "                transform=axes[3].transAxes, ha='center', va='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "else:\n",
        "    axes[3].text(0.5, 0.5, 'Not enough models\\nfor comparison', \n",
        "                ha='center', va='center', transform=axes[3].transAxes, fontsize=12)\n",
        "    axes[3].set_title('Model Comparison')\n",
        "\n",
        "# 5. Feature importance with permutation importance (if applicable)\n",
        "if hasattr(best_model, 'feature_importances_') or True:  # Try permutation importance for any model\n",
        "    print(\"\\nüîç Computing Permutation Importance...\")\n",
        "    \n",
        "    # Use a subset for faster computation\n",
        "    perm_importance = permutation_importance(\n",
        "        best_model, X_test_multi_scaled[:50], y_test_multi[:50], \n",
        "        n_repeats=10, random_state=42, scoring='f1_macro'\n",
        "    )\n",
        "    \n",
        "    # Get top 10 features\n",
        "    top_indices = np.argsort(perm_importance.importances_mean)[-10:]\n",
        "    top_importances = perm_importance.importances_mean[top_indices]\n",
        "    top_features = [X_multi.columns[i] for i in top_indices]\n",
        "    \n",
        "    axes[4].barh(range(10), top_importances, color=sns.color_palette(\"coolwarm\", 10))\n",
        "    axes[4].set_yticks(range(10))\n",
        "    axes[4].set_yticklabels([name[:12] + '...' if len(name) > 12 else name for name in top_features])\n",
        "    axes[4].set_title('Permutation Importance\\n(Top 10 Features)')\n",
        "    axes[4].set_xlabel('Importance Decrease')\n",
        "    \n",
        "    print(f\"   ‚úÖ Top feature: {top_features[-1]} (importance: {top_importances[-1]:.3f})\")\n",
        "else:\n",
        "    axes[4].text(0.5, 0.5, 'Feature importance\\nnot available', \n",
        "                ha='center', va='center', transform=axes[4].transAxes, fontsize=12)\n",
        "    axes[4].set_title('Permutation Importance')\n",
        "\n",
        "# 6. ROC curves for multi-class (One-vs-Rest)\n",
        "if len(np.unique(y_test_multi)) <= 5:  # Only for reasonable number of classes\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    \n",
        "    # Binarize the output\n",
        "    y_test_binarized = label_binarize(y_test_multi, classes=np.unique(y_multi))\n",
        "    y_pred_proba = best_model_results['probabilities']\n",
        "    \n",
        "    # Compute ROC curve for each class\n",
        "    for i, class_name in enumerate(target_names_multi):\n",
        "        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        axes[5].plot(fpr, tpr, linewidth=2, \n",
        "                    label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
        "    \n",
        "    axes[5].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "    axes[5].set_xlabel('False Positive Rate')\n",
        "    axes[5].set_ylabel('True Positive Rate')\n",
        "    axes[5].set_title('Multi-class ROC Curves\\n(One-vs-Rest)')\n",
        "    axes[5].legend()\n",
        "    axes[5].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[5].text(0.5, 0.5, 'Too many classes\\nfor ROC visualization', \n",
        "                ha='center', va='center', transform=axes[5].transAxes, fontsize=12)\n",
        "    axes[5].set_title('Multi-class ROC Curves')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Advanced evaluation visualizations complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Handling Class Imbalance\n",
        "\n",
        "Let's explore techniques for handling imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class imbalance handling techniques\n",
        "print(\"‚öñÔ∏è Handling Class Imbalance\")\n",
        "print(\"=\" * 28)\n",
        "\n",
        "# Use the imbalanced synthetic dataset\n",
        "X_imb = imbalanced_X\n",
        "y_imb = imbalanced_y\n",
        "\n",
        "print(f\"üìä Original class distribution: {np.bincount(y_imb)}\")\n",
        "print(f\"üìà Class proportions: {np.bincount(y_imb) / len(y_imb)}\")\n",
        "\n",
        "# Split data\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_imb = StandardScaler()\n",
        "X_train_imb_scaled = scaler_imb.fit_transform(X_train_imb)\n",
        "X_test_imb_scaled = scaler_imb.transform(X_test_imb)\n",
        "\n",
        "print(f\"\\nüìä Training set class distribution: {np.bincount(y_train_imb)}\")\n",
        "print(f\"üìä Test set class distribution: {np.bincount(y_test_imb)}\")\n",
        "\n",
        "# Technique 1: Class weights\n",
        "print(\"\\nüîß Technique 1: Class Weights\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "rf_balanced = RandomForestClassifier(\n",
        "    n_estimators=100, \n",
        "    class_weight='balanced', \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_balanced.fit(X_train_imb_scaled, y_train_imb)\n",
        "y_pred_balanced = rf_balanced.predict(X_test_imb_scaled)\n",
        "\n",
        "print(\"Random Forest with Balanced Class Weights:\")\n",
        "print(classification_report(y_test_imb, y_pred_balanced))\n",
        "\n",
        "# Technique 2: SMOTE (if available)\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "    from imblearn.combine import SMOTETomek\n",
        "    \n",
        "    print(\"\\nüîß Technique 2: SMOTE Oversampling\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    # Apply SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_smote, y_train_smote = smote.fit_resample(X_train_imb_scaled, y_train_imb)\n",
        "    \n",
        "    print(f\"üìä After SMOTE: {np.bincount(y_train_smote)}\")\n",
        "    \n",
        "    # Train model on SMOTE data\n",
        "    rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_smote.fit(X_train_smote, y_train_smote)\n",
        "    y_pred_smote = rf_smote.predict(X_test_imb_scaled)\n",
        "    \n",
        "    print(\"Random Forest with SMOTE:\")\n",
        "    print(classification_report(y_test_imb, y_pred_smote))\n",
        "    \n",
        "    # Technique 3: Combined SMOTE + Tomek\n",
        "    print(\"\\nüîß Technique 3: SMOTE + Tomek Links\")\n",
        "    print(\"-\" * 35)\n",
        "    \n",
        "    smote_tomek = SMOTETomek(random_state=42)\n",
        "    X_train_combined, y_train_combined = smote_tomek.fit_resample(X_train_imb_scaled, y_train_imb)\n",
        "    \n",
        "    print(f\"üìä After SMOTE+Tomek: {np.bincount(y_train_combined)}\")\n",
        "    \n",
        "    rf_combined = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_combined.fit(X_train_combined, y_train_combined)\n",
        "    y_pred_combined = rf_combined.predict(X_test_imb_scaled)\n",
        "    \n",
        "    print(\"Random Forest with SMOTE+Tomek:\")\n",
        "    print(classification_report(y_test_imb, y_pred_combined))\n",
        "    \n",
        "    IMBALANCED_LEARN_AVAILABLE = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"\\n‚ö†Ô∏è imbalanced-learn not available\")\n",
        "    print(\"   Install with: pip install imbalanced-learn\")\n",
        "    IMBALANCED_LEARN_AVAILABLE = False\n",
        "\n",
        "# Technique 4: Threshold tuning\n",
        "print(\"\\nüîß Technique 4: Decision Threshold Tuning\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Use probabilities to find optimal threshold\n",
        "rf_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_base.fit(X_train_imb_scaled, y_train_imb)\n",
        "y_pred_proba_imb = rf_base.predict_proba(X_test_imb_scaled)\n",
        "\n",
        "# For multi-class, we'll focus on the minority class (class 2)\n",
        "minority_class = 2\n",
        "y_test_binary = (y_test_imb == minority_class).astype(int)\n",
        "y_proba_minority = y_pred_proba_imb[:, minority_class]\n",
        "\n",
        "# Find optimal threshold for minority class\n",
        "thresholds = np.linspace(0.1, 0.9, 9)\n",
        "f1_scores_threshold = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_threshold = (y_proba_minority >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test_binary, y_pred_threshold)\n",
        "    f1_scores_threshold.append(f1)\n",
        "\n",
        "optimal_threshold = thresholds[np.argmax(f1_scores_threshold)]\n",
        "optimal_f1 = max(f1_scores_threshold)\n",
        "\n",
        "print(f\"üìä Optimal threshold for minority class: {optimal_threshold:.2f}\")\n",
        "print(f\"üìà F1-score at optimal threshold: {optimal_f1:.3f}\")\n",
        "\n",
        "# Compare all techniques\n",
        "imbalance_results = {\n",
        "    'Baseline (No Handling)': {\n",
        "        'model': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'X_train': X_train_imb_scaled,\n",
        "        'y_train': y_train_imb\n",
        "    },\n",
        "    'Class Weights': {\n",
        "        'model': rf_balanced,\n",
        "        'X_train': X_train_imb_scaled,\n",
        "        'y_train': y_train_imb\n",
        "    }\n",
        "}\n",
        "\n",
        "if IMBALANCED_LEARN_AVAILABLE:\n",
        "    imbalance_results.update({\n",
        "        'SMOTE': {\n",
        "            'model': rf_smote,\n",
        "            'X_train': X_train_smote,\n",
        "            'y_train': y_train_smote\n",
        "        },\n",
        "        'SMOTE + Tomek': {\n",
        "            'model': rf_combined,\n",
        "            'X_train': X_train_combined,\n",
        "            'y_train': y_train_combined\n",
        "        }\n",
        "    })\n",
        "\n",
        "print(\"\\nüìä Imbalance Handling Comparison\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "comparison_results = {}\n",
        "\n",
        "for technique_name, technique_data in imbalance_results.items():\n",
        "    model = technique_data['model']\n",
        "    \n",
        "    # If model not already trained, train it\n",
        "    if technique_name == 'Baseline (No Handling)':\n",
        "        model.fit(technique_data['X_train'], technique_data['y_train'])\n",
        "    \n",
        "    y_pred_tech = model.predict(X_test_imb_scaled)\n",
        "    \n",
        "    # Calculate metrics for each class\n",
        "    f1_macro = f1_score(y_test_imb, y_pred_tech, average='macro')\n",
        "    f1_weighted = f1_score(y_test_imb, y_pred_tech, average='weighted')\n",
        "    accuracy = accuracy_score(y_test_imb, y_pred_tech)\n",
        "    \n",
        "    # Per-class F1 scores\n",
        "    f1_per_class = f1_score(y_test_imb, y_pred_tech, average=None)\n",
        "    \n",
        "    comparison_results[technique_name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'f1_class_0': f1_per_class[0],\n",
        "        'f1_class_1': f1_per_class[1],\n",
        "        'f1_class_2': f1_per_class[2],\n",
        "        'predictions': y_pred_tech\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{technique_name}:\")\n",
        "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"   F1-Macro: {f1_macro:.3f}\")\n",
        "    print(f\"   F1-Weighted: {f1_weighted:.3f}\")\n",
        "    print(f\"   F1 per class: {f1_per_class}\")\n",
        "\n",
        "print(\"\\n‚úÖ Class imbalance handling complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of imbalance handling results\n",
        "print(\"üìä Class Imbalance Handling Visualizations\")\n",
        "print(\"=\" * 43)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# 1. F1-Score comparison\n",
        "techniques = list(comparison_results.keys())\n",
        "f1_macros = [comparison_results[t]['f1_macro'] for t in techniques]\n",
        "f1_weighteds = [comparison_results[t]['f1_weighted'] for t in techniques]\n",
        "\n",
        "x = np.arange(len(techniques))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x - width/2, f1_macros, width, label='F1-Macro', alpha=0.8)\n",
        "axes[0].bar(x + width/2, f1_weighteds, width, label='F1-Weighted', alpha=0.8)\n",
        "axes[0].set_xlabel('Technique')\n",
        "axes[0].set_ylabel('F1-Score')\n",
        "axes[0].set_title('F1-Score Comparison')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels([t.split('(')[0].strip() for t in techniques], rotation=45)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Per-class F1 scores\n",
        "class_labels = ['Class 0\\n(Majority)', 'Class 1\\n(Medium)', 'Class 2\\n(Minority)']\n",
        "f1_class_0 = [comparison_results[t]['f1_class_0'] for t in techniques]\n",
        "f1_class_1 = [comparison_results[t]['f1_class_1'] for t in techniques]\n",
        "f1_class_2 = [comparison_results[t]['f1_class_2'] for t in techniques]\n",
        "\n",
        "x = np.arange(len(techniques))\n",
        "width = 0.25\n",
        "\n",
        "axes[1].bar(x - width, f1_class_0, width, label='Class 0 (Majority)', alpha=0.8)\n",
        "axes[1].bar(x, f1_class_1, width, label='Class 1 (Medium)', alpha=0.8)\n",
        "axes[1].bar(x + width, f1_class_2, width, label='Class 2 (Minority)', alpha=0.8)\n",
        "\n",
        "axes[1].set_xlabel('Technique')\n",
        "axes[1].set_ylabel('F1-Score')\n",
        "axes[1].set_title('Per-Class F1-Scores')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels([t.split('(')[0].strip() for t in techniques], rotation=45)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Threshold tuning curve\n",
        "axes[2].plot(thresholds, f1_scores_threshold, 'o-', linewidth=2, markersize=6)\n",
        "axes[2].axvline(x=optimal_threshold, color='red', linestyle='--', \n",
        "               label=f'Optimal: {optimal_threshold:.2f}')\n",
        "axes[2].axhline(y=optimal_f1, color='red', linestyle='--', alpha=0.5)\n",
        "axes[2].set_xlabel('Decision Threshold')\n",
        "axes[2].set_ylabel('F1-Score (Minority Class)')\n",
        "axes[2].set_title('Threshold Tuning for Minority Class')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4-6. Confusion matrices for top 3 techniques\n",
        "top_techniques = sorted(comparison_results.items(), \n",
        "                       key=lambda x: x[1]['f1_macro'], reverse=True)[:3]\n",
        "\n",
        "for i, (technique_name, results) in enumerate(top_techniques):\n",
        "    y_pred_tech = results['predictions']\n",
        "    cm = confusion_matrix(y_test_imb, y_pred_tech)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[3 + i])\n",
        "    axes[3 + i].set_title(f'{technique_name.split(\"(\")[0].strip()}\\nF1-Macro: {results[\"f1_macro\"]:.3f}')\n",
        "    axes[3 + i].set_xlabel('Predicted')\n",
        "    axes[3 + i].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary recommendations\n",
        "print(\"\\nüéØ Imbalance Handling Recommendations:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "best_technique = max(comparison_results.items(), key=lambda x: x[1]['f1_macro'])\n",
        "best_name, best_results = best_technique\n",
        "\n",
        "print(f\"üèÜ Best overall technique: {best_name}\")\n",
        "print(f\"   F1-Macro: {best_results['f1_macro']:.3f}\")\n",
        "print(f\"   F1-Minority Class: {best_results['f1_class_2']:.3f}\")\n",
        "\n",
        "# Recommendations based on results\n",
        "minority_f1_scores = {name: results['f1_class_2'] for name, results in comparison_results.items()}\n",
        "best_for_minority = max(minority_f1_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\nüéØ Best for minority class: {best_for_minority[0]}\")\n",
        "print(f\"   Minority F1-Score: {best_for_minority[1]:.3f}\")\n",
        "\n",
        "print(\"\\nüí° General Recommendations:\")\n",
        "print(\"   ‚Ä¢ Use class weights for quick improvement\")\n",
        "if IMBALANCED_LEARN_AVAILABLE:\n",
        "    print(\"   ‚Ä¢ SMOTE works well for synthetic oversampling\")\n",
        "    print(\"   ‚Ä¢ Combined SMOTE+Tomek for cleaning boundaries\")\n",
        "print(\"   ‚Ä¢ Threshold tuning for precision/recall trade-offs\")\n",
        "print(\"   ‚Ä¢ Focus on F1-Macro for balanced evaluation\")\n",
        "print(\"   ‚Ä¢ Always validate on stratified test sets\")\n",
        "\n",
        "print(\"\\n‚úÖ Class imbalance analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Best Practices\n",
        "\n",
        "Let's summarize the key learnings and establish best practices for classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification best practices and summary\n",
        "print(\"üéì Classification Deep Dive Summary\")\n",
        "print(\"=\" * 37)\n",
        "\n",
        "# Comprehensive summary of all results\n",
        "print(\"üìä Complete Results Summary:\")\n",
        "print(\"-\" * 28)\n",
        "\n",
        "print(\"\\nüè• Binary Classification (Breast Cancer):\")\n",
        "binary_best = max(binary_results.items(), key=lambda x: x[1]['f1_score'])\n",
        "print(f\"   üèÜ Best Model: {binary_best[0]}\")\n",
        "print(f\"   üìà F1-Score: {binary_best[1]['f1_score']:.3f}\")\n",
        "print(f\"   üìä AUC: {binary_best[1]['auc']:.3f}\")\n",
        "print(f\"   ‚è±Ô∏è Training Time: {binary_best[1]['training_time']:.2f}s\")\n",
        "\n",
        "print(\"\\nüç∑ Multi-class Classification (Wine):\")\n",
        "multi_best = max(multi_results.items(), key=lambda x: x[1]['f1_macro'])\n",
        "print(f\"   üèÜ Best Model: {multi_best[0]}\")\n",
        "print(f\"   üìà F1-Macro: {multi_best[1]['f1_macro']:.3f}\")\n",
        "print(f\"   üìä Accuracy: {multi_best[1]['accuracy']:.3f}\")\n",
        "print(f\"   ‚è±Ô∏è Training Time: {multi_best[1]['training_time']:.2f}s\")\n",
        "\n",
        "print(\"\\n‚öñÔ∏è Imbalanced Classification:\")\n",
        "imbalance_best = max(comparison_results.items(), key=lambda x: x[1]['f1_macro'])\n",
        "print(f\"   üèÜ Best Technique: {imbalance_best[0]}\")\n",
        "print(f\"   üìà F1-Macro: {imbalance_best[1]['f1_macro']:.3f}\")\n",
        "print(f\"   üéØ Minority F1: {imbalance_best[1]['f1_class_2']:.3f}\")\n",
        "\n",
        "# Algorithm performance across all datasets\n",
        "print(\"\\nü§ñ Algorithm Performance Summary:\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "algorithm_summary = {}\n",
        "\n",
        "# Aggregate results across datasets\n",
        "for name, results in binary_results.items():\n",
        "    if name not in algorithm_summary:\n",
        "        algorithm_summary[name] = {'scores': [], 'times': [], 'datasets': []}\n",
        "    algorithm_summary[name]['scores'].append(results['f1_score'])\n",
        "    algorithm_summary[name]['times'].append(results['training_time'])\n",
        "    algorithm_summary[name]['datasets'].append('Binary')\n",
        "\n",
        "for name, results in multi_results.items():\n",
        "    clean_name = name.split('(')[0].strip()\n",
        "    if clean_name not in algorithm_summary:\n",
        "        algorithm_summary[clean_name] = {'scores': [], 'times': [], 'datasets': []}\n",
        "    algorithm_summary[clean_name]['scores'].append(results['f1_macro'])\n",
        "    algorithm_summary[clean_name]['times'].append(results['training_time'])\n",
        "    algorithm_summary[clean_name]['datasets'].append('Multi-class')\n",
        "\n",
        "# Display algorithm summary\n",
        "for algorithm, data in algorithm_summary.items():\n",
        "    if len(data['scores']) > 1:\n",
        "        avg_score = np.mean(data['scores'])\n",
        "        avg_time = np.mean(data['times'])\n",
        "        print(f\"   {algorithm:20}: Avg F1={avg_score:.3f}, Avg Time={avg_time:.2f}s\")\n",
        "        print(f\"                        Datasets: {', '.join(data['datasets'])}\")\n",
        "\n",
        "# Best practices recommendations\n",
        "print(\"\\nüí° Classification Best Practices:\")\n",
        "print(\"=\" * 34)\n",
        "\n",
        "best_practices = {\n",
        "    \"Data Preparation\": [\n",
        "        \"Always check for class imbalance and handle appropriately\",\n",
        "        \"Use stratified sampling to maintain class proportions\",\n",
        "        \"Scale features, especially for SVM and neural networks\",\n",
        "        \"Handle missing values before training\",\n",
        "        \"Explore feature relationships and correlations\"\n",
        "    ],\n",
        "    \"Algorithm Selection\": [\n",
        "        \"Start with Random Forest for baseline performance\",\n",
        "        \"Try Gradient Boosting for potentially better results\",\n",
        "        \"Use Logistic Regression for interpretability\",\n",
        "        \"Consider SVM for high-dimensional data\",\n",
        "        \"Ensemble methods often provide best performance\"\n",
        "    ],\n",
        "    \"Evaluation Strategy\": [\n",
        "        \"Use stratified k-fold cross-validation\",\n",
        "        \"Don't rely solely on accuracy for imbalanced data\",\n",
        "        \"Report precision, recall, and F1-score\",\n",
        "        \"Use confusion matrices to understand errors\",\n",
        "        \"Consider business costs of different error types\"\n",
        "    ],\n",
        "    \"Model Selection\": [\n",
        "        \"Compare multiple algorithms systematically\",\n",
        "        \"Use statistical significance testing when needed\",\n",
        "        \"Consider computational costs for production\",\n",
        "        \"Validate on truly unseen data\",\n",
        "        \"Document model assumptions and limitations\"\n",
        "    ],\n",
        "    \"Production Considerations\": [\n",
        "        \"Monitor model performance over time\",\n",
        "        \"Implement proper logging and error handling\",\n",
        "        \"Plan for model retraining and updates\",\n",
        "        \"Consider model interpretability requirements\",\n",
        "        \"Validate data quality in production\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, practices in best_practices.items():\n",
        "    print(f\"\\nüìã {category}:\")\n",
        "    for practice in practices:\n",
        "        print(f\"   ‚Ä¢ {practice}\")\n",
        "\n",
        "# Key metrics to remember\n",
        "print(\"\\nüìä Key Metrics Reference:\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "metrics_guide = {\n",
        "    \"Accuracy\": \"Overall correctness - use when classes are balanced\",\n",
        "    \"Precision\": \"True positives / (True positives + False positives)\",\n",
        "    \"Recall\": \"True positives / (True positives + False negatives)\",\n",
        "    \"F1-Score\": \"Harmonic mean of precision and recall\",\n",
        "    \"AUC-ROC\": \"Area under ROC curve - threshold-independent\",\n",
        "    \"Cohen's Kappa\": \"Agreement corrected for chance\",\n",
        "    \"Macro Average\": \"Unweighted mean - treats all classes equally\",\n",
        "    \"Weighted Average\": \"Weighted by class support - accounts for imbalance\"\n",
        "}\n",
        "\n",
        "for metric, description in metrics_guide.items():\n",
        "    print(f\"   {metric:15}: {description}\")\n",
        "\n",
        "# Common pitfalls to avoid\n",
        "print(\"\\n‚ö†Ô∏è Common Pitfalls to Avoid:\")\n",
        "print(\"-\" * 28)\n",
        "\n",
        "pitfalls = [\n",
        "    \"Using accuracy alone for imbalanced datasets\",\n",
        "    \"Not using stratified sampling for train/test splits\",\n",
        "    \"Forgetting to scale features for distance-based algorithms\",\n",
        "    \"Overfitting to validation set through excessive tuning\",\n",
        "    \"Ignoring computational costs in production environments\",\n",
        "    \"Not validating assumptions about data distribution\",\n",
        "    \"Mixing up precision and recall definitions\",\n",
        "    \"Not considering class imbalance in evaluation\"\n",
        "]\n",
        "\n",
        "for i, pitfall in enumerate(pitfalls, 1):\n",
        "    print(f\"   {i}. {pitfall}\")\n",
        "\n",
        "# Final summary statistics\n",
        "print(\"\\nüìà Session Statistics:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "total_models = len(binary_results) + len(multi_results) + len(comparison_results)\n",
        "total_datasets = 3\n",
        "techniques_covered = 6  # Different algorithm types\n",
        "\n",
        "print(f\"   ü§ñ Total Models Trained: {total_models}\")\n",
        "print(f\"   üìä Datasets Analyzed: {total_datasets}\")\n",
        "print(f\"   üîß Techniques Covered: {techniques_covered}\")\n",
        "print(f\"   ‚è±Ô∏è Estimated Total Time: ~45-60 minutes\")\n",
        "\n",
        "print(\"\\nüéâ Classification Deep Dive Complete!\")\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"   ‚Ä¢ Practice with your own datasets\")\n",
        "print(\"   ‚Ä¢ Explore ensemble methods in detail\")\n",
        "print(\"   ‚Ä¢ Study feature engineering techniques\")\n",
        "print(\"   ‚Ä¢ Learn about neural networks for classification\")\n",
        "print(\"   ‚Ä¢ Implement model monitoring in production\")\n",
        "\n",
        "print(\"\\n‚úÖ You are now ready for advanced classification challenges!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
