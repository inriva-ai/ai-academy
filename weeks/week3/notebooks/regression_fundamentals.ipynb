{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression Fundamentals: Comprehensive Tutorial\n",
        "\n",
        "This notebook provides an in-depth exploration of regression algorithms, evaluation techniques, and advanced patterns for real-world applications.\n",
        "\n",
        "## 🎯 Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you'll master:\n",
        "- **6+ regression algorithms** with their strengths and use cases\n",
        "- **Advanced evaluation metrics** beyond simple RMSE\n",
        "- **Regularization techniques** to prevent overfitting\n",
        "- **Feature engineering** for regression\n",
        "- **Model interpretation** and explainability\n",
        "- **Production deployment** considerations\n",
        "\n",
        "## 📋 Table of Contents\n",
        "\n",
        "1. [Data Loading and Exploration](#1-data-loading-and-exploration)\n",
        "2. [Linear Regression Fundamentals](#2-linear-regression-fundamentals)\n",
        "3. [Regularization Techniques](#3-regularization-techniques)\n",
        "4. [Tree-Based Regression](#4-tree-based-regression)\n",
        "5. [Advanced Evaluation Techniques](#5-advanced-evaluation-techniques)\n",
        "6. [Feature Engineering for Regression](#6-feature-engineering-for-regression)\n",
        "7. [Polynomial and Non-linear Regression](#7-polynomial-and-non-linear-regression)\n",
        "8. [Model Selection and Comparison](#8-model-selection-and-comparison)\n",
        "9. [Production Considerations](#9-production-considerations)\n",
        "10. [Summary and Best Practices](#10-summary-and-best-practices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_diabetes, make_regression\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV,\n",
        "    learning_curve, validation_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
        "    HuberRegressor, RANSACRegressor\n",
        ")\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, GradientBoostingRegressor,\n",
        "    ExtraTreesRegressor, BaggingRegressor\n",
        ")\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    mean_absolute_percentage_error, explained_variance_score\n",
        ")\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy import stats\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "print(\"✅ All packages imported successfully!\")\n",
        "print(f\"📅 Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "We'll work with multiple datasets to demonstrate different regression scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load multiple datasets for comprehensive analysis\n",
        "print(\"📊 Loading Regression Datasets\")\n",
        "print(\"=\" * 32)\n",
        "\n",
        "# Dataset 1: Diabetes Dataset (Medical)\n",
        "diabetes_data = load_diabetes()\n",
        "diabetes_X = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
        "diabetes_y = diabetes_data.target\n",
        "\n",
        "print(f\"🏥 Diabetes Dataset:\")\n",
        "print(f\"   Shape: {diabetes_X.shape}\")\n",
        "print(f\"   Target: Disease progression (continuous)\")\n",
        "print(f\"   Target range: [{diabetes_y.min():.1f}, {diabetes_y.max():.1f}]\")\n",
        "print(f\"   Target mean: {diabetes_y.mean():.1f} ± {diabetes_y.std():.1f}\")\n",
        "\n",
        "# Dataset 2: Create synthetic housing dataset\n",
        "print(f\"\\n🏠 Creating Synthetic Housing Dataset:\")\n",
        "np.random.seed(42)\n",
        "n_samples = 506\n",
        "\n",
        "# Generate realistic housing features\n",
        "house_age = np.random.uniform(5, 100, n_samples)\n",
        "rooms = np.random.normal(6, 1, n_samples)\n",
        "crime_rate = np.random.exponential(3, n_samples)\n",
        "distance_to_city = np.random.uniform(1, 12, n_samples)\n",
        "tax_rate = np.random.normal(400, 100, n_samples)\n",
        "pupil_teacher_ratio = np.random.normal(18, 3, n_samples)\n",
        "\n",
        "# Generate price with realistic relationships\n",
        "price = (\n",
        "    rooms * 8 +\n",
        "    -house_age * 0.2 +\n",
        "    -crime_rate * 2 +\n",
        "    -distance_to_city * 1.5 +\n",
        "    -tax_rate * 0.02 +\n",
        "    -pupil_teacher_ratio * 0.8 +\n",
        "    np.random.normal(0, 3, n_samples) + 25\n",
        ")\n",
        "\n",
        "housing_X = pd.DataFrame({\n",
        "    'house_age': house_age,\n",
        "    'avg_rooms': rooms,\n",
        "    'crime_rate': crime_rate,\n",
        "    'distance_to_city': distance_to_city,\n",
        "    'tax_rate': tax_rate,\n",
        "    'pupil_teacher_ratio': pupil_teacher_ratio\n",
        "})\n",
        "housing_y = price\n",
        "\n",
        "print(f\"   Shape: {housing_X.shape}\")\n",
        "print(f\"   Target: Median home value ($1000s)\")\n",
        "print(f\"   Target range: [${housing_y.min():.1f}k, ${housing_y.max():.1f}k]\")\n",
        "print(f\"   Target mean: ${housing_y.mean():.1f}k ± ${housing_y.std():.1f}k\")\n",
        "\n",
        "# Dataset 3: High-dimensional Synthetic Dataset\n",
        "highdim_X, highdim_y = make_regression(\n",
        "    n_samples=800,\n",
        "    n_features=50,\n",
        "    n_informative=30,\n",
        "    noise=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "highdim_X = pd.DataFrame(highdim_X, columns=[f'feature_{i:02d}' for i in range(50)])\n",
        "\n",
        "print(f\"\\n🔬 High-Dimensional Synthetic Dataset:\")\n",
        "print(f\"   Shape: {highdim_X.shape}\")\n",
        "print(f\"   Target: Synthetic continuous variable\")\n",
        "print(f\"   Target range: [{highdim_y.min():.1f}, {highdim_y.max():.1f}]\")\n",
        "print(f\"   Target mean: {highdim_y.mean():.1f} ± {highdim_y.std():.1f}\")\n",
        "\n",
        "print(\"\\n✅ All datasets loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data exploration\n",
        "def explore_regression_dataset(X, y, dataset_name):\n",
        "    \"\"\"\n",
        "    Comprehensive exploration of a regression dataset.\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 Exploring {dataset_name} Dataset\")\n",
        "    print(\"=\" * (len(dataset_name) + 20))\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"📊 Dataset Shape: {X.shape}\")\n",
        "    print(f\"🎯 Target Statistics:\")\n",
        "    print(f\"   Mean: {y.mean():.3f}\")\n",
        "    print(f\"   Std: {y.std():.3f}\")\n",
        "    print(f\"   Min: {y.min():.3f}\")\n",
        "    print(f\"   Max: {y.max():.3f}\")\n",
        "    print(f\"   Range: {y.max() - y.min():.3f}\")\n",
        "    print(f\"   Skewness: {stats.skew(y):.3f}\")\n",
        "    \n",
        "    # Check for normality of target\n",
        "    _, p_value = stats.shapiro(y[:100] if len(y) > 100 else y)\n",
        "    if p_value > 0.05:\n",
        "        print(f\"   ✅ Target appears normally distributed (p={p_value:.3f})\")\n",
        "    else:\n",
        "        print(f\"   ⚠️ Target may not be normally distributed (p={p_value:.3f})\")\n",
        "    \n",
        "    # Outlier detection\n",
        "    Q1 = np.percentile(y, 25)\n",
        "    Q3 = np.percentile(y, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    outlier_threshold = 1.5 * IQR\n",
        "    outliers = np.sum((y < Q1 - outlier_threshold) | (y > Q3 + outlier_threshold))\n",
        "    outlier_percentage = outliers / len(y) * 100\n",
        "    \n",
        "    print(f\"   Outliers: {outliers} ({outlier_percentage:.1f}%)\")\n",
        "    \n",
        "    # Feature statistics\n",
        "    print(f\"\\n📈 Feature Statistics:\")\n",
        "    print(f\"   Feature count: {X.shape[1]}\")\n",
        "    print(f\"   Missing values: {X.isnull().sum().sum()}\")\n",
        "    \n",
        "    # Feature scaling analysis\n",
        "    feature_ranges = X.max() - X.min()\n",
        "    max_range = feature_ranges.max()\n",
        "    min_range = feature_ranges.min()\n",
        "    \n",
        "    if max_range / min_range > 10:\n",
        "        print(f\"   🔧 Feature scaling recommended (range ratio: {max_range/min_range:.1f}:1)\")\n",
        "    else:\n",
        "        print(f\"   ✅ Feature scales are relatively similar\")\n",
        "    \n",
        "    # Correlation analysis\n",
        "    correlations = X.corrwith(pd.Series(y))\n",
        "    strong_correlations = correlations[abs(correlations) > 0.5]\n",
        "    \n",
        "    print(f\"\\n🔗 Target Correlations:\")\n",
        "    print(f\"   Strong correlations (|r| > 0.5): {len(strong_correlations)}\")\n",
        "    if len(strong_correlations) > 0:\n",
        "        best_feature = correlations.abs().idxmax()\n",
        "        print(f\"   Strongest: {best_feature} (r={correlations[best_feature]:.3f})\")\n",
        "    \n",
        "    return {\n",
        "        'shape': X.shape,\n",
        "        'target_mean': y.mean(),\n",
        "        'target_std': y.std(),\n",
        "        'target_range': y.max() - y.min(),\n",
        "        'target_skew': stats.skew(y),\n",
        "        'outlier_percentage': outlier_percentage,\n",
        "        'needs_scaling': max_range / min_range > 10,\n",
        "        'strong_correlations': len(strong_correlations)\n",
        "    }\n",
        "\n",
        "# Explore all datasets\n",
        "diabetes_stats = explore_regression_dataset(diabetes_X, diabetes_y, \"Diabetes\")\n",
        "housing_stats = explore_regression_dataset(housing_X, housing_y, \"Housing\")\n",
        "highdim_stats = explore_regression_dataset(highdim_X, highdim_y, \"High-Dimensional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Linear Regression Fundamentals\n",
        "\n",
        "Let's start with the foundation of regression: linear regression and its assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear regression fundamentals with housing dataset\n",
        "print(\"📈 Linear Regression Fundamentals\")\n",
        "print(\"=\" * 34)\n",
        "\n",
        "# Use housing dataset for realistic example\n",
        "X_lr = housing_X\n",
        "y_lr = housing_y\n",
        "\n",
        "# Split and scale data\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
        "    X_lr, y_lr, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_lr = StandardScaler()\n",
        "X_train_lr_scaled = scaler_lr.fit_transform(X_train_lr)\n",
        "X_test_lr_scaled = scaler_lr.transform(X_test_lr)\n",
        "\n",
        "print(f\"📊 Training set: {X_train_lr_scaled.shape}\")\n",
        "print(f\"📊 Test set: {X_test_lr_scaled.shape}\")\n",
        "print(f\"🎯 Target range: ${y_train_lr.min():.0f}k - ${y_train_lr.max():.0f}k\")\n",
        "\n",
        "# Fit linear regression model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_lr_scaled, y_train_lr)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr_model.predict(X_test_lr_scaled)\n",
        "y_pred_train_lr = lr_model.predict(X_train_lr_scaled)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "mse_test = mean_squared_error(y_test_lr, y_pred_lr)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "mae_test = mean_absolute_error(y_test_lr, y_pred_lr)\n",
        "r2_test = r2_score(y_test_lr, y_pred_lr)\n",
        "mape_test = mean_absolute_percentage_error(y_test_lr, y_pred_lr)\n",
        "explained_var = explained_variance_score(y_test_lr, y_pred_lr)\n",
        "\n",
        "# Training metrics\n",
        "mse_train = mean_squared_error(y_train_lr, y_pred_train_lr)\n",
        "r2_train = r2_score(y_train_lr, y_pred_train_lr)\n",
        "\n",
        "print(f\"\\n📊 Linear Regression Performance:\")\n",
        "print(f\"   Training R²: {r2_train:.3f}\")\n",
        "print(f\"   Test R²: {r2_test:.3f}\")\n",
        "print(f\"   Test RMSE: ${rmse_test:.1f}k\")\n",
        "print(f\"   Test MAE: ${mae_test:.1f}k\")\n",
        "print(f\"   Test MAPE: {mape_test:.1%}\")\n",
        "print(f\"   Explained Variance: {explained_var:.3f}\")\n",
        "\n",
        "# Analyze overfitting\n",
        "overfitting_gap = r2_train - r2_test\n",
        "if overfitting_gap > 0.1:\n",
        "    print(f\"   ⚠️ Potential overfitting detected (gap: {overfitting_gap:.3f})\")\n",
        "elif overfitting_gap < 0.05:\n",
        "    print(f\"   ✅ Good generalization (gap: {overfitting_gap:.3f})\")\n",
        "else:\n",
        "    print(f\"   ⚡ Moderate overfitting (gap: {overfitting_gap:.3f})\")\n",
        "\n",
        "# Analyze coefficients\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': X_lr.columns,\n",
        "    'Coefficient': lr_model.coef_,\n",
        "    'Abs_Coefficient': np.abs(lr_model.coef_)\n",
        "})\n",
        "coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(f\"\\n🔍 Feature Coefficients (Standardized):\")\n",
        "print(f\"   Intercept: ${lr_model.intercept_:.1f}k\")\n",
        "for _, row in coefficients.head(5).iterrows():\n",
        "    sign = '+' if row['Coefficient'] > 0 else ''\n",
        "    print(f\"   {row['Feature']:15}: {sign}{row['Coefficient']:.2f} (impact: ${abs(row['Coefficient']):.2f}k)\")\n",
        "\n",
        "# Cross-validation analysis\n",
        "print(f\"\\n🔄 Cross-Validation Analysis:\")\n",
        "cv_scores = cross_val_score(lr_model, X_train_lr_scaled, y_train_lr, cv=5, scoring='r2')\n",
        "cv_rmse_scores = cross_val_score(lr_model, X_train_lr_scaled, y_train_lr, cv=5, \n",
        "                                scoring='neg_root_mean_squared_error')\n",
        "\n",
        "print(f\"   CV R²: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
        "print(f\"   CV RMSE: ${-cv_rmse_scores.mean():.1f}k ± ${cv_rmse_scores.std():.1f}k\")\n",
        "\n",
        "print(\"\\n✅ Linear regression analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Regularization Techniques\n",
        "\n",
        "Let's explore Ridge, Lasso, and Elastic Net regression to handle overfitting and feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regularization techniques comparison\n",
        "print(\"🎯 Regularization Techniques\")\n",
        "print(\"=\" * 28)\n",
        "\n",
        "# Use high-dimensional dataset for regularization demo\n",
        "X_reg = highdim_X\n",
        "y_reg = highdim_y\n",
        "\n",
        "# Split and scale data\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_reg = StandardScaler()\n",
        "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
        "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
        "\n",
        "print(f\"📊 Training set: {X_train_reg_scaled.shape}\")\n",
        "print(f\"📊 Test set: {X_test_reg_scaled.shape}\")\n",
        "print(f\"🎯 Features: {X_reg.shape[1]} (high-dimensional for regularization demo)\")\n",
        "\n",
        "# Define regularization models with different alpha values\n",
        "regularization_models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge (α=0.1)': Ridge(alpha=0.1, random_state=42),\n",
        "    'Ridge (α=1.0)': Ridge(alpha=1.0, random_state=42),\n",
        "    'Ridge (α=10.0)': Ridge(alpha=10.0, random_state=42),\n",
        "    'Lasso (α=0.1)': Lasso(alpha=0.1, random_state=42, max_iter=2000),\n",
        "    'Lasso (α=1.0)': Lasso(alpha=1.0, random_state=42, max_iter=2000),\n",
        "    'Elastic Net (α=0.1)': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=2000),\n",
        "    'Elastic Net (α=1.0)': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42, max_iter=2000)\n",
        "}\n",
        "\n",
        "print(f\"\\n🤖 Training {len(regularization_models)} regularized models...\")\n",
        "\n",
        "regularization_results = {}\n",
        "\n",
        "for name, model in regularization_models.items():\n",
        "    print(f\"\\n🔄 Training {name}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train_reg_scaled, y_train_reg)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_reg = model.predict(X_test_reg_scaled)\n",
        "    y_pred_train_reg = model.predict(X_train_reg_scaled)\n",
        "    \n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    r2_test = r2_score(y_test_reg, y_pred_reg)\n",
        "    r2_train = r2_score(y_train_reg, y_pred_train_reg)\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
        "    mae_test = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train_reg_scaled, y_train_reg, cv=5, scoring='r2')\n",
        "    \n",
        "    # Feature selection analysis (for Lasso and Elastic Net)\n",
        "    n_selected_features = np.sum(np.abs(model.coef_) > 1e-5) if hasattr(model, 'coef_') else X_reg.shape[1]\n",
        "    \n",
        "    regularization_results[name] = {\n",
        "        'model': model,\n",
        "        'r2_test': r2_test,\n",
        "        'r2_train': r2_train,\n",
        "        'rmse_test': rmse_test,\n",
        "        'mae_test': mae_test,\n",
        "        'cv_r2_mean': cv_scores.mean(),\n",
        "        'cv_r2_std': cv_scores.std(),\n",
        "        'overfitting_gap': r2_train - r2_test,\n",
        "        'n_selected_features': n_selected_features,\n",
        "        'training_time': training_time,\n",
        "        'predictions': y_pred_reg\n",
        "    }\n",
        "    \n",
        "    print(f\"   ✅ R² Test: {r2_test:.3f} | R² Train: {r2_train:.3f} | Gap: {r2_train - r2_test:.3f}\")\n",
        "    print(f\"      RMSE: {rmse_test:.2f} | Features: {n_selected_features}/{X_reg.shape[1]}\")\n",
        "    print(f\"      CV R²: {cv_scores.mean():.3f}±{cv_scores.std():.3f} | Time: {training_time:.3f}s\")\n",
        "\n",
        "print(\"\\n🏆 Regularization training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tree-Based Regression\n",
        "\n",
        "Let's explore decision trees, random forests, and gradient boosting for regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tree-based regression methods\n",
        "print(\"🌳 Tree-Based Regression Methods\")\n",
        "print(\"=\" * 33)\n",
        "\n",
        "# Use diabetes dataset for tree methods\n",
        "X_tree = diabetes_X\n",
        "y_tree = diabetes_y\n",
        "\n",
        "# Split data\n",
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(\n",
        "    X_tree, y_tree, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"📊 Training set: {X_train_tree.shape}\")\n",
        "print(f\"📊 Test set: {X_test_tree.shape}\")\n",
        "print(f\"🎯 Target range: [{y_train_tree.min():.1f}, {y_train_tree.max():.1f}]\")\n",
        "\n",
        "# Define tree-based models\n",
        "tree_models = {\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "print(f\"\\n🤖 Training {len(tree_models)} tree-based models...\")\n",
        "\n",
        "tree_results = {}\n",
        "\n",
        "for name, model in tree_models.items():\n",
        "    print(f\"\\n🔄 Training {name}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train_tree, y_train_tree)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_tree = model.predict(X_test_tree)\n",
        "    y_pred_train_tree = model.predict(X_train_tree)\n",
        "    \n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    r2_test = r2_score(y_test_tree, y_pred_tree)\n",
        "    r2_train = r2_score(y_train_tree, y_pred_train_tree)\n",
        "    rmse_test = np.sqrt(mean_squared_error(y_test_tree, y_pred_tree))\n",
        "    mae_test = mean_absolute_error(y_test_tree, y_pred_tree)\n",
        "    mape_test = mean_absolute_percentage_error(y_test_tree, y_pred_tree)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train_tree, y_train_tree, cv=5, scoring='r2')\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance = None\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        feature_importance = model.feature_importances_\n",
        "    \n",
        "    tree_results[name] = {\n",
        "        'model': model,\n",
        "        'r2_test': r2_test,\n",
        "        'r2_train': r2_train,\n",
        "        'rmse_test': rmse_test,\n",
        "        'mae_test': mae_test,\n",
        "        'mape_test': mape_test,\n",
        "        'cv_r2_mean': cv_scores.mean(),\n",
        "        'cv_r2_std': cv_scores.std(),\n",
        "        'overfitting_gap': r2_train - r2_test,\n",
        "        'training_time': training_time,\n",
        "        'feature_importance': feature_importance,\n",
        "        'predictions': y_pred_tree\n",
        "    }\n",
        "    \n",
        "    print(f\"   ✅ R² Test: {r2_test:.3f} | R² Train: {r2_train:.3f} | Gap: {r2_train - r2_test:.3f}\")\n",
        "    print(f\"      RMSE: {rmse_test:.2f} | MAE: {mae_test:.2f} | MAPE: {mape_test:.1%}\")\n",
        "    print(f\"      CV R²: {cv_scores.mean():.3f}±{cv_scores.std():.3f} | Time: {training_time:.3f}s\")\n",
        "\n",
        "print(\"\\n🏆 Tree-based models training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "This comprehensive regression tutorial covers key machine learning concepts and techniques for building effective regression models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression best practices and summary\n",
        "print(\"🎓 Regression Fundamentals Summary\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Complete session statistics\n",
        "total_models_trained = len(regularization_results) + len(tree_results)\n",
        "total_datasets_used = 3  # Diabetes, Housing, High-dim\n",
        "techniques_covered = [\n",
        "    \"Linear Regression\", \"Ridge Regression\", \"Lasso Regression\", \"Elastic Net\",\n",
        "    \"Random Forest\", \"Gradient Boosting\", \"Decision Trees\", \"Extra Trees\"\n",
        "]\n",
        "\n",
        "print(f\"📊 Session Statistics:\")\n",
        "print(f\"   🤖 Total Models Trained: {total_models_trained}\")\n",
        "print(f\"   📁 Datasets Analyzed: {total_datasets_used}\")\n",
        "print(f\"   🔧 Techniques Covered: {len(techniques_covered)}\")\n",
        "\n",
        "print(f\"\\n🏆 Key Performance Insights:\")\n",
        "print(\"=\" * 26)\n",
        "\n",
        "# Algorithm recommendations by use case\n",
        "print(f\"\\n💡 Algorithm Recommendations by Use Case:\")\n",
        "print(\"=\" * 43)\n",
        "\n",
        "use_cases = {\n",
        "    \"High Interpretability\": {\n",
        "        \"Primary\": \"Linear Regression\",\n",
        "        \"Alternative\": \"Lasso Regression\",\n",
        "        \"Reason\": \"Simple coefficients, clear feature relationships\"\n",
        "    },\n",
        "    \"Feature Selection\": {\n",
        "        \"Primary\": \"Lasso Regression\",\n",
        "        \"Alternative\": \"Elastic Net\",\n",
        "        \"Reason\": \"Automatic feature selection through L1 penalty\"\n",
        "    },\n",
        "    \"High Accuracy\": {\n",
        "        \"Primary\": \"Random Forest\",\n",
        "        \"Alternative\": \"Gradient Boosting\",\n",
        "        \"Reason\": \"Complex patterns, non-linear relationships\"\n",
        "    },\n",
        "    \"Fast Prediction\": {\n",
        "        \"Primary\": \"Linear Regression\",\n",
        "        \"Alternative\": \"Ridge Regression\",\n",
        "        \"Reason\": \"Simple linear transformation\"\n",
        "    },\n",
        "    \"Robust to Outliers\": {\n",
        "        \"Primary\": \"Random Forest\",\n",
        "        \"Alternative\": \"Huber Regression\",\n",
        "        \"Reason\": \"Tree-based methods and robust loss functions\"\n",
        "    }\n",
        "}\n",
        "\n",
        "for use_case, recommendations in use_cases.items():\n",
        "    print(f\"\\n🎯 {use_case}:\")\n",
        "    print(f\"   Primary: {recommendations['Primary']}\")\n",
        "    print(f\"   Alternative: {recommendations['Alternative']}\")\n",
        "    print(f\"   Reason: {recommendations['Reason']}\")\n",
        "\n",
        "# Key metrics reference\n",
        "print(f\"\\n📊 Key Metrics Reference:\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "metrics_guide = {\n",
        "    \"R² Score\": \"Proportion of variance explained (0-1, higher better)\",\n",
        "    \"RMSE\": \"Root Mean Square Error (same units as target, lower better)\",\n",
        "    \"MAE\": \"Mean Absolute Error (robust to outliers, lower better)\",\n",
        "    \"MAPE\": \"Mean Absolute Percentage Error (scale-independent, lower better)\",\n",
        "    \"Cross-validation\": \"Robust estimate using multiple train/validation splits\"\n",
        "}\n",
        "\n",
        "for metric, description in metrics_guide.items():\n",
        "    print(f\"   {metric:18}: {description}\")\n",
        "\n",
        "# Common pitfalls to avoid\n",
        "print(f\"\\n⚠️ Common Pitfalls to Avoid:\")\n",
        "print(\"-\" * 28)\n",
        "\n",
        "pitfalls = [\n",
        "    \"Using R² alone without considering other metrics\",\n",
        "    \"Not checking linear regression assumptions\",\n",
        "    \"Forgetting to scale features for distance-based algorithms\",\n",
        "    \"Overfitting by optimizing on test set\",\n",
        "    \"Ignoring outliers in model selection\",\n",
        "    \"Not considering multicollinearity in linear models\",\n",
        "    \"Using complex models when simple ones suffice\"\n",
        "]\n",
        "\n",
        "for i, pitfall in enumerate(pitfalls, 1):\n",
        "    print(f\"   {i}. {pitfall}\")\n",
        "\n",
        "print(\"\\n🎉 Regression Fundamentals Complete!\")\n",
        "print(\"\\n🚀 Next Steps:\")\n",
        "print(\"   • Practice with your own datasets\")\n",
        "print(\"   • Explore advanced ensemble methods\")\n",
        "print(\"   • Study feature engineering techniques\")\n",
        "print(\"   • Learn about neural networks for regression\")\n",
        "print(\"   • Implement model monitoring in production\")\n",
        "\n",
        "print(\"\\n✅ You are now ready for advanced regression challenges!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}