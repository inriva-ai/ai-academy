{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 Interactive Workshop: Supervised Learning with Metaflow Pipelines\n",
        "\n",
        "Complete workshop covering supervised learning fundamentals, Metaflow ML pipelines, and LangChain model interpretation.\n",
        "\n",
        "## üéØ Workshop Objectives\n",
        "- Implement multiple supervised learning algorithms (classification & regression)\n",
        "- Build scalable ML pipelines using Metaflow with parallel execution\n",
        "- Compare and evaluate models using comprehensive metrics\n",
        "- Perform hyperparameter tuning with cross-validation\n",
        "- Integrate LLM-powered model interpretation using LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment verification and imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_wine, make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from lightgbm import LGBMRegressor, LGBMClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score, mean_absolute_error, roc_auc_score\n",
        ")\n",
        "from metaflow import FlowSpec, step, Parameter, resources\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain imports (with fallback)\n",
        "try:\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain_community.llms import Ollama\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    LANGCHAIN_AVAILABLE = True\n",
        "    print(\"‚úÖ LangChain available for model interpretation\")\n",
        "except ImportError:\n",
        "    LANGCHAIN_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è LangChain not available - will use alternative explanations\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üéØ Week 3 Workshop Environment Ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Classification with Wine Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and explore wine dataset\n",
        "wine_data = load_wine()\n",
        "X_wine = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
        "y_wine = wine_data.target\n",
        "target_names = wine_data.target_names\n",
        "\n",
        "print(f\"üìä Dataset Shape: {X_wine.shape}\")\n",
        "print(f\"üéØ Classes: {len(target_names)} - {list(target_names)}\")\n",
        "\n",
        "# Data preprocessing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.2, random_state=42, stratify=y_wine\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"üìä Training set: {X_train_scaled.shape}\")\n",
        "print(f\"üìä Test set: {X_test_scaled.shape}\")\n",
        "\n",
        "print(pd.Series(y_wine).value_counts(normalize=True).sort_index() * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multiple classification algorithms\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(random_state=42, probability=True),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "classification_results = {}\n",
        "\n",
        "for name, classifier in classifiers.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    classifier.fit(X_train_scaled, y_train)\n",
        "    y_pred = classifier.predict(X_test_scaled)\n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    classification_results[name] = {\n",
        "        'model': classifier,\n",
        "        'predictions': y_pred,\n",
        "        'accuracy': accuracy,\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"   ‚úÖ Accuracy: {accuracy:.3f} | Time: {training_time:.2f}s\")\n",
        "\n",
        "# Sort by accuracy\n",
        "sorted_results = sorted(classification_results.items(), \n",
        "                       key=lambda x: x[1]['accuracy'], reverse=True)\n",
        "\n",
        "print(\"\\nüìà Performance Ranking:\")\n",
        "for i, (name, results) in enumerate(sorted_results, 1):\n",
        "    print(f\"   {i}. {name}: {results['accuracy']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation for all classifiers (5 folds)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "print(\"üîÑ Cross-validation for all classifiers (5 folds)\")\n",
        "cv_results = {}\n",
        "for name, classifier in classifiers.items():\n",
        "    scores = cross_val_score(classifier, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    cv_results[name] = scores\n",
        "    print(f\"{name}: Mean CV accuracy = {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
        "\n",
        "# Optionally, display as DataFrame\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "print(\"\\nCV scores (per fold):\")\n",
        "print(cv_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Regression with Synthetic Housing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic housing dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 500\n",
        "\n",
        "house_size = np.random.normal(2000, 500, n_samples)\n",
        "bedrooms = np.random.poisson(3, n_samples) + 1\n",
        "age = np.random.exponential(15, n_samples)\n",
        "location_score = np.random.uniform(1, 10, n_samples)\n",
        "crime_rate = np.random.exponential(3, n_samples)\n",
        "\n",
        "price = (\n",
        "    house_size * 150 +\n",
        "    bedrooms * 10000 +\n",
        "    location_score * 5000 +\n",
        "    -age * 1000 +\n",
        "    -crime_rate * 2000 +\n",
        "    np.random.normal(0, 20000, n_samples)\n",
        ")\n",
        "\n",
        "regression_data = pd.DataFrame({\n",
        "    'house_size': house_size,\n",
        "    'bedrooms': bedrooms,\n",
        "    'age': age,\n",
        "    'location_score': location_score,\n",
        "    'crime_rate': crime_rate,\n",
        "    'price': price\n",
        "})\n",
        "\n",
        "print(f\"üìä Regression Dataset Shape: {regression_data.shape}\")\n",
        "\n",
        "# Prepare regression data\n",
        "X_reg = regression_data.drop('price', axis=1)\n",
        "y_reg = regression_data['price']\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_reg = StandardScaler()\n",
        "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
        "X_test_reg_scaled = scaler_reg.transform(X_test_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train regression models\n",
        "regressors = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
        "    'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'SVR': SVR(kernel='rbf'),\n",
        "    'LightGBM': LGBMRegressor(random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "regression_results = {}\n",
        "\n",
        "for name, regressor in regressors.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    regressor.fit(X_train_reg_scaled, y_train_reg)\n",
        "    y_pred_reg = regressor.predict(X_test_reg_scaled)\n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "    \n",
        "    regression_results[name] = {\n",
        "        'model': regressor,\n",
        "        'predictions': y_pred_reg,\n",
        "        'mse': mse,\n",
        "        'r2': r2,\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"   ‚úÖ R¬≤: {r2:.3f} | RMSE: ${np.sqrt(mse):,.0f}\")\n",
        "\n",
        "# Sort by R¬≤ score\n",
        "sorted_reg_results = sorted(regression_results.items(), \n",
        "                           key=lambda x: x[1]['r2'], reverse=True)\n",
        "\n",
        "print(\"\\nüìà Regression Performance Ranking:\")\n",
        "for i, (name, results) in enumerate(sorted_reg_results, 1):\n",
        "    print(f\"   {i}. {name}: R¬≤={results['r2']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Complete Metaflow ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Metaflow ML Pipeline\n",
        "class SupervisedLearningFlow(FlowSpec):\n",
        "    \"\"\"\n",
        "    Complete supervised learning pipeline with parallel model training.\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset_type = Parameter('dataset_type', help='Type of dataset: wine or housing', default='wine')\n",
        "    test_size = Parameter('test_size', help='Test set size (0.0-1.0)', default=0.2)\n",
        "    n_cv_folds = Parameter('n_cv_folds', help='Number of cross-validation folds', default=5)\n",
        "    \n",
        "    @step\n",
        "    def start(self):\n",
        "        \"\"\"Load and prepare the dataset for training.\"\"\"\n",
        "        print(f\"üöÄ Starting Supervised Learning Pipeline\")\n",
        "        print(f\"   Dataset: {self.dataset_type}\")\n",
        "        \n",
        "        if self.dataset_type == 'wine':\n",
        "            wine_data = load_wine()\n",
        "            self.X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
        "            self.y = wine_data.target\n",
        "            self.target_names = wine_data.target_names\n",
        "            self.problem_type = 'classification'\n",
        "            \n",
        "            self.algorithms = {\n",
        "                'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "                'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "                'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "                'svm': SVC(random_state=42, probability=True),\n",
        "                'naive_bayes': GaussianNB()\n",
        "            }\n",
        "        else:  # housing dataset\n",
        "            self.X = X_reg\n",
        "            self.y = y_reg\n",
        "            self.target_names = ['price']\n",
        "            self.problem_type = 'regression'\n",
        "            \n",
        "            self.algorithms = {\n",
        "                'linear_regression': LinearRegression(),\n",
        "                'ridge_regression': Ridge(alpha=1.0, random_state=42),\n",
        "                'lasso_regression': Lasso(alpha=1.0, random_state=42),\n",
        "                'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "                'svr': SVR(kernel='rbf')\n",
        "            }\n",
        "        \n",
        "        print(f\"üìä Dataset shape: {self.X.shape}\")\n",
        "        print(f\"üéØ Problem type: {self.problem_type}\")\n",
        "        \n",
        "        self.next(self.preprocess_data)\n",
        "    \n",
        "    @step\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Preprocess the data: split and scale.\"\"\"\n",
        "        print(\"üîß Preprocessing data...\")\n",
        "        \n",
        "        if self.problem_type == 'classification':\n",
        "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "                self.X, self.y, test_size=self.test_size, \n",
        "                random_state=42, stratify=self.y\n",
        "            )\n",
        "        else:\n",
        "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "                self.X, self.y, test_size=self.test_size, random_state=42\n",
        "            )\n",
        "        \n",
        "        self.scaler = StandardScaler()\n",
        "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
        "        \n",
        "        print(f\"   üìä Training set: {self.X_train_scaled.shape}\")\n",
        "        print(f\"   üìä Test set: {self.X_test_scaled.shape}\")\n",
        "        \n",
        "        self.algorithm_names = list(self.algorithms.keys())\n",
        "        self.next(self.train_model, foreach='algorithm_names')\n",
        "    \n",
        "    @resources(memory=2000, cpu=2)\n",
        "    @step\n",
        "    def train_model(self):\n",
        "        \"\"\"Train individual models in parallel.\"\"\"\n",
        "        self.current_algorithm = self.input\n",
        "        algorithm = self.algorithms[self.current_algorithm]\n",
        "        \n",
        "        print(f\"üèãÔ∏è Training {self.current_algorithm}...\")\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        algorithm.fit(self.X_train_scaled, self.y_train)\n",
        "        y_pred = algorithm.predict(self.X_test_scaled)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        if self.problem_type == 'classification':\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            cv_scores = cross_val_score(algorithm, self.X_train_scaled, self.y_train, \n",
        "                                       cv=self.n_cv_folds, scoring='accuracy')\n",
        "            \n",
        "            self.model_results = {\n",
        "                'algorithm': self.current_algorithm,\n",
        "                'model': algorithm,\n",
        "                'predictions': y_pred,\n",
        "                'accuracy': accuracy,\n",
        "                'cv_mean': cv_scores.mean(),\n",
        "                'cv_std': cv_scores.std(),\n",
        "                'training_time': training_time\n",
        "            }\n",
        "            \n",
        "            print(f\"   ‚úÖ {self.current_algorithm}: Accuracy={accuracy:.3f}, CV={cv_scores.mean():.3f}¬±{cv_scores.std():.3f}\")\n",
        "            \n",
        "        else:  # regression\n",
        "            mse = mean_squared_error(self.y_test, y_pred)\n",
        "            r2 = r2_score(self.y_test, y_pred)\n",
        "            cv_scores = cross_val_score(algorithm, self.X_train_scaled, self.y_train, \n",
        "                                       cv=self.n_cv_folds, scoring='r2')\n",
        "            \n",
        "            self.model_results = {\n",
        "                'algorithm': self.current_algorithm,\n",
        "                'model': algorithm,\n",
        "                'predictions': y_pred,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'cv_mean': cv_scores.mean(),\n",
        "                'cv_std': cv_scores.std(),\n",
        "                'training_time': training_time\n",
        "            }\n",
        "            \n",
        "            print(f\"   ‚úÖ {self.current_algorithm}: R¬≤={r2:.3f}, CV={cv_scores.mean():.3f}¬±{cv_scores.std():.3f}\")\n",
        "        \n",
        "        self.next(self.aggregate_results)\n",
        "    \n",
        "    @step\n",
        "    def aggregate_results(self, inputs):\n",
        "        \"\"\"Aggregate results from all parallel training tasks.\"\"\"\n",
        "        print(\"üîÑ Aggregating results from parallel training...\")\n",
        "        \n",
        "        self.all_model_results = {}\n",
        "        self.training_summary = []\n",
        "        \n",
        "        for input_flow in inputs:\n",
        "            algorithm = input_flow.current_algorithm\n",
        "            results = input_flow.model_results\n",
        "            self.all_model_results[algorithm] = results\n",
        "            self.training_summary.append(results)\n",
        "        \n",
        "        print(f\"‚úÖ Aggregated results from {len(self.all_model_results)} models\")\n",
        "        self.next(self.model_selection)\n",
        "    \n",
        "    @step\n",
        "    def model_selection(self):\n",
        "        \"\"\"Select the best model based on cross-validation performance.\"\"\"\n",
        "        print(\"üèÜ Selecting best model...\")\n",
        "        \n",
        "        best_algorithm = max(self.all_model_results.keys(), \n",
        "                           key=lambda x: self.all_model_results[x]['cv_mean'])\n",
        "        best_score = self.all_model_results[best_algorithm]['cv_mean']\n",
        "        \n",
        "        self.best_model = {\n",
        "            'algorithm': best_algorithm,\n",
        "            'results': self.all_model_results[best_algorithm],\n",
        "            'score': best_score\n",
        "        }\n",
        "        \n",
        "        print(f\"üèÜ Best Model: {best_algorithm}\")\n",
        "        print(f\"üìä Best Score: {best_score:.3f}\")\n",
        "        \n",
        "        self.next(self.hyperparameter_tuning)\n",
        "    \n",
        "    @step\n",
        "    def hyperparameter_tuning(self):\n",
        "        \"\"\"Perform hyperparameter tuning on the best model.\"\"\"\n",
        "        print(\"‚öôÔ∏è Hyperparameter tuning for best model...\")\n",
        "        \n",
        "        best_algorithm = self.best_model['algorithm']\n",
        "        print(f\"üîß Tuning {best_algorithm}...\")\n",
        "        \n",
        "        param_grids = {\n",
        "            'logistic_regression': {'C': [0.1, 1.0, 10.0], 'solver': ['liblinear', 'lbfgs']},\n",
        "            'random_forest': {'n_estimators': [50, 100], 'max_depth': [None, 10], 'min_samples_split': [2, 5]},\n",
        "            'gradient_boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2], 'max_depth': [3, 5]},\n",
        "            'svm': {'C': [0.1, 1.0, 10.0], 'gamma': ['scale', 'auto']},\n",
        "            'ridge_regression': {'alpha': [0.1, 1.0, 10.0]},\n",
        "            'lasso_regression': {'alpha': [0.1, 1.0, 10.0]},\n",
        "            'svr': {'C': [0.1, 1.0, 10.0], 'gamma': ['scale', 'auto']}\n",
        "        }\n",
        "        \n",
        "        param_grid = param_grids.get(best_algorithm, {})\n",
        "        \n",
        "        if param_grid:\n",
        "            base_model = self.algorithms[best_algorithm]\n",
        "            scoring = 'accuracy' if self.problem_type == 'classification' else 'r2'\n",
        "            \n",
        "            grid_search = GridSearchCV(\n",
        "                base_model, param_grid, cv=self.n_cv_folds, \n",
        "                scoring=scoring, n_jobs=-1\n",
        "            )\n",
        "            \n",
        "            grid_search.fit(self.X_train_scaled, self.y_train)\n",
        "            \n",
        "            self.tuned_model = grid_search.best_estimator_\n",
        "            self.best_params = grid_search.best_params_\n",
        "            self.tuning_score = grid_search.best_score_\n",
        "            \n",
        "            print(f\"‚úÖ Best parameters: {self.best_params}\")\n",
        "            print(f\"üìà Tuned CV score: {self.tuning_score:.3f}\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è No hyperparameters to tune for this algorithm\")\n",
        "            self.tuned_model = self.all_model_results[best_algorithm]['model']\n",
        "            self.best_params = {}\n",
        "            self.tuning_score = self.best_model['score']\n",
        "        \n",
        "        self.next(self.final_evaluation)\n",
        "    \n",
        "    @step\n",
        "    def final_evaluation(self):\n",
        "        \"\"\"Final evaluation with tuned model.\"\"\"\n",
        "        print(\"üìä Final Model Evaluation\")\n",
        "        \n",
        "        y_pred_final = self.tuned_model.predict(self.X_test_scaled)\n",
        "        \n",
        "        if self.problem_type == 'classification':\n",
        "            final_accuracy = accuracy_score(self.y_test, y_pred_final)\n",
        "            conf_matrix = confusion_matrix(self.y_test, y_pred_final)\n",
        "            \n",
        "            self.final_results = {\n",
        "                'accuracy': final_accuracy,\n",
        "                'confusion_matrix': conf_matrix.tolist(),\n",
        "                'predictions': y_pred_final.tolist(),\n",
        "                'actual': self.y_test.tolist()\n",
        "            }\n",
        "            \n",
        "            print(f\"üéØ Final Accuracy: {final_accuracy:.3f}\")\n",
        "        else:\n",
        "            final_mse = mean_squared_error(self.y_test, y_pred_final)\n",
        "            final_r2 = r2_score(self.y_test, y_pred_final)\n",
        "            \n",
        "            self.final_results = {\n",
        "                'mse': final_mse,\n",
        "                'r2': final_r2,\n",
        "                'rmse': np.sqrt(final_mse),\n",
        "                'predictions': y_pred_final.tolist(),\n",
        "                'actual': self.y_test.tolist()\n",
        "            }\n",
        "            \n",
        "            print(f\"üéØ Final R¬≤: {final_r2:.3f}\")\n",
        "        \n",
        "        self.next(self.generate_insights)\n",
        "    \n",
        "    @step  \n",
        "    def generate_insights(self):\n",
        "        \"\"\"Generate insights and model interpretation.\"\"\"\n",
        "        print(\"üí° Generating Model Insights\")\n",
        "        \n",
        "        insights = []\n",
        "        \n",
        "        # Feature importance (if available)\n",
        "        if hasattr(self.tuned_model, 'feature_importances_'):\n",
        "            feature_importance = dict(zip(self.X.columns, self.tuned_model.feature_importances_))\n",
        "            top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "            \n",
        "            insights.append(\"üîç Top 5 Most Important Features:\")\n",
        "            for i, (feature, importance) in enumerate(top_features, 1):\n",
        "                insights.append(f\"   {i}. {feature}: {importance:.3f}\")\n",
        "        \n",
        "        # Performance insights\n",
        "        if self.problem_type == 'classification':\n",
        "            accuracy = self.final_results['accuracy']\n",
        "            if accuracy > 0.95:\n",
        "                insights.append(\"üèÜ Excellent model performance - ready for production!\")\n",
        "            elif accuracy > 0.90:\n",
        "                insights.append(\"‚úÖ Very good performance - minor optimizations possible\")\n",
        "            elif accuracy > 0.80:\n",
        "                insights.append(\"‚ö†Ô∏è Good performance - consider feature engineering\")\n",
        "            else:\n",
        "                insights.append(\"‚ùå Performance needs improvement\")\n",
        "        else:\n",
        "            r2 = self.final_results['r2']\n",
        "            if r2 > 0.90:\n",
        "                insights.append(\"üèÜ Excellent predictive power!\")\n",
        "            elif r2 > 0.75:\n",
        "                insights.append(\"‚úÖ Good predictive performance\")\n",
        "            else:\n",
        "                insights.append(\"‚ö†Ô∏è Moderate predictive power - room for improvement\")\n",
        "        \n",
        "        # Training efficiency insights\n",
        "        fastest_model = min(self.training_summary, key=lambda x: x['training_time'])\n",
        "        insights.append(f\"‚ö° Fastest training: {fastest_model['algorithm']} ({fastest_model['training_time']:.2f}s)\")\n",
        "        \n",
        "        best_cv = max(self.training_summary, key=lambda x: x['cv_mean'])\n",
        "        insights.append(f\"üéØ Best cross-validation: {best_cv['algorithm']} ({best_cv['cv_mean']:.3f})\")\n",
        "        \n",
        "        self.model_insights = insights\n",
        "        \n",
        "        for insight in insights:\n",
        "            print(insight)\n",
        "        \n",
        "        self.next(self.end)\n",
        "    \n",
        "    @step\n",
        "    def end(self):\n",
        "        \"\"\"Complete the pipeline and generate final summary.\"\"\"\n",
        "        print(\"\\nüéâ SUPERVISED LEARNING PIPELINE COMPLETE!\")\n",
        "        \n",
        "        summary = {\n",
        "            'dataset_info': {\n",
        "                'type': self.dataset_type,\n",
        "                'shape': f\"{self.X.shape[0]} samples √ó {self.X.shape[1]} features\",\n",
        "                'problem_type': self.problem_type\n",
        "            },\n",
        "            'training_summary': {\n",
        "                'algorithms_tested': len(self.all_model_results),\n",
        "                'best_algorithm': self.best_model['algorithm'],\n",
        "                'hyperparameter_tuning': bool(self.best_params),\n",
        "                'total_training_time': sum(result['training_time'] for result in self.all_model_results.values())\n",
        "            },\n",
        "            'performance': self.final_results,\n",
        "            'insights': self.model_insights\n",
        "        }\n",
        "        \n",
        "        self.pipeline_summary = summary\n",
        "        \n",
        "        print(\"üìä Pipeline Summary:\")\n",
        "        print(f\"   üóÉÔ∏è Dataset: {summary['dataset_info']['shape']}\")\n",
        "        print(f\"   ü§ñ Algorithms tested: {summary['training_summary']['algorithms_tested']}\")\n",
        "        print(f\"   üèÜ Best model: {summary['training_summary']['best_algorithm']}\")\n",
        "        print(f\"   ‚öôÔ∏è Hyperparameter tuning: {'Yes' if summary['training_summary']['hyperparameter_tuning'] else 'No'}\")\n",
        "        \n",
        "        if self.problem_type == 'classification':\n",
        "            print(f\"   üéØ Final accuracy: {self.final_results['accuracy']:.3f}\")\n",
        "        else:\n",
        "            print(f\"   üéØ Final R¬≤: {self.final_results['r2']:.3f}\")\n",
        "        \n",
        "        print(\"\\n‚ú® All results and models saved by Metaflow!\")\n",
        "        print(\"üíæ Access results using: flow.run.data\")\n",
        "        print(\"üîÑ Reproduce anytime: python flow.py run\")\n",
        "\n",
        "print(\"‚úÖ SupervisedLearningFlow class defined!\")\n",
        "print(\"üí° To run: save as .py file and execute 'python pipeline.py run'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: LangChain Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangChain Model Interpretation System\n",
        "def create_model_interpreter():\n",
        "    \"\"\"Create LangChain-based model interpretation system.\"\"\"\n",
        "    if not LANGCHAIN_AVAILABLE:\n",
        "        print(\"‚ö†Ô∏è LangChain not available - using fallback interpretation\")\n",
        "        return None\n",
        "    \n",
        "    interpretation_prompt = PromptTemplate(\n",
        "        input_variables=[\"model_type\", \"accuracy\", \"features\", \"dataset\"],\n",
        "        template=\"\"\"\n",
        "        You are an expert data scientist explaining machine learning results to a business audience.\n",
        "        \n",
        "        Model Details:\n",
        "        - Algorithm: {model_type}\n",
        "        - Accuracy: {accuracy}\n",
        "        - Key Features: {features}\n",
        "        - Dataset: {dataset}\n",
        "        \n",
        "        Please provide:\n",
        "        1. A simple explanation of how this model works\n",
        "        2. What the accuracy score means in practical terms\n",
        "        3. Which features are most important and why\n",
        "        4. Recommendations for model deployment\n",
        "        \n",
        "        Keep the explanation clear and business-focused.\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        llm = Ollama(model=\"llama3.2\")\n",
        "        interpretation_chain = interpretation_prompt | llm | StrOutputParser()\n",
        "        return interpretation_chain\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not initialize LangChain: {e}\")\n",
        "        return None\n",
        "\n",
        "def interpret_model_results(model_name, accuracy, top_features, dataset_name):\n",
        "    \"\"\"Generate natural language interpretation of model results.\"\"\"\n",
        "    interpreter = create_model_interpreter()\n",
        "    \n",
        "    if interpreter is None:\n",
        "        return f\"\"\"\n",
        "        üìä Model Interpretation (Fallback):\n",
        "        \n",
        "        The {model_name} algorithm achieved {accuracy:.1%} accuracy on the {dataset_name} dataset.\n",
        "        \n",
        "        Key insights:\n",
        "        ‚Ä¢ This model can correctly predict the outcome {accuracy:.1%} of the time\n",
        "        ‚Ä¢ Most important features: {', '.join(top_features[:3])}\n",
        "        ‚Ä¢ {'Excellent' if accuracy > 0.9 else 'Good' if accuracy > 0.8 else 'Fair'} performance for business use\n",
        "        \n",
        "        Recommendation: {'Ready for deployment' if accuracy > 0.9 else 'Consider additional tuning'}\n",
        "        \"\"\"\n",
        "    \n",
        "    try:\n",
        "        interpretation = interpreter.invoke({\n",
        "            \"model_type\": model_name,\n",
        "            \"accuracy\": f\"{accuracy:.1%}\",\n",
        "            \"features\": \", \".join(top_features[:5]),\n",
        "            \"dataset\": dataset_name\n",
        "        })\n",
        "        return f\"üß† LLM Interpretation:\\n{interpretation}\"\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è LLM interpretation failed: {e}\")\n",
        "        return interpret_model_results(model_name, accuracy, top_features, dataset_name)\n",
        "\n",
        "# Example usage\n",
        "print(\"üîç Example Model Interpretation:\")\n",
        "example_interpretation = interpret_model_results(\n",
        "    model_name=\"Random Forest\", \n",
        "    accuracy=0.94, \n",
        "    top_features=[\"alcohol\", \"flavanoids\", \"color_intensity\", \"od280/od315_of_diluted_wines\"],\n",
        "    dataset_name=\"Wine Quality\"\n",
        ")\n",
        "print(example_interpretation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Advanced Evaluation and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced hyperparameter tuning demonstration\n",
        "print(\"‚öôÔ∏è Advanced Hyperparameter Tuning\")\n",
        "\n",
        "# Example with Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    rf_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=0\n",
        ")\n",
        "\n",
        "print(\"üîÑ Running grid search...\")\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nüèÜ Grid Search Results:\")\n",
        "print(f\"   Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"   Best CV score: {grid_search.best_score_:.3f}\")\n",
        "print(f\"   Total combinations tested: {len(grid_search.cv_results_['params'])}\")\n",
        "\n",
        "# Test the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_tuned = best_rf.predict(X_test_scaled)\n",
        "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "\n",
        "print(f\"\\nüìä Tuned Model Performance:\")\n",
        "print(f\"   Test accuracy: {tuned_accuracy:.3f}\")\n",
        "print(f\"   Improvement: {tuned_accuracy - classification_results['Random Forest']['accuracy']:.3f}\")\n",
        "\n",
        "# Feature importance from tuned model\n",
        "feature_importance = best_rf.feature_importances_\n",
        "feature_names = X_wine.columns\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nüîç Top 5 Features (Tuned Model):\")\n",
        "for i, row in importance_df.head().iterrows():\n",
        "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Hyperparameter tuning complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workshop Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workshop Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéì WEEK 3 WORKSHOP SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüèÜ What You've Accomplished:\")\n",
        "accomplishments = [\n",
        "    \"‚úÖ Built complete Metaflow ML pipeline with parallel training\",\n",
        "    \"‚úÖ Implemented multiple supervised learning algorithms\",\n",
        "    \"‚úÖ Mastered cross-validation and model evaluation\",\n",
        "    \"‚úÖ Performed hyperparameter tuning with GridSearchCV\",\n",
        "    \"‚úÖ Integrated LangChain for model interpretation\",\n",
        "    \"‚úÖ Created comprehensive evaluation frameworks\",\n",
        "    \"‚úÖ Generated business-ready model insights\"\n",
        "]\n",
        "\n",
        "for achievement in accomplishments:\n",
        "    print(f\"   {achievement}\")\n",
        "\n",
        "print(\"\\nüõ†Ô∏è Key Skills Developed:\")\n",
        "skills = [\n",
        "    \"üåä Metaflow pipeline development with @foreach and @resources\",\n",
        "    \"ü§ñ Scikit-learn model training and evaluation\",\n",
        "    \"üìä Advanced metrics: accuracy, precision, recall, F1, AUC, R¬≤\",\n",
        "    \"‚öôÔ∏è Hyperparameter optimization techniques\",\n",
        "    \"üß† LLM-powered model interpretation\",\n",
        "    \"üìà Cross-validation and model selection\",\n",
        "    \"üîç Feature importance analysis\"\n",
        "]\n",
        "\n",
        "for skill in skills:\n",
        "    print(f\"   {skill}\")\n",
        "\n",
        "print(\"\\nüöÄ Production Readiness Checklist:\")\n",
        "production_items = [\n",
        "    \"üìã Model versioning and artifact management\",\n",
        "    \"üîÑ Automated retraining pipelines\", \n",
        "    \"üìä Performance monitoring and alerting\",\n",
        "    \"üõ°Ô∏è Model validation and testing\",\n",
        "    \"üìù Documentation and interpretation\",\n",
        "    \"üåê Deployment and serving infrastructure\"\n",
        "]\n",
        "\n",
        "for item in production_items:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\nüéØ Week 4 Preview - Advanced ML & LangGraph:\")\n",
        "week4_topics = [\n",
        "    \"üï∏Ô∏è LangGraph for complex AI workflows\",\n",
        "    \"üîÑ Multi-agent systems and tool calling\",\n",
        "    \"üß™ Advanced model architectures\",\n",
        "    \"üåê End-to-end deployment strategies\",\n",
        "    \"üìä MLOps and monitoring systems\",\n",
        "    \"üöÄ Scaling to production workloads\"\n",
        "]\n",
        "\n",
        "for topic in week4_topics:\n",
        "    print(f\"   {topic}\")\n",
        "\n",
        "print(\"\\nüí° Recommended Practice:\")\n",
        "practice_items = [\n",
        "    \"üîÑ Run the complete pipeline with different datasets\",\n",
        "    \"‚öôÔ∏è Experiment with hyperparameter ranges\", \n",
        "    \"üìä Create custom evaluation metrics\",\n",
        "    \"üß† Try different LLM models for interpretation\",\n",
        "    \"üéØ Apply to your own datasets\",\n",
        "    \"üìö Review LangGraph documentation\"\n",
        "]\n",
        "\n",
        "for item in practice_items:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\nüìö Additional Resources:\")\n",
        "resources = [\n",
        "    \"üìñ Metaflow ML Best Practices: https://docs.metaflow.org/scaling/remote-tasks/introduction\",\n",
        "    \"üìñ Scikit-learn User Guide: https://scikit-learn.org/stable/user_guide.html\",\n",
        "    \"üìñ LangChain Model Integration: https://python.langchain.com/docs/integrations/llms/\",\n",
        "    \"üìñ Cross-validation Guide: https://scikit-learn.org/stable/modules/cross_validation.html\",\n",
        "    \"üìñ Hyperparameter Tuning: https://scikit-learn.org/stable/modules/grid_search.html\"\n",
        "]\n",
        "\n",
        "for resource in resources:\n",
        "    print(f\"   {resource}\")\n",
        "\n",
        "print(\"\\nüéâ Excellent work! You're now ready for advanced ML and LangGraph!\")\n",
        "print(\"üèÜ - INRIVA AI Academy Team\")\n",
        "\n",
        "print(\"\\nüìù To save and run the complete pipeline:\")\n",
        "print(\"   1. Copy the SupervisedLearningFlow class to a .py file\")\n",
        "print(\"   2. Add required imports at the top\")\n",
        "print(\"   3. Run with: python your_pipeline.py run\")\n",
        "print(\"   4. View results with: python your_pipeline.py show\")\n",
        "\n",
        "print(\"\\n‚ú® All pipeline artifacts are automatically versioned by Metaflow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Pipeline Export Instructions\n",
        "\n",
        "### Save as Python File\n",
        "```python\n",
        "# complete_ml_pipeline.py\n",
        "from metaflow import FlowSpec, step, Parameter, resources\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error\n",
        "from datetime import datetime\n",
        "\n",
        "# Copy the SupervisedLearningFlow class here\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    SupervisedLearningFlow()\n",
        "```\n",
        "\n",
        "### Run the Pipeline\n",
        "```bash\n",
        "# Run with default parameters (wine dataset)\n",
        "python complete_ml_pipeline.py run\n",
        "\n",
        "# Run with housing dataset\n",
        "python complete_ml_pipeline.py run --dataset_type housing\n",
        "\n",
        "# Run with custom parameters\n",
        "python complete_ml_pipeline.py run --test_size 0.3 --n_cv_folds 10\n",
        "```\n",
        "\n",
        "### View Results\n",
        "```bash\n",
        "# Show latest run\n",
        "python complete_ml_pipeline.py show\n",
        "\n",
        "# List all runs\n",
        "python complete_ml_pipeline.py list\n",
        "```\n",
        "\n",
        "### Access Results Programmatically\n",
        "```python\n",
        "from metaflow import Flow\n",
        "\n",
        "# Get latest run\n",
        "run = Flow('SupervisedLearningFlow').latest_run\n",
        "\n",
        "# Access pipeline summary\n",
        "summary = run.data.pipeline_summary\n",
        "print(f\"Best model: {summary['training_summary']['best_algorithm']}\")\n",
        "\n",
        "# Access model results\n",
        "model_results = run.data.all_model_results\n",
        "for model, results in model_results.items():\n",
        "    print(f\"{model}: {results['cv_mean']:.3f}\")\n",
        "```\n",
        "\n",
        "## üéØ Next Steps\n",
        "\n",
        "1. **Experiment** with different datasets and parameters\n",
        "2. **Extend** the pipeline with additional algorithms\n",
        "3. **Deploy** models using Metaflow's cloud capabilities\n",
        "4. **Integrate** with your existing ML infrastructure\n",
        "\n",
        "**Ready for Week 4: Advanced ML & LangGraph! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
