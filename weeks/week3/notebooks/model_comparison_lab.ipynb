{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Model Comparison Lab\n",
    "## Side-by-Side Algorithm Comparison\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Compare multiple machine learning algorithms on the same dataset\n",
    "- Evaluate model performance using various metrics\n",
    "- Visualize and interpret comparison results\n",
    "- Make informed decisions about algorithm selection\n",
    "- Understand the trade-offs between different algorithms\n",
    "\n",
    "### Dataset Overview\n",
    "We'll use the classic **Wine Quality** dataset to compare different classification algorithms. This dataset contains physicochemical properties of wines and their quality ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine quality dataset\n",
    "# You can download from: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "# For this lab, we'll create a sample dataset\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the wine dataset\n",
    "wine_data = load_wine()\n",
    "X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "y = wine_data.target\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(X.head())\n",
    "\n",
    "print(\"\\nTarget classes:\")\n",
    "print(f\"Classes: {wine_data.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(X.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(X.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "plt.bar(wine_data.target_names, counts, color=['#FF9999', '#66B2FF', '#99FF99'])\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(counts, labels=wine_data.target_names, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Class Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = X.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Testing class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"Original feature means: {X_train.mean().round(2).values[:5]}...\")\n",
    "print(f\"Scaled feature means: {X_train_scaled.mean().round(2).values[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Algorithm Implementation and Comparison\n",
    "\n",
    "We'll compare the following algorithms:\n",
    "1. **Logistic Regression** - Linear classifier\n",
    "2. **Decision Tree** - Tree-based classifier\n",
    "3. **Random Forest** - Ensemble method\n",
    "4. **Support Vector Machine (SVM)** - Kernel-based classifier\n",
    "5. **K-Nearest Neighbors (KNN)** - Instance-based classifier\n",
    "6. **Naive Bayes** - Probabilistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import time\n",
    "\n",
    "# Initialize algorithms with default parameters\n",
    "algorithms = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"Initialized {len(algorithms)} algorithms for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a single algorithm\n",
    "def evaluate_algorithm(name, algorithm, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate a single algorithm and return performance metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(algorithm, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    return {\n",
    "        'Algorithm': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'CV_Mean': cv_mean,\n",
    "        'CV_Std': cv_std,\n",
    "        'Training_Time': training_time,\n",
    "        'Prediction_Time': prediction_time,\n",
    "        'Model': algorithm\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all algorithms\n",
    "results = []\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    result = evaluate_algorithm(name, algorithm, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    results.append(result)\n",
    "\n",
    "print(\"\\nAll algorithms evaluated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.drop('Model', axis=1)  # Remove model objects for display\n",
    "\n",
    "# Sort by accuracy\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== ALGORITHM COMPARISON RESULTS ===\")\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].bar(results_df['Algorithm'], results_df['Accuracy'], color='skyblue')\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0, 1].bar(results_df['Algorithm'], results_df['F1-Score'], color='lightcoral')\n",
    "axes[0, 1].set_title('F1-Score Comparison')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Cross-validation scores with error bars\n",
    "axes[1, 0].bar(results_df['Algorithm'], results_df['CV_Mean'], \n",
    "               yerr=results_df['CV_Std'], capsize=5, color='lightgreen')\n",
    "axes[1, 0].set_title('Cross-Validation Accuracy (Mean ± Std)')\n",
    "axes[1, 0].set_ylabel('CV Accuracy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# Training time comparison\n",
    "axes[1, 1].bar(results_df['Algorithm'], results_df['Training_Time'], color='gold')\n",
    "axes[1, 1].set_title('Training Time Comparison')\n",
    "axes[1, 1].set_ylabel('Training Time (seconds)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for comprehensive comparison\n",
    "from math import pi\n",
    "\n",
    "# Select top 4 algorithms for radar chart\n",
    "top_algorithms = results_df.head(4)\n",
    "\n",
    "# Metrics for radar chart\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "num_metrics = len(metrics)\n",
    "\n",
    "# Create angles for each metric\n",
    "angles = [n / float(num_metrics) * 2 * pi for n in range(num_metrics)]\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for i, (_, row) in enumerate(top_algorithms.iterrows()):\n",
    "    values = [row[metric] for metric in metrics]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Algorithm'], color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Top 4 Algorithms - Performance Radar Chart', size=16, y=1.1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Detailed Analysis of Best Performing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing algorithm\n",
    "best_algorithm_name = results_df.iloc[0]['Algorithm']\n",
    "best_algorithm = next(result['Model'] for result in results if result['Algorithm'] == best_algorithm_name)\n",
    "\n",
    "print(f\"\\n=== DETAILED ANALYSIS: {best_algorithm_name} ===\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_best = best_algorithm.predict(X_test_scaled)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=wine_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=wine_data.target_names, \n",
    "            yticklabels=wine_data.target_names)\n",
    "plt.title(f'Confusion Matrix - {best_algorithm_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"{wine_data.target_names[i]}: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based algorithms)\n",
    "if hasattr(best_algorithm, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_algorithm.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Feature Importances - {best_algorithm_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    display(feature_importance.head(10))\n",
    "    \n",
    "elif hasattr(best_algorithm, 'coef_'):\n",
    "    # For linear models, show coefficient magnitudes\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'coef_magnitude': np.abs(best_algorithm.coef_[0])\n",
    "    }).sort_values('coef_magnitude', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=coef_df.head(10), x='coef_magnitude', y='feature')\n",
    "    plt.title(f'Top 10 Feature Coefficient Magnitudes - {best_algorithm_name}')\n",
    "    plt.xlabel('Coefficient Magnitude')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Features by Coefficient Magnitude:\")\n",
    "    display(coef_df.head(10))\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_algorithm_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for top 3 algorithms\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "top_3_algorithms = results_df.head(3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (_, row) in enumerate(top_3_algorithms.iterrows()):\n",
    "    algorithm_name = row['Algorithm']\n",
    "    algorithm = next(result['Model'] for result in results if result['Algorithm'] == algorithm_name)\n",
    "    \n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        algorithm, X_train_scaled, y_train, cv=5, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    axes[i].plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "    axes[i].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    axes[i].plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
    "    axes[i].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    axes[i].set_title(f'Learning Curve - {algorithm_name}')\n",
    "    axes[i].set_xlabel('Training Set Size')\n",
    "    axes[i].set_ylabel('Accuracy Score')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"           ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n🏆 BEST PERFORMING ALGORITHM: {results_df.iloc[0]['Algorithm']}\")\n",
    "print(f\"   • Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
    "print(f\"   • F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"   • Cross-validation: {results_df.iloc[0]['CV_Mean']:.4f} ± {results_df.iloc[0]['CV_Std']:.4f}\")\n",
    "\n",
    "print(f\"\\n⚡ FASTEST TRAINING: {results_df.loc[results_df['Training_Time'].idxmin(), 'Algorithm']}\")\n",
    "print(f\"   • Training time: {results_df['Training_Time'].min():.4f} seconds\")\n",
    "\n",
    "print(f\"\\n🎯 MOST CONSISTENT (lowest CV std): {results_df.loc[results_df['CV_Std'].idxmin(), 'Algorithm']}\")\n",
    "print(f\"   • CV std: {results_df['CV_Std'].min():.4f}\")\n",
    "\n",
    "print(\"\\n📊 RANKING BY ACCURACY:\")\n",
    "for i, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Algorithm']}: {row['Accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "accuracy_range = results_df['Accuracy'].max() - results_df['Accuracy'].min()\n",
    "if accuracy_range < 0.05:\n",
    "    print(\"   • All algorithms perform similarly (accuracy difference < 5%)\")\n",
    "else:\n",
    "    print(f\"   • Significant performance differences observed (range: {accuracy_range:.3f})\")\n",
    "\n",
    "if results_df.iloc[0]['Training_Time'] > 1.0:\n",
    "    print(\"   • Consider faster algorithms for real-time applications\")\n",
    "else:\n",
    "    print(\"   • Training times are reasonable for all algorithms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Exercises and Questions\n",
    "\n",
    "### Exercise 1: Parameter Tuning\n",
    "Choose the best performing algorithm and try to improve its performance by tuning hyperparameters. Use GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "### Exercise 2: Feature Selection\n",
    "Implement feature selection techniques and see how they affect algorithm performance. Try:\n",
    "- SelectKBest\n",
    "- Recursive Feature Elimination\n",
    "- Feature importance thresholding\n",
    "\n",
    "### Exercise 3: Ensemble Methods\n",
    "Create a voting classifier that combines the top 3 algorithms. Compare its performance to individual algorithms.\n",
    "\n",
    "### Discussion Questions:\n",
    "1. Why might different algorithms perform differently on the same dataset?\n",
    "2. What factors should you consider when choosing an algorithm for a real-world problem?\n",
    "3. How would you explain the trade-offs between accuracy and training time to a non-technical stakeholder?\n",
    "4. When might you choose a slightly less accurate but much faster algorithm?\n",
    "5. How reliable are these results? What could you do to make them more robust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution Template - Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO: Choose best algorithm and define parameter grid\n",
    "# Example for Random Forest:\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "#     'min_samples_split': [2, 5, 10]\n",
    "# }\n",
    "\n",
    "# TODO: Implement GridSearchCV\n",
    "# grid_search = GridSearchCV(...)\n",
    "\n",
    "print(\"\\nExercise 1: Implement hyperparameter tuning here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution Template - Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# TODO: Implement feature selection\n",
    "# selector = SelectKBest(score_func=f_classif, k=10)\n",
    "# X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nExercise 2: Implement feature selection here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution Template - Ensemble Methods\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# TODO: Create voting classifier with top 3 algorithms\n",
    "# voting_clf = VotingClassifier(\n",
    "#     estimators=[...],\n",
    "#     voting='soft'\n",
    "# )\n",
    "\n",
    "print(\"\\nExercise 3: Implement ensemble methods here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you have:\n",
    "\n",
    "1. **Compared multiple algorithms** on the same dataset using consistent evaluation metrics\n",
    "2. **Analyzed performance trade-offs** between accuracy, speed, and consistency\n",
    "3. **Visualized results** using various charts and plots\n",
    "4. **Identified the best performing algorithm** for this specific problem\n",
    "5. **Learned about feature importance** and model interpretability\n",
    "\n",
    "### Key Takeaways:\n",
    "- **No single algorithm** works best for all problems\n",
    "- **Context matters**: Consider accuracy, speed, interpretability, and deployment constraints\n",
    "- **Cross-validation** provides more reliable performance estimates\n",
    "- **Feature engineering** and preprocessing can significantly impact results\n",
    "- **Ensemble methods** often provide the best performance by combining multiple algorithms\n",
    "\n",
    "### Next Steps:\n",
    "- Try this comparison on different datasets\n",
    "- Experiment with hyperparameter tuning\n",
    "- Explore advanced ensemble techniques\n",
    "- Consider algorithm-specific optimizations\n",
    "- Practice explaining results to non-technical audiences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}